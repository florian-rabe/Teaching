\section{Context-Free Syntax}

Abstractly, context-free syntax is specified using grammars.
Concretely, it is implemented using inductive types.

In the sequel, we will start with the standard definitions and then make a series of variation to each of these definitions until they become equivalent.
The intended equivalence is as follows:
\begin{center}
\begin{tabular}{l|l}
CFG & IDT \\
\hline
non-terminal & type \\
production & constructor \\
non-terminal on left of production & return type of constructor \\
non-terminals on right of production & arguments types of constructor \\
terminals on right of production & notation of constructor\\
words derived from non-terminal $N$ & expressions of type $N$
\end{tabular}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Context-Free Grammars}

We start with the usual definition:

\begin{definition}[Context-Free Grammar]
Given a set $\Sigma$ of characters (containing the terminal symbols), a \textbf{context-free grammar} consists of
\begin{compactitem}
\item a set $N$ of names called \textbf{non-terminal symbols}
\item a set of \textbf{productions} each consisting of
 \begin{compactitem}
  \item an element of $N$, called the \textbf{left-hand side}
  \item a word over $\Sigma\cup N$, called the \textbf{right-hand side}
 \end{compactitem}
\end{compactitem}
\end{definition}

\begin{example}
Let $\Sigma=\{0,1,+,\cdot,\doteq,\leq\}$.
We give a grammar for arithmetic expressions and formulas about them:
\begin{commgrammar}
\gprod{E}{0}{}\\
\galtprod{1}{}\\
\galtprod{E+E}{}\\
\galtprod{E\cdot E}{}\\
\gprod{F}{E\doteq E}{}\\
\galtprod{E\leq E}{}\\
\end{commgrammar}
Here we use the BNF style of writing grammars, where the productions are grouped by their left-hand side and written with $\bbc$ and $\bnfalt$.
We have $N=\{E,F\}$.
\end{example}

First, we give a name to each production of a CFG:

\begin{definition}[Context-Free Grammar with Named Productions]
Given a set $\Sigma$ of characters (containing the terminal symbols), a \textbf{context-free grammar} consists of
\begin{compactitem}
\item a set $N$ of names called \emph{non-terminal symbols}
\item a set of \emph{productions} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{left-hand side}
  \item a word over $\Sigma\cup N$, called the \textbf{right-hand side}
 \end{compactitem}
\end{compactitem}
\end{definition}

\begin{example}
The grammar from above with names written to the right of each production
\begin{commgrammar}
\gprod{E}{0}{zero}\\
\galtprod{1}{one}\\
\galtprod{E+E}{sum}\\
\galtprod{E\cdot E}{product}\\
\gprod{F}{E\doteq E}{equality}\\
\galtprod{E\leq E}{lessOrEqual}\\
\end{commgrammar}
This is not common BNF anymore.
\end{example}

Then we add base types to the productions:

\begin{definition}[Context-Free Grammar with Named Productions and Base Types]
Given a set $\Sigma$ of characters (containing the terminal symbols) and a set $T$ of names (containing the base types allowed in productions), a \textbf{context-free grammar} consists of
\begin{compactitem}
\item a set $N$ of names called \emph{non-terminal symbols}
\item a set of \emph{productions} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{left-hand side}
  \item a word over $\Sigma\cup T\cup N$, called the \textbf{right-hand side}
 \end{compactitem}
\end{compactitem}
\end{definition}

The intuition behind base types is that we commonly like to delegate some primitive parts of the grammar to be defined elsewhere.
A typical example are literals such as numbers $0, 1, 2,\ldots$: We could give regular expression syntax for digit-strings.
Instead, it is nicer to just assume we have a set of base types that we can use to insert an infinite set of literals into the grammar.

\begin{example}
Let $Nat$ be the type of natural numbers and let $T=\{Nat\}$.
Then we can improve the grammar from above as follows:
\begin{commgrammar}
\gprod{E}{Nat}{literal}\\
\galtprod{E+E}{sum}\\
\galtprod{E*E}{product}\\
\gprod{F}{E\doteq E}{equality}\\
\galtprod{E\leq E}{lessOrEqual}\\
\end{commgrammar}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inductive Data Types}

We start with the usual definition:

\begin{definition}[Inductive Data Type]\label{def:idt}
Given a set of names $T$ (containing the types known in the current context), an \emph{inductive data type} consists of
\begin{compactitem}
 \item a name, called the \textbf{type},
 \item a set of \textbf{constructors} each consisting of
 \begin{compactitem}
  \item a name $n$
  \item a list of elements of $T\cup\{n\}$, called the \textbf{argument} types
 \end{compactitem} 
\end{compactitem}
\end{definition}

\newcommand{\cons}[2]{\mathtt{#1}\,\kw{of}\,\fold{*}{#2}}
\newcommand{\indtype}[2]{#1\,=\,\fold{\;\tb|\tb}{#2}}
\newcommand{\consnot}[3]{\mathtt{#1}\,\kw{of}\,\fold{*}{#2}\,\#\,#3}
\newcommand{\consn}[2]{\mathtt{#1}\,\#\,#2}

\begin{example}
Let $Nat$ be the type of natural numbers and $T=\{Nat\}$.
We give an inductive type for arithmetic expressions:
\[
\indtype{E}{\cons{literal}{Nat}, \cons{sum}{E,E}, \cons{product}{E,E}} \\
\]
Here we use ML-style notation for inductive data types, which separates constructors by $|$ and writes them as \texttt{name of argument-type-product}.
\end{example}

First we generalize to mutually inductive types:

\begin{definition}[Mutually Inductive Data Types]
Given a set $T$ of names (containing the types known in the current context), a family of \textbf{mutually inductive data type} consists of
\begin{compactitem}
 \item a set $N$ of names, called the \textbf{types},
 \item a set of \emph{constructors} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{return type}
  \item a list of elements of $N\cup T$, called the \textbf{argument} types
 \end{compactitem} 
\end{compactitem}
\end{definition}

\begin{example}
We extend the type definition from above by adding a second type for formulas.
Thus, $N=\{E,F\}$.
\[\mathll{
\indtype{E}{\cons{literal}{Nat}, \cons{sum}{E,E}, \cons{product}{E,E}} \\
\indtype{F}{\cons{equality}{E,E}, \cons{lessOrEqual}{E,E}}
}\]
\end{example}

%It may look $T$ and $\Sigma$ correspond to each other.
%But that is not true, we need to add them to each definition.

Then we add notations to the constructors:

\begin{definition}[Mutually Inductive Data Types with Notations]
Given a set $\Sigma$ of characters (containing the terminal symbols) and a set $T$ of names (containing the types known in the current context), a family of \textbf{mutually inductive data type with notations} consists of
\begin{compactitem}
 \item a set $N$ of names, called the \textbf{types},
 \item a set of \emph{constructors} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{return type}
  \item a list of elements of $T\cup N$, called the \textbf{argument} types
  \item a word over the alphabet $\Sigma\cup T\cup N$ containing the argument types in order and only elements from $\Sigma$ otherwise, called the \textbf{notation} of the constructor
 \end{compactitem} 
\end{compactitem}
\end{definition}

The intuition behind notations is that it can get cumbersome to write all constructor applications as $Name(arguments)$.
It is more convenient to attach a notation to them such as 

\begin{example}
We extend the type definitions from above by adding notations to each constructor.
We use the set $\Sigma=\{+,\cdot,\doteq,\leq\}$ as terminals in the notations.
\[\mathll{
\indtype{E}{\consnot{literal}{Nat}{Nat}, \consnot{sum}{E,E}{E+E}, \consnot{product}{E,E}{E\cdot E}} \\
\indtype{F}{\consnot{equality}{E,E}{E\doteq E}, \consnot{lessOrEqual}{E,E}{E\leq E}}
}\]
Here we write the constructors as \texttt{name of argument-type-product \# notation}.
It is easy to see that this has introduced redundancy: we can infer the argument types from the notation.
So we can just drop the argument types:
\[\mathll{
\indtype{E}{\consn{literal}{Nat}, \consn{sum}{E+E}, \consn{product}{E\cdot E}} \\
\indtype{F}{\consn{equality}{E\doteq E}, \consn{lessOrEqual}{E\leq E}}
}\]
\end{example}


\subsection{Merged Definition}

With the variation from above we have arrived at the following equivalence:

\begin{theorem}
Given a set $\Sigma$ of characters and a set $T$ of names, the following notions are equivalent:
\begin{compactitem}
\item a family of mutually inductive data types in the context of types $T$ with notations using characters from $\Sigma$,
\item a context-free grammar with named productions, terminal symbols from $\Sigma$, and base types $T$.
\end{compactitem}
\end{theorem}
\begin{proof}
The key idea is that
\begin{compactitem}
 \item the types and constructors of the former correspond to the non-terminals and productions of the latter
 \item for each constructor-production pair
  \begin{compactitem}
   \item the right-hand side of the latter corresponds to the notation of the former,
   \item the argument types of the former correspond to the non-terminals occurring on the right-hand side of the latter.
  \end{compactitem}
\end{compactitem}
\end{proof}

In implementations in programming languages, we often drop the notations.
Instead, those are handled, if needed, by special parsing and serialization functions.

However, in an implementation, it is often helpful to additionally give names to each argument of a production/constructor.
That yields the following definition:

\begin{definition}[Context-Free Syntax]
Given a set $\Sigma$ of characters and a set $T$ of names, a context-free syntax consists of
\begin{compactitem}
 \item a set $N$ of names, called the \textbf{non-terminals/types},
 \item a set of \textbf{productions/constructors} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{left-hand side/return type}
  \item a sequence of objects, called the \textbf{right-hand side/arguments} which are one of the following
   \begin{compactitem}
    \item an element of $\Sigma$
    \item a pair written $(n:t)$ of a name $n$, called the \textbf{argument name}, and an element $t\in T\cup N$ called the \textbf{argument type}.
   \end{compactitem}
 \end{compactitem}
\end{compactitem}
\end{definition}

\begin{example}
Using ad hoc language to write the constructors, our example from above as a context-free syntax could look as follows:
\[\mathll{
\indtype{E}{\consn{literal}{(value:Nat)}, \consn{sum}{(left:E)+(right:E)}, \consn{product}{(left:E)\cdot (right:E)}} \\
\indtype{F}{\consn{equality}{(left:E)\doteq (right:E)}, \consn{lessOrEqual}{(left:E)\leq (right:E)}}
}\]
\end{example}

\subsection{Contexts}

We assume a context-free language $l$.

\begin{definition}[Context]
A \textbf{context} is a list of
\begin{compactitem}
\item grammar terminology: productions $N\bbc x$
\item type terminology: declarations $x:N$
\end{compactitem}
where each $x$ is a unique name and each $N$ is non-terminal symbol.

The $x$ are called \textbf{variables}.
\end{definition}

\begin{remark}
Sometimes the grammar itself has specific productions for contexts and variables.
In that case, we speak of \emph{meta-variable contexts} and \textbf{meta-variables} to distinguish them from those of the language.
\end{remark}

\begin{definition}[Expressions in Context]
Given a context $\Gamma$, a word $E$ derived from non-terminal $N$ that may additionally use the productions of the context is called an \emph{expression of type} $N$ \emph{in context} $\Gamma$.

We write this as $\Gamma\vdash_l E:N$.
\end{definition}

\begin{definition}[Substitution]
Given two contexts $\vdash_l \Gamma$ and $\vdash_l\Delta$, a \textbf{substitution} $\gamma$ from $\Gamma=x_1:N_1,\ldots,x_n:N_n$ to $\Delta$ is a list $x_1:=e_1,\ldots,x_n:=e_n$ where every $e_i$ is an expression of type $N_i$ in context $\Delta$ (i.e., $\Delta\vdash_l e_i:N_i$).

We write this as $\Delta\vdash_l \gamma:\Gamma$ or as $\vdash_l \gamma:\Gamma\to \Delta$.
\end{definition}

\begin{definition}[Substitution Application]\label{def:subapp}
Given an expression $\Gamma\vdash_l E:N$ and a substitution $\vdash_l\gamma:\Gamma\to\Delta$ where $\Gamma=x_1:N_1,\ldots,x_n:N_n$ and $\gamma=x_1:=e_1,\ldots,x_n:=e_n$, we write $E[\gamma]$ for the result of replacing every $x_i$ in $E$ with $e_i$.
\end{definition}

\begin{theorem}
If $\Gamma\vdash_l E:N$ and $\vdash_l\gamma:\Gamma\to\Delta$, then $\Delta\vdash_l E[\gamma]:N$.
\end{theorem}

We often want to substitute only a single variable $x:N$ even though $E$ may be defined in a larger context $\Gamma$.
This is often written $E[x:=N]$.
That is just an abbreviation for $E[\gamma]$, where $\gamma$ contains $x:=N$ as well as $y:=y$ for every other variable $y$ of $\Gamma$.


\section{Implementation}

Context-free syntax can be implemented systematically in all programming languages.
But, depending on the style of the language, they make drastically different.
We give the two most important paradigms as examples.

\subsection{Functional Programming Languages}

In a function programming language, inductive data types are a primitive feature.
However, notations and named arguments are not available.
So helper functions must be used.

The basic recipe is as follows:
\begin{compactitem}
\item The types and constructors (without the notations and named arguments) are implemented as family of mutually inductive data types.
\item For each argument of each constructor, a partial projective function is defined.
\item A set of mutually recursive string rendering functions are defined, one for each constructor, that implement the notations.
\end{compactitem}

\begin{example}
We define our example syntax in ML.

First the inductive types (assuming a type $Nat$ already exists in the context):
\[\mathll{
\kw{data}\, \indtype{E}{\cons{literal}{Nat}, \cons{sum}{E,E}, \cons{product}{E,E}} \\
\kw{and}\,\indtype{F}{\cons{equality}{E,E}, \cons{lessOrEqual}{E,E}}
}\]

Now the projection functions:
\[\mathll{
 \kw{fun}\; \mathtt{literal\_value}(\mathtt{literal}(v)) = SOME\; v \\
 |\tb \mathtt{literal\_value}(\_)= NONE \\
 \kw{fun}\; \mathtt{sum\_left}(\mathtt{sum}(x,\_)) = SOME\; x \\
 |\tb \mathtt{sum\_left}(\_)= NONE\\
 \kw{fun}\; \mathtt{sum\_right}(\mathtt{sum}(\_,x)) = SOME\; x \\
 |\tb \mathtt{sum\_right}(\_)= NONE
}\]
and so on for each constructor argument.

Finally, the string rendering functions (assuming a function $natToString$ already exists in the context):
\[\mathll{
 \kw{fun}\; \mathtt{E\_toString}(\mathtt{literal}(v)) = natToString\;v \\
 |\tb \mathtt{E\_toString}(\mathtt{sum}(x,y))= \mathtt{E\_toString}(x) + "+" + \mathtt{E\_toString}(y) \\
 |\tb \mathtt{E\_toString}(\mathtt{product}(x,y))= \mathtt{E\_toString}(x) + "\cdot" + \mathtt{E\_toString}(y) \\
 \kw{and}\; \mathtt{F\_toString}(\mathtt{equality}(x,y)) = \mathtt{E\_toString}(x) + "\doteq" + \mathtt{E\_toString}(y)\\
 |\tb \mathtt{F\_toString}(\mathtt{lessOrEqual}(x,y)) = \mathtt{E\_toString}(x) + "\leq" + \mathtt{E\_toString}(y)
}\]
\end{example}

Because ML has inductive data types as primitives, pattern-matching on our syntax comes for free.
We will get back to that when defining the semantics.

\subsection{Object-Oriented Programming Languages}

In a object-oriented programming language, inductive data types are not available.
Therefore, they must be mimicked using classes.
On the positive side, this supports arguments names, and notations are a bit easier.

The basic recipe is as follows:
\begin{compactitem}
\item Each type is implemented as an abstract class.
\item Each constructor of type $t$ is implemented as a concrete class that extends the abstract class $t$.
\item The arguments names and type of each constructor $c$ are exactly the argument names and types of the class $c$.
The constructor arguments are stored as fields in the class.
\item The abstract classes require a \texttt{toString} method, which is implemented in every concrete class according to its notation.
\end{compactitem}


\begin{example}
We define our example syntax in a generic OO-language somewhat similar to Scala.\footnote{We could use Java or C++ here. But their concrete syntax makes this less clear than it could be. It is straightforward to refine the syntax into that of any specific OO-language.}

In particular, we assume that the sy

\begin{lstlisting}
abstract class E {
  def toString: String
}
class literal extends E {
  field value: Nat
  constructor (value: Nat) {
    this.value = value
  }
  def toString = value.toString
}
class sum extends E {
  field left: Nat
  field right: Nat
  constructor (left: E, right: E) {
    this.left = left
    this.right = right
  }
  def toString = left.toString + "+" + right.toString
}
class product extends E {
  field left: Nat
  field right: Nat
  constructor (left: E, right: E) {
    this.left = left
    this.right = right
  }
  def toString = left.toString + "$\cdot$" + right.toString
}

abstract class F {
  def toString: String
}
class equality extends E {
  field left: Nat
  field right: Nat
  constructor (left: E, right: E) {
    this.left = left
    this.right = right
  }
  def toString = left.toString + "$\doteq$" + right.toString
}
class product extends E {
  field left: Nat
  field right: Nat
  constructor (left: E, right: E) {
    this.left = left
    this.right = right
  }
  def toString = left.toString + "$\leq$" + right.toString
}
\end{lstlisting}
\end{example}

Because OO-languages do not have inductive data types as primitives, pattern-matching on our syntax requires awkward switch statements.
We will get back to that when defining the semantics.

\subsection{Combining Paradigms}

The Scala language combines ideas from functional and OO-programming.
That makes its representation of context-free syntax particularly elegant.

In Scala, the constructor arguments are listed right after the class name.
These are automatically fields of the class, and a default constructor always exists that defines those fields.
That gets rid of a lot of boilerplate.

If we want to make those fields public (and we do because those are the projection functions, we add the keyword \texttt{val} in front of them.
But even that is too much boilerplate. So Scala defines a convenience modifier: if we put \texttt{case} in front of the classes corresponding to constructors of our syntax, Scala puts in the \texttt{val} automatically.
It also generates a default implementation of \texttt{toString}, which we have to override if we want to implement notations, too.
Finally, Scala also generates pattern-matching functions so that we can pattern-match in the same way as in ML.

Then our example becomes (as usual, assuming a class \texttt{Nat} already exists):

\begin{lstlisting}
abstract class E {
  def toString: String
}
case class literal(value: Nat) extends E {
  override def toString = value.toString
}
case class sum(left: Nat, right: Nat) extends E {
  override def toString = left.toString + "+" + right.toString
}
case class product(left: Nat, right: Nat) extends E {
  override def toString = left.toString + "$\cdot$" + right.toString
}

abstract class F {
  def toString: String
}
case class equality(left: Nat, right: Nat) extends E {
  override def toString = left.toString + "$\doteq$" + right.toString
}
case class lessOrEqual(left: Nat, right: Nat) extends E {
  override def toString = left.toString + "$\leq$" + right.toString
}
\end{lstlisting}

\subsection{Issues Regarding Equivalence}

There are a few subtle issues.

\paragraph{Equality}
Forming the same value twice should yield the same result, e.g., $\mathtt{literal}(1)=\mathtt{literal}(1)$.
In ML-style inductive types, that is automatic.
In OO-style class-based implementations, we have $\kw{new}\,\mathtt{literal}(1)=\kw{new}\,\mathtt{literal}(1)$ only if we override the equality check.
Scala allows adding the keyword \kw{case} to a class representing a constructor/production.
Among other things, it allows dropping the \kw{new} and overrides the equality method automatically. 

\paragraph{Null Value}
Object-oriented language automatically provide the \cn{null} value for every class.
Therefore, if we implement a context-free syntax via classes, we always get one spurious value that should not exist.
Programmers have to use programming discipline to avoid using it.
Occasionally, static analysis tools can non-null annotations to check this discipline.

\paragraph{Sealing}
When we seal a context-free syntax, it is impossible to add new constructors to it.
That guarantees implementors of inductive functions that they know all possible cases and can therefore pattern-match on them.
An inductive function on an unsealed datatype can fail when new constructors are added later.

ML-style inductive data types are always sealed.
OO-style class-based implementations are never sealed.
In Scala, the keyword \kw{sealed} can be added to an abstract class --- it forbids any constructors other than the ones present in the same source file.


%The equivalence is as follows:
%\begin{center}
%\begin{tabular}{l|l}
%syntax & case-based function & recursive function \\
%\hline
%non-terminal $N$ & function $N$ & function with input $x:N$\\ 
%production & constructor \\
%non-terminal on left of production & return type of constructor \\
%non-terminals on right of production & arguments types of constructor \\
%terminals on right of production & notation of constructor
%\end{tabular}
%\end{center}


\section{Context-Sensitive Syntax}

It is common to define a language as the set of words that can be produced from the syntax, i.e., from a distinguished non-terminal (the start symbol) of the context-free grammar.
It is common to define a context-sensitive language as a special case: the set of words that can be produced from a context-sensitive grammar.

This is, however, not helpful in practice.
While the above remains the official definition of what the context-sensitive languages are, all practical definitions are entirely different.
In fact, context-sensitive grammars are virtually never used to define a specific language.
Instead, more restrictive definitions are used that capture more properties of practical languages.
In the sequel, we give one possible definition.\footnote{This definition works well in the context of this lecture but is non-standard. There is no standard definition at all. Instead, various similar definitions exist.}

\begin{definition}
A \textbf{language system} consists of
\begin{compactitem}
 \item a context-free syntax,
 \item a distinguished non-terminal symbol $\ThySym$, whose words are called \textbf{vocabularies},
 \item a set of distinguished non-terminal symbols $\ExpSym$, whose words are called $\ExpSym$-\textbf{expressions},
 \item a unary predicate $\wft{\Theta}$ on vocabularies $\Theta$,
 \item for every vocabulary $\Theta$ and every $\ExpSym$, a unary predicate $\wff{\Theta}{\ExpSym}{E}$ on $\ExpSym$-expressions $E$.
\end{compactitem}

In case of $\wft{\Theta}$, we call $\Theta$ \textbf{well-formed}.
In case of $\wff{\Theta}{\ExpSym}{E}$, we call $E$ a \textbf{well-formed} $\ExpSym$-expression over $\Theta$.
\end{definition}

\begin{remark}[Terminology]
``language system'' is not a standard term. We usually just say ``language''.

``Well-formed $\ExpSym$-expression over $\Theta$'' can be a mouthful.
Therefore, it is common to simply say that $E$ is an $\ExpSym$-expression, or that $E$ is a $\Theta$-expression, and expect readers to fill in the details.

It is also common to give the non-terminal $\ExpSym$ names, such as ``term'', ``type'', or ``formula''.
Then we simply say ``term'' instead of ``term-expression'' and so on.
\end{remark}

The vocabularies are typically lists of typically named declarations.
They introduce the names that can be used to form expressions.
The expression kinds almost always include formulas.

Often declarations contain additional expressions, most importantly types or definitions.
In general, all expressions may occur in declarations, but many language systems do not use all of them.

Very different names are used for the vocabularies in different communities.
The following table gives an overview:

\begin{center}
\begin{tabular}{l|ll}
Aspect & vocabulary $\Theta$ & expression kinds $\ExpSym$ \\
\hline
Ontologization  & ontology & individual, concept, relation, property, formula \\
Concretization & database schema & cell, row, table, formula \\
Computation & program & term, type, object, class, \ldots \\
Logic & theory & term, type, formula, \ldots \\
Narration & dictionary & phrases, sentences, texts \\
\end{tabular}
\end{center}

In practice, it is most useful to think of a language system as family of languages: one language (containing the expressions) for every vocabulary.

\section{Absolute Semantics: By an Inference System}

To define a language system, we need to define the well-formedness predicates.
One way to do that is by translating the context-free syntax into another language and then use existing definitions of well-formedness there.

But we also need to be able to get off the ground, i.e., to define a semantics from scratch when we do not have another language available.
This is usually done by giving an inference system for the predicates for well-formedness, also called a type system.

We can also do both: if we have a semantics via an inference system and another one via translation, we can show that the latter respects the former.
That leads to the concepts of soundness (everything well-formed is translated to something well-formed) and its dual completeness.

\section{Relative Semantics: By Translation}

\subsection{General Definition}

The correspondence for the syntax between context-free grammars and inductive data types can be extended to the semantics.
Now we have a correspondence between case-based function definitions and inductive functions.

\begin{definition}
A \textbf{semantics by translation} consists of the following parts:
\begin{compactitem}
 \item syntax: a formal language $l$
 \item semantic language: a formal language $L$ (from a different or the same aspect as $l$)
 \item semantic prefix: a vocabulary $P$ in $L$ that is prefixed to the translation of all vocabularies of $l$
 \item interpretation: a function that translates every $l$-vocabulary $T$ to an $L$-vocabulary $P,\sem{T}$
\end{compactitem}
\end{definition}

Critically, the semantic language (which is itself a formal language and can thus have a semantics itself) must be a language whose semantics we already know.
Therefore, it is often important to give multiple equivalent semantics --- choosing a different semantics for different audiences, who might be familiar with different languages.

The role of the semantic prefix $P$ is to define once and for all the $L$-material that we need in general to interpret $l$-theories (in our case: ontologies).
It occurs at the beginning of all interpretations of ontologies.
In particular, it is equal to the interpretation of empty ontology.

\subsection{Compositional Semantics}

\paragraph{Compositionality}
There are some general principles shared by all translations:
\begin{compactitem}
 \item Every $l$-declaration is translated to an $L$-declaration for the same name, and ontologies are translated declaration-wise.
 \item For every non-terminal $N$ of $l$, there is one inductive function $\sem{-}_N$ mapping complex $l$-expressions derived from $N$ to $L$-expressions.
 \item The base cases of references to declared $l$-identifiers are translated to themselves, i.e., to the identifiers of the same name declared in $L$.
 \item The other cases are compositional: every case for a complex $l$-expression recurses only into the semantics of the direct subexpressions.
\end{compactitem}

The notion of \emph{compositionality} captures these properties.
An interpretation function is compositional if the interpretation of any kind of expression $E(e_1,\ldots,e_n)$ with subexpressions $e_i$ only depends on $E$ and the interpretation of the $e_i$, i.e., \[\sem{E(e_1,\ldots,e_n)}=\sem{E}(\sem{e_1},\ldots,\sem{e_n})\] for some semantic operation $\sem{E}$.
Compositionality is also called the substitution property or the homomorphism property.
See also Def.~\ref{ex:compositional}.

More rigorously, we define a compositional translation as follows:
\begin{definition}[Compositional Semantics]
Consider a semantics for syntax grammar $l$ and interpretation function $\sem{-}$.

$\sem{-}$ is compositional if it is defined as follows:
\begin{compactitem}
 \item a family of functions $\sem{-}_N$, one for every non-terminal $N$ of $l$
 \item for every expressions $E$ derived from $N$, we put $\sem{E}=\sem{E}_N$
 \item each $\sem{-}_N$ is defined by induction on the productions for $N$
 \item for each production $N\bbc *(N_1,\ldots,N_r)$ and all expressions $e_i$ derived from $N_i$
   \[\sem{*(e_1,\ldots,e_r)}_N=\sem{*}(\sem{e_1}_{N_1},\ldots,\sem{e_r}_{N_r})\]
   for some $L$-expression $\sem{*}$
\end{compactitem}

Without loss of generality, we can assume that every production is of the form $N\bbc *(N_1,\ldots,N_r)$ where the $N_i$ are all the non-terminals on the right-hand side and $*$ is a stand-in for all the terminal symbols.
\end{definition}

\paragraph{Compositional Translations of Contexts}
We can extend every compositional translation to contexts, substitutions, and expressions in contexts:

\begin{definition}
Given a translation $\sem{-}$ as above, for a non-terminal $N$, we define $\sem{N}$ as the non-terminal from which the translations of $N$-expressions are derived.

Then we define:
\[\sem{x_1:N_1,\ldots,x_n:N_n}:=x_1:\sem{N_1},\ldots,x_n:\sem{N_n}\]
\[\sem{x_1:=w_1,\ldots,x_n:=w_n}:=x_1:=\sem{w_1},\ldots,x_n:=\sem{w_n}\]
\[\sem{x}:=x\]
\end{definition}

The requirement of compositionality is critical for two reasons:
\begin{compactitem}
\item A non-compositional translation could translate $l$-expressions derived from the same non-terminal $N$ to $L$-expressions derived from different non-terminals. Then we would not be able to define $\sem{N}$.
\item The definition $\sem{x}:=x$ adds a case to the case distinction in the compositional translation function.
Without compositionality, this would not make sense.
\end{compactitem}

\begin{theorem}[Type Preservation]
For a compositional translation as above, we have
  \[\Gamma\vdash_l w:N \tb\mimplies\tb \sem{\Gamma}\vdash_L \sem{w}:\sem{N}\]
\end{theorem}

\paragraph{Substitution Theorem}
The main value of compositionality is the following:
\begin{theorem}[Substitution Theorem]
Consider a compositional semantics.

For every context $\Gamma=x_1:N_1,\ldots,x_r:N_r$, every syntax expression $\Gamma \vdash_l E:N$,
and every substitution $\vdash_l \gamma:\Gamma$
\[\sem{E[\gamma]}_N=\sem{E}[\sem{\gamma}]\]
\end{theorem}

Formulated without substitutions, this means that for every syntax expression $E(e_1,\ldots,e_r)$ derived from $N$, where the $e_i$ are subexpression derived from non-terminal $N_i$, we have
\[\sem{E(e_1,\ldots,e_n)}_N=\sem{E}(\sem{e_1}_{N_1},\ldots,\sem{e_n}_{N_r})\]

Simply put, a semantics is compositional iff it is defined by mutually inductive translation functions with only compositional cases.
The latter is very easy to check by inspecting the shape of the finitely many cases of the definition.
The former is a powerful property because it applies to any of the infinitely many expressions of the syntax.

\subsection{Non-Compositional Semantics}

It is highly desirable but not always possible to give a compositional translation.
Sometimes a feature of the syntactic language cannot be directly interpreted in the semantic language.
In that case, it may still be possible to give a non-compositional translation.

\begin{example}[Non-Compositional Translation via Sub-Induction]
A simple example of non-compositionality is the translation of natural numbers based on zero, one, and addition (i.e., $N\bbc 0\bnfalt 1\bnfalt N+N$) into natural numbers based on zero and successor (i.e., $N\bbc 0\bnfalt\cn{succ}(N)$):
It is straightforward to translate zero and one compositionally:
\[\sem{0}=0 \tb\sem{1}=\cn{succ}(0)\]
Now we would like to translate \[\sem{m+n}=\sem{+}(\sem{m},\sem{n}),\] but there is no way to define $\sem{+}$ in terms of zero and successor.
Instead, we need subcases:
\[\sem{m+n}=\cas{\sem{m}\mifc n=0 \\ \cn{succ}(\sem{m})\mifc n=1 \\ \sem{(m+n_1)+n_2}\mifc n=n_1+n_2}\]
This corresponds to the usual definition of addition, i.e., $\sem{+}$, by induction.
\end{example}

Other common examples of non-compositional translations are
\begin{compactitem}
 \item several important logical theorems such as
  \begin{compactitem}
   \item cut elimination, which is the translation from sequent calculus with cut to sequent calculus without cut,
   \item the deduction theorem, which is the translation from natural deduction to Hilbert calculus,
  \end{compactitem}
 \item almost anything done by an optimizing compiler, e.g., loop unrolling or function inlining,
 \item query optimization done by a database, e.g., turning a WHERE of a join into a join of WHEREs,
 \item almost all translations between natural languages, e.g., when words are ambiguous and a different translation must be chosen for the same word based on the context (The introduction of richer intermediate structures like ASTs and functions as values into the translation can recover some compositionality here).
\end{compactitem}

Typical sources of non-compositionality in formal language translations are:
\begin{compactitem}
 \item A case in the translation function requires subcases which inspect the $e_i$ and treat them differently.
 \item A case in the translation function requires subcases which translate an expression differently based on the context in which it occurs.
 \item The translation function requires nested inductions, i.e., a case in the translation function (which is already inductive) requires a sub-induction on one of the sub-expressions.
 \item The semantic prefix is not fixed but depends on the translated object, i.e, the top-level case of the translation scans through the entire argument $X$ to collect all occurrences of a particular feature and then custom-builds the semantic prefix of $\sem{X}$.
\end{compactitem}
See also Ex.~\ref{ex:noncompositional}.

Such non-compositional translations are undesirable for multiple reasons:
\begin{compactitem}
 \item The implementation is more complicated and error-prone.
 \item Reasoning about the translation is more difficult.
 \item The custom semantic prefix can be large.
\end{compactitem}

But most importantly, non-compositional translations are less robust.
Firstly, if we add a production to the syntax, a compositional translation is easy to extend: just add a case to the translation.
But a non-compositional translation may additionally require a new subcase wherever subcases/subinductions are used.
Moreover, if a custom semantic prefix is used, its definition may have to be amended, at least it must be rechecked.

Secondly, in practice there are two sources of complex expressions: the ones already mentioned in the language, and the ones used later for other reasons.
For example, some complex expressions occur already \emph{statically} in the definition of a vocabulary $V$.
But others might be appear \emph{dynamically} later, e.g., when talking about $V$, proving properties of $V$, or running queries on $V$.
Thus, the definition of $V$ and the use of complex expressions are decoupled: $V$ is defined statically once and for all, and complex expressions relative to $V$ can be created and used dynamically.
But if a custom semantic prefix is used, only the static occurrences inside $V$ can be considered for building the prefix.
Thus, it is not possible to translate expressions dynamically unless the semantic prefix is extended all the time while $V$ is used.