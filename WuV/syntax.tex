\section{The 4 Layers of a Formal Language}

Languages are designed, implemented, and evaluated in layers.
Fig.~\ref{fig:layers} shows the most common design.

\begin{compactitem}
\item First, we define a grammar and implement it using a set of inductive types.
This is called the AST (abstract syntax tree) of the language.
A parser translates string into the corresponding syntax trees, i.e., objects of those inductive types.
Objects conforming to the grammar are called \textbf{well-formed}.
\item Most processing of the language is implemented by inductive functions that traverse the AST.
In particular, a context-sensitive inference system is used for type-checking.
The type-checking functions take well-formed objects and return booleans, to indicate which are \textbf{well-typed}.
This defines a context-sensitive sublanguage.
That is the language we are really interested in.
\item Now the semantics can be implemented: it traverses the AST with the precondition that the input in well-typed.
If that succeeds, the objects are called \textbf{well-defined}.
Ideally, our language satisfies the invariant that for all well-typed objects, the semantics exists.
But in some languages well-definedness proofs may be necessary.
\item Finally humans evaluate the language by using it. They may report design flaws that feed back into the AST design.
\end{compactitem}

At every layer we detect different issues.
Fig.~\ref{fig:errors-ded} and~\ref{fig:errors-comp} give some examples.

\begin{figure}[hbt]
\begin{tabular}{l|lll|l}
Layer & Specified by & Implemented by & Possible error \\\hline
Context-Free Syntax & grammar & parser & not derivable from grammar & \\
Context-Sensitive Syntax & inference system & type checker & symbols not used as declared & KRP \\
Semantics & \multicolumn{2}{l}{inference system, interpretation, or translation} & undefined semantics & \\
\hline
Pragmatics & human preferences & human judgment & not useful & not KRP \\
\end{tabular}
\caption{The 4 Layers of a Language}\label{fig:layers}
\end{figure}

\begin{figure}[hbt]
\begin{tabular}{l|lll}
Layer & Expression & Issue & Explanation \\\hline
Context-Free Syntax & $1/$ & syntax error & argument missing\\
Context-Sensitive Syntax & $1/"2"$ & typing error & argument has wrong type\\
Semantics & $1/0$ & run-time error & undefined semantics \\
Pragmatics & $1/1$ & code review comment & unnecessarily complex expression \\
\end{tabular}
\caption{Typical Errors by Layer in a Programming Language}\label{fig:errors-comp}
\end{figure}

\begin{figure}[hbt]
\begin{tabular}{l|lll}
Layer & Expression & Issue & Explanation \\\hline
Context-Free Syntax & $\forall x$ & not well-formed & body missing\\
Context-Sensitive Syntax & $\forall x.P(y)$ & not well-typed & $y$ not declared\\
Semantics & the $x\in \N$ such that $x<0$ & not well-defined & no such $x$ exists \\
Pragmatics & $\exists x.x\neq x$ & inconsistent & no model exists\\
\end{tabular}
\caption{Typical Errors by Layer in Logic}\label{fig:errors-ded}
\end{figure}

%non-termination

\section{Context-Free Syntax}

Abstractly, context-free syntax is specified using grammars.
Concretely, it is implemented using inductive types.

In the sequel, we will start with the standard definitions and then make a series of variation to each of these definitions until they become equivalent.
The intended equivalence is as follows:
\begin{center}
\begin{tabular}{l|l}
CFG & IDT \\
\hline
non-terminal & type \\
production & constructor \\
non-terminal on left of production & return type of constructor \\
non-terminals on right of production & arguments types of constructor \\
terminals on right of production & notation of constructor\\
words derived from non-terminal $N$ & expressions of type $N$
\end{tabular}
\end{center}

\begin{remark}[Chomsky Hierarchy]
The Chomsky hierarchy of grammars has been quite influential and is still taught a lot in introductory courses.
But it has little relevance for modern language design.
\begin{compactitem}
\item CH-0, regular grammars: These are equivalent to regular expressions and finite automata. The latter two are usually better ways to work with these languages than a grammar.
\item CH-1, context-free grammars: There are what we consider here.
\item CH-2, context-sensitive grammars: Context-sensitive languages are very important.
  But it is more practical to use context-free grammars and then define a context-sensitive subset later in a different way.
\item CH-3, unrestricted grammars: These are Turing-complete and are only relevant theoretically.
\end{compactitem}
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Context-Free Grammars}

We start with the usual definition:

\begin{definition}[Context-Free Grammar]
Given a set $\Sigma$ of characters (containing the terminal symbols), a \textbf{context-free grammar} consists of
\begin{compactitem}
\item a set $N$ of names called \textbf{non-terminal symbols}
\item a set of \textbf{productions} each consisting of
 \begin{compactitem}
  \item an element of $N$, called the \textbf{left-hand side}
  \item a word over $\Sigma\cup N$, called the \textbf{right-hand side}
 \end{compactitem}
\end{compactitem}
\end{definition}

\begin{example}\label{ex:cfs}
Let $\Sigma=\{0,1,+,\cdot,\doteq,\leq\}$.
We give a grammar for arithmetic expressions and formulas about them:
\begin{commgrammar}
\gprod{E}{0}{}\\
\galtprod{1}{}\\
\galtprod{E+E}{}\\
\galtprod{E\cdot E}{}\\
\gprod{F}{E\doteq E}{}\\
\galtprod{E\leq E}{}\\
\end{commgrammar}
Here we use the BNF style of writing grammars, where the productions are grouped by their left-hand side and written with $\bbc$ and $\bnfalt$.
We have $N=\{E,F\}$.
\end{example}

First, we give a name to each production of a CFG:

\begin{definition}[Context-Free Grammar with Named Productions]
Given a set $\Sigma$ of characters (containing the terminal symbols), a \textbf{context-free grammar} consists of
\begin{compactitem}
\item a set $N$ of names called \emph{non-terminal symbols}
\item a set of \emph{productions} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{left-hand side}
  \item a word over $\Sigma\cup N$, called the \textbf{right-hand side}
 \end{compactitem}
\end{compactitem}
\end{definition}

\begin{example}
The grammar from above with names written to the right of each production
\begin{commgrammar}
\gprod{E}{0}{zero}\\
\galtprod{1}{one}\\
\galtprod{E+E}{sum}\\
\galtprod{E\cdot E}{product}\\
\gprod{F}{E\doteq E}{equality}\\
\galtprod{E\leq E}{lessOrEqual}\\
\end{commgrammar}
This is not common BNF anymore.
\end{example}

Then we add base types to the productions:

\begin{definition}[Context-Free Grammar with Named Productions and Base Types]
Given a set $\Sigma$ of characters (containing the terminal symbols) and a set $T$ of names (containing the base types allowed in productions), a \textbf{context-free grammar} consists of
\begin{compactitem}
\item a set $N$ of names called \emph{non-terminal symbols}
\item a set of \emph{productions} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{left-hand side}
  \item a word over $\Sigma\cup T\cup N$, called the \textbf{right-hand side}
 \end{compactitem}
\end{compactitem}
\end{definition}

The intuition behind base types is that we commonly like to delegate some primitive parts of the grammar to be defined elsewhere.
A typical example are literals such as numbers $0, 1, 2,\ldots$: We could give regular expression syntax for digit-strings.
Instead, it is nicer to just assume we have a set of base types that we can use to insert an infinite set of literals into the grammar.

\begin{example}
Let $Nat$ be the type of natural numbers and let $T=\{Nat\}$.
Then we can improve the grammar from above as follows:
\begin{commgrammar}
\gprod{E}{Nat}{literal}\\
\galtprod{E+E}{sum}\\
\galtprod{E*E}{product}\\
\gprod{F}{E\doteq E}{equality}\\
\galtprod{E\leq E}{lessOrEqual}\\
\end{commgrammar}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inductive Data Types}\label{sec:idt}

We start with the usual definition:

\begin{definition}[Inductive Data Type]\label{def:idt}
Given a set of names $T$ (containing the types known in the current context), an \emph{inductive data type} consists of
\begin{compactitem}
 \item a name $t$, called the \textbf{type},
 \item a set of \textbf{constructors} each consisting of
 \begin{compactitem}
  \item a name $n$
  \item a list of elements of $T\cup\{t\}$, called the \textbf{argument} types
 \end{compactitem} 
\end{compactitem}
\end{definition}

\newcommand{\cons}[2]{\mathtt{#1}\,\kw{of}\,\fold{*}{#2}}
\newcommand{\indtype}[2]{#1\,=\,\fold{\;\tb|\tb}{#2}}
\newcommand{\consnot}[3]{\mathtt{#1}\,\kw{of}\,\fold{*}{#2}\,\#\,#3}
\newcommand{\consn}[2]{\mathtt{#1}\,\#\,#2}

\begin{example}
Let $Nat$ be the type of natural numbers and $T=\{Nat\}$.
We give an inductive type for arithmetic expressions:
\[
\indtype{E}{\cons{literal}{Nat}, \cons{sum}{E,E}, \cons{product}{E,E}} \\
\]
Here we use ML-style notation for inductive data types, which separates constructors by $|$ and writes them as \texttt{name of argument-type-product}.
\end{example}

First we generalize to mutually inductive types:

\begin{definition}[Mutually Inductive Data Types]
Given a set $T$ of names (containing the types known in the current context), a family of \textbf{mutually inductive data type} consists of
\begin{compactitem}
 \item a set $N$ of names, called the \textbf{types},
 \item a set of \emph{constructors} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{return type}
  \item a list of elements of $N\cup T$, called the \textbf{argument} types
 \end{compactitem} 
\end{compactitem}
\end{definition}

\begin{example}
We extend the type definition from above by adding a second type for formulas.
Thus, $N=\{E,F\}$.
\[\mathll{
\indtype{E}{\cons{literal}{Nat}, \cons{sum}{E,E}, \cons{product}{E,E}} \\
\indtype{F}{\cons{equality}{E,E}, \cons{lessOrEqual}{E,E}}
}\]
\end{example}

%It may look $T$ and $\Sigma$ correspond to each other.
%But that is not true, we need to add them to each definition.

Then we add notations to the constructors:

\begin{definition}[Mutually Inductive Data Types with Notations]
Given a set $\Sigma$ of characters (containing the terminal symbols) and a set $T$ of names (containing the types known in the current context), a family of \textbf{mutually inductive data type with notations} consists of
\begin{compactitem}
 \item a set $N$ of names, called the \textbf{types},
 \item a set of \emph{constructors} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{return type}
  \item a list of elements of $T\cup N$, called the \textbf{argument} types
  \item a word over the alphabet $\Sigma\cup T\cup N$ containing the argument types in order and only elements from $\Sigma$ otherwise, called the \textbf{notation} of the constructor
 \end{compactitem} 
\end{compactitem}
\end{definition}

The intuition behind notations is that it can get cumbersome to write all constructor applications as $Name(arguments)$.
It is more convenient to attach a notation to them such as 

\begin{example}
We extend the type definitions from above by adding notations to each constructor.
We use the set $\Sigma=\{+,\cdot,\doteq,\leq\}$ as terminals in the notations.
\[\mathll{
\indtype{E}{\consnot{literal}{Nat}{Nat}, \consnot{sum}{E,E}{E+E}, \consnot{product}{E,E}{E\cdot E}} \\
\indtype{F}{\consnot{equality}{E,E}{E\doteq E}, \consnot{lessOrEqual}{E,E}{E\leq E}}
}\]
Here we write the constructors as \texttt{name of argument-type-product \# notation}.
It is easy to see that this has introduced redundancy: we can infer the argument types from the notation.
So we can just drop the argument types:
\[\mathll{
\indtype{E}{\consn{literal}{Nat}, \consn{sum}{E+E}, \consn{product}{E\cdot E}} \\
\indtype{F}{\consn{equality}{E\doteq E}, \consn{lessOrEqual}{E\leq E}}
}\]
\end{example}


\subsection{Merged Definition}

With the variation from above we have arrived at the following equivalence:

\begin{theorem}
Given a set $\Sigma$ of characters and a set $T$ of names, the following notions are equivalent:
\begin{compactitem}
\item a family of mutually inductive data types in the context of types $T$ with notations using characters from $\Sigma$,
\item a context-free grammar with named productions, terminal symbols from $\Sigma$, and base types $T$.
\end{compactitem}
\end{theorem}
\begin{proof}
The key idea is that
\begin{compactitem}
 \item the types and constructors of the former correspond to the non-terminals and productions of the latter
 \item for each constructor-production pair
  \begin{compactitem}
   \item the right-hand side of the latter corresponds to the notation of the former,
   \item the argument types of the former correspond to the non-terminals occurring on the right-hand side of the latter.
  \end{compactitem}
\end{compactitem}
\end{proof}

In implementations in programming languages, we often drop the notations.
Instead, those are handled, if needed, by special parsing and serialization functions.

However, in an implementation, it is often helpful to additionally give names to each argument of a production/constructor.
That yields the following definition:

\begin{definition}[Context-Free Syntax]
Given a set $\Sigma$ of characters and a set $T$ of names, a context-free syntax consists of
\begin{compactitem}
 \item a set $N$ of names, called the \textbf{non-terminals/types},
 \item a set of \textbf{productions/constructors} each consisting of
 \begin{compactitem}
  \item a name
  \item an element of $N$, called the \textbf{left-hand side/return type}
  \item a sequence of objects, called the \textbf{right-hand side/arguments} which are one of the following
   \begin{compactitem}
    \item an element of $\Sigma$
    \item a pair written $(n:t)$ of a name $n$, called the \textbf{argument name}, and an element $t\in T\cup N$ called the \textbf{argument type}.
   \end{compactitem}
 \end{compactitem}
\end{compactitem}
\end{definition}

\begin{example}
Using ad hoc language to write the constructors, our example from above as a context-free syntax could look as follows:
\[\mathll{
\indtype{E}{\consn{literal}{(value:Nat)}, \consn{sum}{(left:E)+(right:E)}, \consn{product}{(left:E)\cdot (right:E)}} \\
\indtype{F}{\consn{equality}{(left:E)\doteq (right:E)}, \consn{lessOrEqual}{(left:E)\leq (right:E)}}
}\]
\end{example}

\subsection{Contexts}

We assume a context-free language $l$.

\begin{definition}[Context]
A \textbf{context} is a list of
\begin{compactitem}
\item grammar terminology: productions $N\bbc x$
\item type terminology: declarations $x:N$
\end{compactitem}
where each $x$ is a unique name and each $N$ is non-terminal symbol.

The $x$ are called \textbf{variables}.
\end{definition}

\begin{remark}
Sometimes the grammar itself has specific productions for contexts and variables.
In that case, we speak of \emph{meta-variable contexts} and \textbf{meta-variables} to distinguish them from those of the language.
\end{remark}

\begin{definition}[Expressions in Context]
Given a context $\Gamma$, a word $E$ derived from non-terminal $N$ that may additionally use the productions of the context is called an \emph{expression of type} $N$ \emph{in context} $\Gamma$.

We write this as $\Gamma\vdash_l E:N$.
\end{definition}

\begin{definition}[Substitution]
Given two contexts $\vdash_l \Gamma$ and $\vdash_l\Delta$, a \textbf{substitution} $\gamma$ from $\Gamma=x_1:N_1,\ldots,x_n:N_n$ to $\Delta$ is a list $x_1:=e_1,\ldots,x_n:=e_n$ where every $e_i$ is an expression of type $N_i$ in context $\Delta$ (i.e., $\Delta\vdash_l e_i:N_i$).

We write this as $\Delta\vdash_l \gamma:\Gamma$ or as $\vdash_l \gamma:\Gamma\to \Delta$.
\end{definition}

\begin{definition}[Substitution Application]\label{def:subapp}
Given an expression $\Gamma\vdash_l E:N$ and a substitution $\vdash_l\gamma:\Gamma\to\Delta$ where $\Gamma=x_1:N_1,\ldots,x_n:N_n$ and $\gamma=x_1:=e_1,\ldots,x_n:=e_n$, we write $E[\gamma]$ for the result of replacing every $x_i$ in $E$ with $e_i$.
\end{definition}

\begin{theorem}
If $\Gamma\vdash_l E:N$ and $\vdash_l\gamma:\Gamma\to\Delta$, then $\Delta\vdash_l E[\gamma]:N$.
\end{theorem}

We often want to substitute only a single variable $x:N$ even though $E$ may be defined in a larger context $\Gamma$.
This is often written $E[x:=e]$.
That is just an abbreviation for $E[\gamma]$, where $\gamma$ contains $x:=e$ as well as $y:=y$ for every other variable $y$ of $\Gamma$.

\begin{notation}
There are many different notations for substitutions that are in common use.
Examples include $E(x)$, $E[x:=e]$, $E[x/e]$, or $[e/x]E$.
Usually authors of a paper or textbook briefly mention their preferred notation briefly at the beginning.
\end{notation}

\section{Context-Sensitive Syntax}

It is common to define a language as the set of words that can be produced from the syntax, i.e., from a distinguished non-terminal (the start symbol) of the context-free grammar.
It is common to define a context-sensitive language as a special case: the set of words that can be produced from a context-sensitive grammar.

This is, however, not helpful in practice.
While the above remains the official definition of what the context-sensitive languages are, all practical definitions are entirely different.
In fact, context-sensitive grammars are virtually never used to define a specific language.
Instead, more restrictive definitions are used that capture more properties of practical languages.

A typical context-sensitive syntax includes the following:
\begin{compactitem}
 \item a context-free syntax,
 \item a distinguished non-terminal symbol $\ThySym$, whose words are called \textbf{vocabularies},
 \item a set of distinguished non-terminal symbols $\ExpSym$, whose words are called $\ExpSym$-\textbf{expressions},
 \item a unary predicate $\wft{\Theta}$ on vocabularies $\Theta$,
 \item for every vocabulary $\Theta$ and every $\ExpSym$, a unary predicate $\wff{\Theta}{E}$ on $\ExpSym$-expressions $E$.
\end{compactitem}
In case of $\wft{\Theta}$, we call $\Theta$ \textbf{well-formed}.
In case of $\wff{\Theta}{E}$, we call $E$ a \textbf{well-formed} $\ExpSym$-expression over $\Theta$.

\begin{remark}[Terminology]
%``language system'' is not a standard term. We usually just say ``language''.
``Well-formed $\ExpSym$-expression over $\Theta$'' can be a mouthful.
Therefore, it is common to simply say that $E$ is an $\ExpSym$-expression, or that $E$ is a $\Theta$-expression, and expect readers to fill in the details.

It is also common to give the non-terminal $\ExpSym$ names, such as ``term'', ``type'', ``formula'', or (confusingly) ``expression''.
Then we simply say ``term'' instead of ``term-expression'' and so on.
\end{remark}

\begin{example}\label{ex:css}
We extend the context-free language from Ex.~\ref{ex:cfs} to a context-sensitive one as follows:
\begin{commgrammar}
\gcomment{Vocabularies}\\
\gprod{Voc}{\rep{Decl}}{list of declarations}\\
\gcomment{Declarations}\\
\gprod{Decl}{id:\rep{Type}\to Type}{typed function symbols}\\
\galtprod{id:\rep{Type}\to Form}{typed predicate symbols}\\
\gcomment{Types}\\
\gprod{Type}{Nat \bnfalt String}{base types}\\
\gcomment{Expressions}\\
\gprod{Expr}{0\bnfalt 1\bnfalt Expr+Expr \bnfalt Expr*Expr}{as before}\\
\galtprod{id(\rep{Expr})}{application of a function symbol}\\
\gcomment{Formulas}\\
\gprod{Form}{Expr\doteq Expr \bnfalt Expr\leq Expr}{as before}\\
\galtprod{id(\rep{Expr})}{application of a predicate symbol}\\
\end{commgrammar}

$Voc$ is the special non-terminal $\ThySym$. The special non-terminals for expressions are $Type$, $Expr$, and $Form$.

The well-formedness predicates check that every identifier is used according to its declaration.
\end{example}

\paragraph{Vocabularies}
The vocabularies are typically lists of typically named declarations.
They introduce the names that can be used to form expressions.
The expression kinds almost always include formulas.

Often declarations contain additional expressions, most importantly types or definitions.
In general, all expressions may occur in declarations, but many language systems do not use all of them.

Very different names are used for the vocabularies in different communities.
The following table gives an overview:

\begin{center}
\begin{tabular}{l|ll}
Aspect & vocabulary $\Theta$ & expression kinds $\ExpSym$ \\
\hline
Ontologization  & ontology & individual, concept, relation, property, formula \\
Concretization & database schema & cell, row, table, formula \\
Computation & program & term, type, object, class, \ldots \\
Logic & theory & term, type, formula, \ldots \\
Narration & dictionary & phrases, sentences, texts \\
\end{tabular}
\end{center}

In practice, it is most useful to think of a language system as family of languages: one language (containing the expressions) for every vocabulary.

\paragraph{Standard Libraries}
The standard library is a fixed vocabulary that is always present.
Shifting operators from the grammar into the standard library is a common technique to simplify the language.

\begin{example}\label{ex:css2}
We can rewrite Ex.~\ref{ex:css} into a much shorter grammar, in which all name-like productions are declared in the standard library:
\begin{commgrammar}
\gprod{Voc}{\rep{Decl}}{list of declarations}\\
\gprod{Decl}{id:\rep{Type}\to Type}{typed function symbols}\\
\galtprod{id:\rep{Type}\to FORM}{typed predicate symbols}\\
\galtprod{id:TYPE}{type symbols}\\
\gprod{Type}{id}{reference to a type symbol}\\
\gprod{Expr}{id(\rep{Expr})}{application of a function symbol}\\
\gprod{Form}{id(\rep{Expr})}{application of a predicate symbol}\\
\end{commgrammar}

The standard library is the vocabulary containing the following declarations:
\begin{compactitem}
\item type symbols: $Nat: TYPE$, $String:TYPE$
\item function symbols: $0:Nat$, $1:Nat$, $sum: Nat\,Nat\to Nat$, $product: Nat\,Nat\to Nat$,
\item predicate symbols: $equals: Nat\,Nat\to FORM$, $lesseq: Nat\,Nat\to FORM$
\end{compactitem}
\end{example}

\section{Implementation}

Data structures for context-free syntax can be implemented systematically in all programming languages.
But, depending on the style of the language, they make drastically different.
Below we give the two most important paradigms as examples.

In each case, context-sensitive syntax is implemented as a set of traversal functions over these data structures that return the well-formedness property, i.e., a boolean.

\subsection{Functional Programming Languages}

In a function programming language, inductive data types are a primitive feature.
However, notations and named arguments are not available.
So helper functions must be used.

The basic recipe is as follows:
\begin{compactitem}
\item The types and constructors (without the notations and named arguments) are implemented as family of mutually inductive data types.
\item For each argument of each constructor, a partial projective function is defined.
\item A set of mutually recursive string rendering functions are defined, one for each constructor, that implement the notations.
\end{compactitem}

\begin{example}
We define our example syntax in ML.

First the inductive types (assuming a type $Nat$ already exists in the context):
\[\mathll{
\kw{data}\, \indtype{E}{\cons{literal}{Nat}, \cons{sum}{E,E}, \cons{product}{E,E}} \\
\kw{and}\,\indtype{F}{\cons{equality}{E,E}, \cons{lessOrEqual}{E,E}}
}\]

Now the projection functions:
\[\mathll{
 \kw{fun}\; \mathtt{literal\_value}(\mathtt{literal}(v)) = SOME\; v \\
 |\tb \mathtt{literal\_value}(\_)= NONE \\
 \kw{fun}\; \mathtt{sum\_left}(\mathtt{sum}(x,\_)) = SOME\; x \\
 |\tb \mathtt{sum\_left}(\_)= NONE\\
 \kw{fun}\; \mathtt{sum\_right}(\mathtt{sum}(\_,x)) = SOME\; x \\
 |\tb \mathtt{sum\_right}(\_)= NONE
}\]
and so on for each constructor argument.

Finally, the string rendering functions (assuming a function $natToString$ already exists in the context):
\[\mathll{
 \kw{fun}\; \mathtt{E\_toString}(\mathtt{literal}(v)) = natToString\;v \\
 |\tb \mathtt{E\_toString}(\mathtt{sum}(x,y))= \mathtt{E\_toString}(x) + "+" + \mathtt{E\_toString}(y) \\
 |\tb \mathtt{E\_toString}(\mathtt{product}(x,y))= \mathtt{E\_toString}(x) + "\cdot" + \mathtt{E\_toString}(y) \\
 \kw{and}\; \mathtt{F\_toString}(\mathtt{equality}(x,y)) = \mathtt{E\_toString}(x) + "\doteq" + \mathtt{E\_toString}(y)\\
 |\tb \mathtt{F\_toString}(\mathtt{lessOrEqual}(x,y)) = \mathtt{E\_toString}(x) + "\leq" + \mathtt{E\_toString}(y)
}\]
\end{example}

Because ML has inductive data types as primitives, pattern-matching on our syntax comes for free.
We will get back to that when defining the semantics.

\subsection{Object-Oriented Programming Languages}

In a object-oriented programming language, inductive data types are not available.
Therefore, they must be mimicked using classes.
On the positive side, this supports arguments names, and notations are a bit easier.

The basic recipe is as follows:
\begin{compactitem}
\item Each type is implemented as an abstract class.
\item Each constructor of type $t$ is implemented as a concrete class that extends the abstract class $t$.
\item The arguments names and type of each constructor $c$ are exactly the argument names and types of the class $c$.
The constructor arguments are stored as fields in the class.
\item The abstract classes require a \texttt{toString} method, which is implemented in every concrete class according to its notation.
\end{compactitem}


\begin{example}
We define our example syntax in a generic OO-language somewhat similar to Scala.\footnote{We could use Java or C++ here. But their concrete syntax makes this less clear than it could be. It is straightforward to refine the syntax into that of any specific OO-language.}

In particular, we assume that the sy

\begin{lstlisting}
abstract class E {
  def toString: String
}
class literal extends E {
  field value: Nat
  constructor (value: Nat) {
    this.value = value
  }
  def toString = value.toString
}
class sum extends E {
  field left: Nat
  field right: Nat
  constructor (left: E, right: E) {
    this.left = left
    this.right = right
  }
  def toString = left.toString + "+" + right.toString
}
class product extends E {
  field left: Nat
  field right: Nat
  constructor (left: E, right: E) {
    this.left = left
    this.right = right
  }
  def toString = left.toString + "$\cdot$" + right.toString
}

abstract class F {
  def toString: String
}
class equality extends E {
  field left: Nat
  field right: Nat
  constructor (left: E, right: E) {
    this.left = left
    this.right = right
  }
  def toString = left.toString + "$\doteq$" + right.toString
}
class product extends E {
  field left: Nat
  field right: Nat
  constructor (left: E, right: E) {
    this.left = left
    this.right = right
  }
  def toString = left.toString + "$\leq$" + right.toString
}
\end{lstlisting}
\end{example}

Because OO-languages do not have inductive data types as primitives, pattern-matching on our syntax requires awkward switch statements.
We will get back to that when defining the semantics.

\subsection{Combining Paradigms}\label{syn:impl:scala}

The Scala language combines ideas from functional and OO-programming.
That makes its representation of context-free syntax particularly elegant.

In Scala, the constructor arguments are listed right after the class name.
These are automatically fields of the class, and a default constructor always exists that defines those fields.
That gets rid of a lot of boilerplate.

If we want to make those fields public (and we do because those are the projection functions, we add the keyword \texttt{val} in front of them.
But even that is too much boilerplate. So Scala defines a convenience modifier: if we put \texttt{case} in front of the classes corresponding to constructors of our syntax, Scala puts in the \texttt{val} automatically.
It also generates a default implementation of \texttt{toString}, which we have to override if we want to implement notations, too.
Finally, Scala also generates pattern-matching functions so that we can pattern-match in the same way as in ML.

Then our example becomes (as usual, assuming a class \texttt{Nat} already exists):

\begin{lstlisting}
abstract class E {
  def toString: String
}
case class literal(value: Nat) extends E {
  override def toString = value.toString
}
case class sum(left: Nat, right: Nat) extends E {
  override def toString = left.toString + "+" + right.toString
}
case class product(left: Nat, right: Nat) extends E {
  override def toString = left.toString + "$\cdot$" + right.toString
}

abstract class F {
  def toString: String
}
case class equality(left: Nat, right: Nat) extends E {
  override def toString = left.toString + "$\doteq$" + right.toString
}
case class lessOrEqual(left: Nat, right: Nat) extends E {
  override def toString = left.toString + "$\leq$" + right.toString
}
\end{lstlisting}

\subsection{Issues Regarding Equivalence}

There are a few subtle issues.

\paragraph{Equality}
Forming the same value twice should yield the same result, e.g., $\mathtt{literal}(1)=\mathtt{literal}(1)$.
In ML-style inductive types, that is automatic.
In OO-style class-based implementations, we have $\kw{new}\,\mathtt{literal}(1)=\kw{new}\,\mathtt{literal}(1)$ only if we override the equality check.
Scala allows adding the keyword \kw{case} to a class representing a constructor/production.
Among other things, it allows dropping the \kw{new} and overrides the equality method automatically. 

\paragraph{Null Value}
Object-oriented language automatically provide the \cn{null} value for every class.
Therefore, if we implement a context-free syntax via classes, we always get one spurious value that should not exist.
Programmers have to use programming discipline to avoid using it.
Occasionally, static analysis tools can non-null annotations to check this discipline.

\paragraph{Sealing}
When we seal a context-free syntax, it is impossible to add new constructors to it.
That guarantees implementors of inductive functions that they know all possible cases and can therefore pattern-match on them.
An inductive function on an unsealed datatype can fail when new constructors are added later.

ML-style inductive data types are always sealed.
OO-style class-based implementations are never sealed.
In Scala, the keyword \kw{sealed} can be added to an abstract class --- it forbids any constructors other than the ones present in the same source file.

\subsection{Implementing Context-Sensitive Checking}

We extend the implementation of our example language in Scala from Sect.~\ref{syn:impl:scala} to cover the context-sensitive language from Ex.~\ref{ex:css}.

\begin{lstlisting}
abstract class NonTerminal {
    def print(): String
}

// At the toplevel, every formal language has a vocabulary, which is a list of declarations
// (we can skip the abstract class if the non-terminal has exactly one production)
case class Vocabulary(decls: List[Declaration]) extends NonTerminal {
  // print all declarations and concatenate with separator ", "
  def print() = decls.map(_.print()).mkString(", ")
}

// there can be many different kinds of declarations
abstract class Declaration extends NonTerminal {
  // most declarations have a name, but not all e.g., axioms
  def nameO: Option[String]
}
case class FunctionSymbolDeclaration(name: String, inputs: List[Type], output: Type) extends Declaration {
  def nameO = Some(name)
  // print as "name: inputs -> output"
  def print() = name + ": " + inputs.map(_.print()).mkString(" ") + " -> " + output.print()
}

abstract class Type extends NonTerminal
abstract class Expr extends NonTerminal
abstract class Form extends NonTerminal

case class Nat() extends Type {
  def print() = "Nat"
}
case class String() extends Type {
  def print() = "String"
}

// for every declaration, there is a production to refer to the declared identifer, i.e., to use it in an expression
case class FunctionSymbolReference(name: String, arguments: List[Expr]) extends Expr {
  // print as "name(arguments)"
  def print() = name + "(" + arguments.map(_.print()).mkString(",") + ")"
}

case class Sum(left: Expr, right: Expr) extends Expr {
  def print() = left.print() + "+" + right.print()
}  

case class Product(left: Expr, right: Expr) extends Expr {
  def print() = left.print() + "*" + right.print()
}  

case class Zero() extends Expr {
  def print() = "zero"
}

case class One() extends Expr {
  def print() = "one"
}

case class Equals(left: Expr, right: Expr) extends Form {
  def print() = left.print() + "=" + right.print()
}  

case class LessEq(left: Expr, right: Expr) extends Form {
  def print() = left.print() + "<=" + right.print()
}
\end{lstlisting}

Then we implement a context-sensitive type-checker for it.
It implements the wff-predicates from Def.~\ref{def:css}.

A type-checker traverses the AST. Any such traversal consists of
\begin{compactitem}
 \item one function for every non-terminal (mutually recursive)
 \item one case for every constructor
 \item one recursive call for every constructor argument
\end{compactitem}

We first do the simple case where we only have a single base type.
In this case, the base types in the inputs and outputs of the symbols are always the same and only the number of arguments must be checked.
In that case, the type-checker is a traversal that
\begin{compactitem}
 \item takes a Vocabulary as an extra argument - that's the context that makes everything context-sensitive
 \item returns Boolean
\end{compactitem}

\begin{lstlisting}
object TypeChecker {
  
  // check every declaration relative to the vocabulary before it
  // this is the same for virtually every formal language
  def check_Voc(voc: Vocabulary): Boolean = {
    var seenSofar: List[Declaration] = List()
    voc.decls.forall {d =>
       // check the current declaration relative to the ones seen before
       val r = check_Decl(Vocabulary(seenSofar), d)
       // append it to the list of seen declarations
       seenSofar = seenSofar ::: List(d)
       r
    }
  }
  
  def check_Decl(voc: Vocabulary, d: Declaration): Boolean = {
    d match {
      case FunctionSymbolDeclaration(n,ins,out) =>
        if (voc.decls.exists(e => e.nameO == Some(n)))
          false // in practice, throw "name already defined" error
        else {
           ins.forall(i => check_Type(voc, i)) && check_Type(voc, out)
         }
    }
  }

  // compared the the lecture, I've removed function types for simplification
  def check_Type(voc: Vocabulary, tp: Type) = {
    tp match {
      case Nat() => true
      case String() => true
    }
  }

  // In our simplified variant, where we only check arity, expressions are well-typed if
  // all subexpressions are and applications use the right number of arguments.
  // We will expand on that later.
  def check_Expr(voc: Vocabulary, n: Expr): Boolean = {
     n match {
       case Zero() => true
       case One() => true
       case Sum(l,r) => check_Expr(voc, l) && check_Expr(voc, r)
       case Product(l,r) => check_Expr(voc, l) && check_Expr(voc, r)
       
       case FunctionSymbolReference(s, args) =>
         // for the reference to the identifiers in the vocabulary, we have to lookup the identifier in the vocabulary
         voc.decls.find(d => d.nameO == Some(s)) match {
           case None => false // in practice, throw "symbol not declared" error
           case Some(d) => d match {
             case FunctionSymbolDeclaration(_, ins,out) =>
               if (ins.length != args.length) {
                 false // in practice, throw "wrong number of argumetns" error
               } else {
                 // check every argument recursively
                 args.forall(a => check_Expr(voc, a))
               }
           }
         }
     }
  }
  
  def check_Form(voc: Vocabulary, f: Form): Boolean = {
    f match {
      case Equals(l,r) => check_Expr(voc, l) && check_Expr(voc, r)
      case LessEq(l,r) => check_Expr(voc, l) && check_Expr(voc, r)
    }
  }
}
\end{lstlisting}

The following code shows a simple test of the type checker:
\begin{lstlisting}
object Test {
    def main(args: Array[String]) = {
        // because we do not have a parser, we need to build some objects manually for testing
        
        // an example vocabulary
        // fib: Nat -> Nat
        val fibDecl = FunctionSymbolDeclaration("fib", List(Nat()), Nat())
        val voc = Vocabulary(List(fibDecl))
        System.out.println(voc.print()) 
        // check the vocabulary
        System.out.println(TypeChecker.check_Voc(voc))  

        // example expressions relative to that vocabulary
        val x = Sum(Zero(), One())
        val y = FunctionSymbolReference("fib", List(x))
        val z = LessEq(x,y)
        System.out.println(z.print())
        // check the expressions      
        System.out.println(TypeChecker.check_Form(voc, z))
    }
}
\end{lstlisting}