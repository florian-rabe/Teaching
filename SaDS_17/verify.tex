Two sub-fields can be distinguished.

\paragraph{Provers for Dynamic Logics}
These tools are specifically designed to prove correctness of programs.

The programming language may be designed for the tool, or the tool may be designed to work with an existing programming language.
The used programming languages tend to be simpler and more cleanly designed than mainstream languages.
Often they arise as well-behaved fragments of existing languages.

They tend to focus on the programming aspects, allowing for user-facing syntax that looks as similar as possible to typical programs.
They tend to try to hide the logic aspects from the user as much as possible.
Ideally, they only require the user to ascribe loop invariants, class invariants, and pre/postconditions for functions.

Then automated provers can perform the proving in the background.
Usually no proof object is produced.

Examples are Why3\footnote{\url{http://why3.lri.fr/}} \cite{why3} (for a custom imperative language) and KeY\footnote{\url{https://www.key-project.org/}} \cite{key} (for Java-like languages).
    % https://www.lri.fr/~marche/MPRI-2-36-1/


\paragraph{General Purpose Proof Assistants}
These are general-purpose tools for proving formulas.
Many use formal systems that are strong enough to be Turing-complete.

They tend to focus on \emph{pure} programs, i.e., programs where evaluation does not require an environment.
These are usually functional programming languages emphasizing inductive data types and recursive functions.

All proofs are interactive, and the proof assistant supplies as much automation as possible.
The result is a proof term that can be verified independently.

Examples are Coq \cite{coq}, Agda \cite{agda}, PVS \cite{pvs}, and the HOL family \cite{holsemantics}, which includes Isabelle \cite{isabelle}, HOL Light \cite{hollight}, and HOL \cite{hol}.
Major verifications have been done of a C compiler\footnote{\url{http://compcert.inria.fr/}} using Coq\footnote{\url{https://coq.inria.fr/}} and of the L4 micro kernel\footnote{\url{http://ts.data61.csiro.au/projects/TS/l4.verified/}} in Isabelle\footnote{\url{https://isabelle.in.tum.de/}}.

\section{Proof Rules for Dynamic Logic}

We can focus on \emph{partial} correctness because termination can be established separately using termination orders.

Partial correctness requires proving formulas that contain the box operator $\abox{t}{Q}$.
Its proof rules requires a separate case for every possible $t$.

There is quite some variety in choosing the proof rules.
We only give some examples.

Often simplifying assumptions can be made to reduce the difficulty.
Most importantly, it is desirable to restrict side effects as much as possible.
Therefore, we make some restrictions that will be in place throughout this section.

\paragraph{Isolating Side Effects}
One useful restriction is to say that any term that has side effects must occur by itself.

It is straightforward to rewrite programs into equivalent ones that satisfy these conditions.
For example, if $t$ has a side effect, we can rewrite $f(t)$ into the equivalent $\{\aval{x}{A}{t};f(x)\}$.
Exhaustively applying such rewrites eventually yields assignments like $\aval{x}{\Unit}{\aprint{t}}$, which can be rewritten into simply $\aprint{t}$.

That way we can assume that many terms are pure, e.g.,
\begin{compactitem}
 \item definition of a value or mutable variable,
 \item right hand side of an assignment,
 \item conditions in if and while commands,
 \item arguments of a function application, operator application, or print command.
\end{compactitem}

\paragraph{Reasoning Must Be Pure}
A non-optional restriction is that the logical formulas may not have any side-effects at all.
Otherwise, the reasoning itself would affect the thing we want to reason about. That would not make sense.

So in $\abox{P}{F}$, only $P$ but not $F$ may contain assignments, loops, I/O, or control flow commands.
Of course, $P$ may contain booleans again, which may or may not be pure.

Similarly, all annotations such as loop invariants, class invariants, pre/postconditions of functions, and termination orderings must be pure terms.

\subsection{Sequencing}

The rule for $s;t$ first applies $s$, then $t$:

\[\rul{\iscons{}{\Gamma}{}{\abox{s}{\abox{t}{F}}}}{\iscons{}{\Gamma}{}{\abox{s;t}{F}}}
\]

To prove the soundness of this rule, we show that for all states $q$ if all premises hold in $q$, then so does the conclusion.
That is straightforward because the two formulas are equivalent.

\subsection{Branching}

The rule for if commands distinguishes two cases:

\[\rul{\iscons{}{\Gamma}{}{C\impl \abox{t}{F}} \tb \iscons{}{\Gamma}{}{\neg C\impl \abox{t'}{F}}}
      {\iscons{}{\Gamma}{}{\abox{\aifelseI{C}{t}{t'}}{F}}}\]

(Recall that $C$ is assumed to be pure.)

\subsection{Loops}

The rule for while-loops invents a loop invariant $I$:

\[\rul{\iscons{}{\Gamma}{}{I} \tb \iscons{}{\Gamma^*}{}{(I\wedge C)\impl \abox{t}{I}} \tb \iscons{}{\Gamma^*}{}{(I\wedge \neg C) \impl F}}
      {\iscons{}{\Gamma}{}{\abox{\awhileI{C}{t}}{F}}}\]

(Recall that $C$ and $I$ are assumed to be pure.)

Here $\Gamma^*$ is like $\Gamma$ but all definitions of mutable variables whose value may change while executing $t$ are dropped.
This is necessary because every iteration of the loop may change the state of those variables, and we cannot make any assumptions about their values other than the loop invariant.\footnote{This clarification was added after presenting the rule in the lecture. It is not critical for the homeworks.}


%To prove the soundness of this rule, we use induction: we show inductively for all $n$ that $I$ holds are $n$ iterations of the loop.
%The first two premises are exactly the base and step case of the induction. 
%The third premise yields the desired formula $F$ if and whenever the while-loop terminates.

\subsection{Assignments}

Reasoning about assignments is very difficult: we have to keep track of the current value of each mutable variable.
Evaluation delegated that to the environment.
But a proof is static: there is no environment because we are reasoning about what evaluation would do for an arbitrary, unknown initial state.

One simple way is to replace the definition of $x$ in the context.
Effectively, that means we use the context as the environment that holds the current values of the variables.

\[\rul{t \text{ pure} \tb \iscons{}{\Gamma,\amval{x}{A}{t[x/t']},\Gamma'}{}{F}}
      {\iscons{}{\Gamma,\amval{x}{A}{t'},\Gamma'}{}{\abox{x:=t}{F}}}\]
Because $x$ may occur in $t$, we have to substitute its current value for it.

The above rule is sound in many simple situations.
Given the frequency of assignments and the cost of substitution (the entire formula must be traversed), this is not an efficient solution.

Moreover, it easily runs into difficulties when aliasing is allowed, e.g., when using pointers or classes.
If multiple parts of the program can access the same memory location, each assignment must be communicated to all other branches of the derivation.
The above rule does not allow for that because all assignments modify only the context in the current branch.

\subsection{Function Application}

For a function application, we create a local variable for the argument and evaluate the body:

\[\rul{\iscons{}{\Gamma}{}{\abox{\{\aval{x}{A}{a};t\}}{F}}}
      {\iscons{}{\Gamma}{}{\abox{(\alam[A]{x}{t})\,a}{F}}}
\]

\paragraph{Function Specifications}
We do not need a special rule for the application of named functions: those can be replaced with their definition using the usual rules for reasoning about equality.
However, it is useful to have a special rule for when a named function has a specification.

\[\rul{\iscons{}{\Gamma}{}{PfQ} \tb
       \iscons{}{\Gamma}{}{P(t_1,\ldots,t_n)} \tb
       \iscons{}{\Gamma^z}{}{}{Q(t_1,\ldots,t_n,z)\impl F}}
     {\iscons{}{\Gamma}{}{\abox{z:=f(t_1,\ldots,t_n)}{F}}}\]

Here $PfQ$ abbreviates that $f$ satisfies the specification given by $P$ and $Q$.
That has to be verified once and for all when $f$ is declared.

$\Gamma^z$ is the variant of $\Gamma$ where the definition of $z$ is dropped.\footnote{This clarification was added after presenting this rule in the lecture. It is not critical for the homeworks.}
That is necessary because we do not know the exact value of $z$ afterwards---we only know that the postcondition holds.
Therefore, the postcondition must be so strong that we can continue the proof without explicitly knowing the value of $z$.
Most of the time the postcondition is strong enough to uniquely determine the value of $z$, but not always.

If $f$ is not recursive and has no separate return-command, the following rule is sound to prove $PfQ$:

\[\rul{\iscons{}{
          \begin{array}{c}\Gamma,
          \afunI[A]{f}{x_1:A_1,\ldots, x_n:A_n}{t},\\
          \aval{x_1}{A_1}{},\ldots,\aval{x_n}{A_n}{},
          \aval{x}{A}{}\end{array}
      }{}{
          P(x_1,\ldots,x_n)\impl \abox{x:=t}{Q(x_1,\ldots,x_n,x)}
      }
    }{\iscons{}{\Gamma,\afunI[A]{f}{x_1:A_1,\ldots, x_n:A_n}{t}}{}{PfQ}}\]

If $f$ is recursive, $PfQ$ has to be assumed in the premise of the rule.

If $f$ uses a return-command, we need a more complex rule, because we cannot use the assignment $x:=t$.

\subsection{Pure Terms}

All other term constructors from Ch.~\ref{sec:sd:typetheory} can simply be ignored because their evaluation has no side effects:

\[\rul{\iscons{}{\Gamma}{}{F}}
      {\iscons{}{\Gamma}{}{\abox{x}{F}}}
\tb\tb
\rul{l \text{ is literal} \tb \iscons{}{\Gamma}{}{F}}
      {\iscons{}{\Gamma}{}{\abox{l}{F}}}
\tb\tb
\rul{\iscons{}{\Gamma}{}{F}}
      {\iscons{}{\Gamma}{}{\abox{\alam[A]{x}{t}}{F}}}
\]

\subsection{Input/Output}

Reasoning about input and output is very difficult and depends on various subtleties.

\paragraph{Unknown Environment}
For example, if the logic does not allow every referring to the special locations $IN$ and $OUT$ of the environment, then formulas can never talk about future reads or past prints.
That makes it easy to give rules that reason about arbitrary states of the environment:

\[\rul{\iscons{}{\Gamma,\aval{x}{\Int}{}}{}{F}}
      {\iscons{}{\Gamma}{}{\abox{x:=\aread}{F}}}
\tb\tb
\rul{t\text{ pure}\tb\iscons{}{\Gamma}{}{F}}
      {\iscons{}{\Gamma}{}{\abox{\aprint{t}}{F}}}
\]

If $F$ cannot talk about upcoming reads, then the result of a read could be anything.
Thus, the only way for $\abox{x:=\aread}{F}$ to hold is if $F$ holds for arbitrary $x$.

Similarly, if $F$ cannot talk about previous prints, the side effect of printing can be ignored.
Thus, if $t$ is pure, $\abox{\aprint{t}}{F}$ holds iff $F$ holds.

\paragraph{Reasoning about Side Effects}
The assumption that the logic cannot talk about the environment is often too restrictive: after all, the communication with the environment is what can be observed by the user.
Therefore, we want our specifications to be able to talk about what will be printed and how the program will respond differently depending on what is read.

That requires additional language features in the logic, which then require more complex proof rules.

\subsection{Local Declarations}

Local value declarations can simply be moved into the context:

\[\rul{t\text{ pure}\tb\iscons{}{\Gamma,\aval{x}{A}{t}}{}{\abox{P}{F}}}
      {\iscons{}{\Gamma}{}{\abox{\{\aval{x}{A}{t}; P\}}{F}}}
\]

Local variable declarations can be moved to the context, too. They will be modified later when assignments are made or when the definition is dropped because it is not statically known anymore.
\[\rul{t\text{ pure}\tb\iscons{}{\Gamma,\amval{x}{A}{t}}{}{\abox{P}{F}}}
      {\iscons{}{\Gamma}{}{\abox{\{\amval{x}{A}{t}; P\}}{F}}}
\]


\subsection{Inductive Data Types}

Inductive data types do not introduce major practical complications.
We only have to add induction rules to reason about inductive functions.

However, they introduce theoretical complications: completeness may be out of reach (see below).

For every inductive data type
\[\adata{a}{c_1(A_1),\ldots,c_n(A_n)}\]
we add a set of rules.
The intuition behind the rules is that the possible values of type $a$ are exactly the terms that can be formed from the constructors.
Intuitively, the constructors are jointly injective (two different terms have different values) and jointly surjective (all values are terms).

\begin{example}[Natural Numbers]\label{ex:typetheory:nat2}
For the natural numbers from Ex.~\ref{ex:typetheory:nat}, the induction axioms are exactly the Peano axioms:
\begin{compactitem}
 \item Different terms have different values:
   \begin{compactitem}
    \item $0$ is not a successor
    \item the successor function is injective
   \end{compactitem}
 \item All values are terms: if $P(n)$ holds for all terms (i.e., $P(0)$ and $P(n)\impl P(succ(n))$, then $\forall x.P(x)$.
\end{compactitem}
\end{example}

\paragraph{Constructors Are Jointly Injective}
In the special case, where all constructors are unary, we add the following axioms
 \[\iscons{}{\aval{x}{A_i}{},\aval{y}{A_j}{}}{}{c_i(x)\neq c_j(y)} \tb\mfor i,j= 1,\ldots,n\mand i\neq j\]
 \[\iscons{}{\aval{x}{A_i}{},\aval{y}{A_i}{}}{}{x\neg y\impl c_i(x)\neq c_i(y)} \tb \mfor i=1,\ldots,n\] 
For other constructors, we adapt the axioms accordingly.

The first axiom ensures that different constructors produce different values.
The second axiom makes all constructors injective.

These axioms are sometimes called \emph{no confusion axioms} because they make sure that no two terms can be confused with each other.

\paragraph{Constructors Are Mutually Surjective}
The second group consists of what is usually called \emph{the} induction axiom.
It is also sometimes called \emph{no junk axiom} because it makes sure that there are no other values around.

It can be stated as an axiom.
But it is more intuitive and more practical to state it as a rule.

For simplicity, we assume that all constructors take two arguments: a recursive argument $y$, whose type is the inductive type $a$, and a non-recursive argument $x$, whose type $A$ does not mention $a$.
This distinction is necessary because the induction hypothesis only applies to he recursive arguments.
In that simple case, the rule is:
\[
\rul{\overbrace{\iscons{}{\aval{x}{A}{},\aval{y}{a}{}, p: \aproof{F(y)}}{}{F(c_i(x,y))}}^{\mfor i=1,\ldots,n\mwith c_i(A,a)}}
    {\iscons{}{x:a}{}{F(x)}}
\]

For any $F:a\to\Bool$, if $F(c_i(x,y))$ holds whenever the induction hypothesis holds for the recursive arguments, then $F$ holds for arbitrary values.

%Here $F^{A_i}:A_i\to \Bool$ is the induction hypothesis at type $A$.
%It depends on the argument type $A_i$.
%This can be defined by induction for any $A_i$, but the following cases often suffice in practice:
%\begin{compactitem}
% \item $A_i$ is a base type: $F^{A_i}(x)=\true$ (i.e., no induction hypothesis),
% \item $A_i=a$ is the inductive data type: $F^a(x)=F(x)$ (i.e., the induction hypothesis).
%\end{compactitem}

\begin{example}[Continuing Ex.\ref{ex:typetheory:nat2}]\label{ex:typetheory:nat3}
For the data type of natural numbers, the induction axioms are the following

\begin{compactitem}
\item Different constructors yield different terms
 \[\iscons{}{y:nat}{}{zero()\neq succ(y)}\]
\item Constructors are injective
 \[\iscons{}{x:nat,y:nat}{}{x\neq y\impl succ(x)\neq succ(y)}\]
\item Induction rule
\[
\rul{\iscons{}{}{}{F(zero)}\tb\iscons{}{x:nat, p: \aproof{F(x)}}{}{F(succ(x))}}
    {\iscons{}{x:nat}{}{F(x)}}
\]
\end{compactitem}
\end{example}

\subsection{Abstract Data Types}

Abstract data types (i.e., classes) introduce major theoretical and practical problems.
Even for the simplified variants presented in Sect.~\ref{sec:typetheory:adt} and even if class invariants are known, it is an open problem to give elegant proof rules.

We discuss the most critical problems.

\subsubsection{Referential Transparency}

Every new instance of a class allocates a memory location, and future method calls may change the values at these memory locations.
That makes certain intuitively sound rules unsound.

For example, reflexivity is not sound anymore: $\anew{A}{}\neq \anew{A}{}$.
That is because every new instance occupies a fresh memory location.
And even if those happen to contain the exact same values at some point, they may differ in the future.

\subsubsection{Equality}

Two instances of the same class are \textbf{observationally equal} if the same finite sequence of method calls on them will produce the same results.
Two instances are \textbf{reference equal} if they point to the same memory location.
Two instances are \textbf{state equal} if they agree in the current values of all their private variables.
Clearly, reference equality implies state equality, which in turn implies observational equality.

For example, consider
\begin{acode}
\aclass{A}{x:\Int}{}{
 \afun[\Int]{next}{}{
   x := x+1\\
   x \modop 2
 }
}\\
i := \anew{A}{0}\\
j := \anew{A}{2}
\end{acode}
Because $x$ is a private variable, the instances $i$ and $j$ are observationally equivalent, e.g., $i.next.next == j.next.next == 0$.
But $i$ and $j$ are not state equal, let alone reference equal.

Observational equality is undecidable.
Therefore, most object-oriented programming languages simply implement $==$ as reference equality.
This is often desirable.
For example, we usually want to have $\anew{A}{0}\neq \anew{A}{0}$ even though the two instances are state equal.

But observational equality could be very helpful for reasoning: if $x$ and $y$ are observationally equal, proving a theorem about $x$ also yields the corresponding theorem about $y$.
That is because there is no formula that can distinguish between them.

\subsubsection{Instance Lifetime}

Creating a new instance of a class is similar to making a named declaration in that they introduces a new object that can be used later on.
But the latter is static: we know statically where names are declared and in which parts of the program a name is visible.
Instance creation is dynamic: We know only at run-time which objects are created and who is still using an object.

Therefore, evaluation needs to use the environment to store all instances (usually stored in what is called the \emph{heap}), and explicit deallocation or garbage collection are needed to remove instances that are not used anymore.

Mimicking this process in proof rules is extremely difficult.
For example, consider the following local declaration:
\[\{\aval{x}{A}{\anew{A}{}} ;\; f(x)\}\]
Normally, we want to throw away the local declaration of $x$ after reasoning about the result of $f(x)$.
Indeed, at that point the \emph{name} $x$ can never be accessed again because it is out of scope.
However, the instance that $x$ referred to may still be around: $f(x)$ may have assigned $x$ to some variable that has greater scope.

Because it is undecidable when an instance is not used anymore, we would have to keep all instances around forever.
That would explode the size of the reasoning problems.

The solution has to involve lifetime annotations that clarify where and when an instance can be accessed.
But we do not have a good solution for that yet.

\subsubsection{Aliasing}

Aliasing occurs when multiple names refer to the same memory location.
For example, consider the following two code snippets:

\begin{multicols}{2}
\begin{acode}
x:\Int = 1\\
y:\Int = x\\
x = x+1\\
\aprint{x==y}
\end{acode}
\columnbreak

\begin{acode}
x:A = \anew{A}{}\\
y:A = x\\
x.a = x.a+1\\
\aprint{x.a==y.a}
\end{acode}
\end{multicols}
for some class $A$ with a mutable field $a:\Int$.

The left snippet does not introduce aliasing: it will print $\false$.
This is because the assignment $y:=x$ copies the current value of $x$ into the newly allocated memory location for $y$.

But the right snippet introduces aliasing: it will $\true$.
This is because the assignment $y:=x$ makes $y$ an alias that also points to the memory location of $x$.

In the presence of aliasing, any part of the program may be able to access and alter variables in all other parts of the code.
Effectively, all mutable fields of all class instances become global variables.
That explodes the complexity of reasoning.

Even worse, when handling an assignment, we do not know which variables will be affected: as assignment $x.a:=1$ must affect not only $x.a$ but also $y.a$ for every alias $y$ of $x$.
And it is undecidable whether $y$ and $x$ are aliases.

A good solution will likely include a requirement that all mutable fields of a class must be private.
That offers a chance to keep track of aliasing locally by annotating the class with invariants and its methods with specifications.

\paragraph{Other Sources of Aliasing}
Just like classes with mutable fields, C-style pointers can be used to create aliasing.
But here the problem is even worse: \emph{any} variable is potentially subject to aliasing, not just the ones related to classes.
(And because all variables are mutable in C, that really includes everything.)

\section{Soundness and Completeness}

\paragraph{Theorems}
Because all formulas are pure, their semantics is just a function from states to $\{\true,\false\}$.

\begin{definition}[Theorem]
A pure term $t:\Bool$ is a \textbf{theorem} if it is $\true$ in every state.
\end{definition}


\paragraph{Soundness}
To formally state the soundness theorem, we need some additional definitions:

\begin{definition}[Soundness]
A set of rules is \textbf{sound} if all provable formulas are theorems.
\end{definition}

In particular, if a set of rules is sound, if every rule's conclusion is a theorem whenever its premises are.

We can now prove the soundness of all rules described above.

\paragraph{Completeness}

It is easy to define completeness:

\begin{definition}[Completeness]
A set of rules is \textbf{complete} if every theorem is provable.
\end{definition}

But it is very difficult to make a dynamic logic complete.

First of all, we have to find a complete calculus for the logic even in the absence of any programs.
That can already be impossible if we allow function types, a built-in type of natural numbers, or inductive data types.
Ultimately, this is due to G\"odel's incompleteness theorem.

Therefore, we usually aim at relative completeness: the rules for programs should be such that they yield a complete calculus if there is/were one for the pure logic.
Dynamic logics are usually relatively complete.
The basic idea of the proof is to show that exhaustively applying the rules (in the presence of sufficient annotations like loop invariants) reduces $\abox{P}{F}$ to a formula without the box operator.

However, our set of rules is not complete: we would have to add some rules or make more restrictions about what programs $P$ we allow.

\section{Verification in Pure Logics}

General purpose proof assistants use pure logics.
There are no assignments, while-loops, I/O, control flow commands, or classes.
Thus, evaluation does not need an environment and can never lead to run-time errors.

Reasoning in these logics can be carried out entirely without the formulas $\abox{P}{F}$.
Any program $P$ is just a term that evaluates to some other term $P'$ in such a way that $P==P'$ is a theorem.
Therefore, the usual equality reasoning is sufficient to reason about programs.
We only have to add induction so that we can reason about inductive data types and inductive functions.

Recursive functions are the only source of non-termination.
And non-termination is the only source of abnormal behavior (i.e., there are no run-time errors).
To prove termination, a variety of families of termination orderings are used, most importantly the subterm ordering.

Because proving is so much easier, we can afford to be a lot more rigorous about the proofs and step through the derivations explicitly.
We give some examples below.

\subsection{Inductive Functions}

Consider the inductive data type of natural numbers:
\begin{acode}
\adata{nat}{zero,succ(nat)}
\end{acode}

An example of a recursive function is given by addition:
\begin{acode}
\afun[nat]{plus}{x:nat,y:nat}{
  \amatch{x}{\acase{zero}{y},\acase{succ(n)}{succ(plus(n)(y))}}
}
\end{acode}
We will write $x+y$ instead of $plus(x,y)$.

The termination of this function follows immediately from the subterm ordering: the arguments of the recursive calls are always subterms of the original inputs.

\subsection{Inductive Proofs}

Our goal is to prove the following theorems
\[\oftype{}{x:nat}{zero\_left:\aproof{zero+x==x}}\]
\[\oftype{}{x:nat}{zero\_right:\aproof{x+zero==x}}\]
\[\oftype{}{x:nat,y:nat}{succ\_left:\aproof{succ(x)+y==succ(x+y)}}\]
\[\oftype{}{x:nat,y:nat}{succ\_right:\aproof{x+succ(y)==succ(x+y)}}\]
Together, these allow statically reasoning about all possible additions where at least one argument is given by a constructor application.

The proof $zero\_left$ is easy to obtain.
The term $zero+x$ evaluates directly to $x$ even if the value of $x$ is unknown.
Similarly, the proof $succ\_left$ is easy.
\medskip

However, the evaluation of $x+zero$ gets stuck: if $x$ is unknown, we cannot evaluate the pattern-match.
Therefore, to show $x+zero==x$, we need to give a proof by induction.

We apply the induction rule to the proof goal $x+zero==x$, which yields two subgoals:
\[
\rul{\iscons{}{}{}{zero+zero==zero} \tb \iscons{}{x:nat, p: \aproof{x+zero==x}}{}{succ(x)+zero==succ(x)}}
    {\iscons{}{x:nat}{}{x+zero==x}}
\]

The first subgoal can be discharged immediately because it contains no free variables: $zero+zero==zero$ evaluates to $\true$.

The second subgoal cannot be fully evaluated because it still contains a free variable.
But we can still try to evaluate it step-by-step and see how far we get:
First we expand the definition of $plus$ to obtain (after $\alpha$-renaming the bound variables for readability):
\[(succ(x)+zero==succ(x))\tb\rewrites\tb \big(\alam{a}{\alam{b}{\amatchI{a}{\acase{zero}{b},\acase{succ(n)}{succ(n+b)}}}}\big)(succ(x))(zero) == succ(x)\]
Now the evaluation rule for function application ($\beta$-reduction) yields
\[\rewrites\tb \amatchI{succ(x)}{\acase{zero}{zero},\acase{succ(n)}{succ(n+zero)}} == succ(x)\]
Then the evaluation rule for inductive data types (pattern-match reduction) yields
\[\rewrites\tb succ(x+zero) == succ(x)\]
Now we can use the induction hypothesis to substitute for $x+zero$, which yields
\[\rewrites\tb succ(x) == succ(x)\]
Finally, we can make $x$ disappear using the reflexivity of $==$:
\[\rewrites\tb \true\]

Thus, we were able to evaluate the second subgoal as well even though it contained a free variable.
\medskip

For the proof of $succ\_right$, we use induction as well.
The induction rule is given by
\[
\rul{\begin{matrix}{c}\iscons{}{}{}{zero+succ(y)==succ(zero+y)} \\ \iscons{}{x:nat, p: \aproof{x+succ(y)==succ(x+y)}}{}{succ(x)+succ(y)==succ(succ(x)+y)}\end{matrix}}
    {\iscons{}{x:nat}{}{x+succ(y)==succ(x+y)}}
\]

Again the first subgoal evaluates easily to $\true$ because $zero+n$ evaluates to $n$, which yields $succ(y)==succ(y)$.

In the second subgoal, we again expand the definition of $plus$ and reduce the function application and the pattern-match.
That yields
\[succ(x)+succ(y)==succ(succ(x)+y) \rewrites succ(x+succ(y))==succ(succ(x)+y)\]
Now we can substitute with the induction hypothesis on the left and evaluate $succ(x)+y\rewrites succ(x+y)$ on the right to obtain
\[succ(succ(x+y))==succ(succ(x+y))\]
which evaluates to $\true$.
\medskip
