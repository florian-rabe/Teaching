After lists, trees are the next most important data structure in computer science.
They can be seen as a generalization of lists where the elements are not arranged in a row, but branching is allowed.

\section{Specification}

\subsection{General Trees}

There are many equivalent definitions.
The easiest is by graphical example: A tree is something that looks like

\tikzstyle{node}=[circle,draw]
\begin{center}
\begin{tikzpicture}
\node[node] (0) at (0,0) {};
\node[node] (00) at (-2,-1) {};
\node[node] (01) at (0,-1) {};
\node[node] (02) at (2,-1) {};
\node[node] (000) at (-2,-2) {};
\node[node] (010) at (-1,-2) {};
\node[node] (011) at (1,-2) {};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (0) -- (02);
\draw[arrow] (00) -- (000);
\draw[arrow] (01) -- (010);
\draw[arrow] (01) -- (011);
\end{tikzpicture}
\end{center}

A more formal definition is this:

\begin{definition}[Tree]\label{def:ad:tree}
A \textbf{tree} is a connected directed graph in which
\begin{compactitem}
 \item there is exactly one node (called the \textbf{root}) with in-degree $0$,
 \item all other nodes have in-degree $1$.
\end{compactitem}
\end{definition}
Here we already used the more general concept of graphs, which we define formally in Sect.~\ref{sec:ad:graphs}.

Talking about the shape and parts of a tree can be confusing.
Therefore, we introduce some vocabulary that helps us:

\begin{definition}[Parts of a Tree]\label{def:ad:treeaux}
For every edge from $p$ to $c$, we call $p$ the \textbf{parent} of $c$ and $n$ a \textbf{child} of $p$.
Thus, the root has no parent; every non-root node has exactly one parent.
A node may have any number of children.
A node with $0$ children is called a \textbf{leaf}.
A node that is neither the root nor a child is called an \textbf{inner node}.

For every path from $a$ to $d$, we call $a$ an \textbf{ancestor} of $d$ and $d$ a \textbf{descendant} of $a$.
Thus, all nodes are descendants of the root
Every node is an ancestor/descendant of itself; a \textbf{proper} ancestor/descendant of $n$ is an ancestor/descendant that is not $n$ itself.

The number of proper ancestors of $n$ is called the \textbf{depth} of $n$.
Thus, the root has depth $0$.

For a node $n$, the descendants of $n$ form a tree again, which has root $n$.
It is called the \textbf{subtree} at $n$.

A path from the root to a leaf is called a \textbf{branch}.
Thus, every leaf $l$ is part of exactly one branch, whose length is the depth of $l$.
The length of the longest branch(es) is called the \textbf{height} of the tree.
\end{definition}

\begin{remark}
Contrary to all these tree metaphors, computer scientists prefer drawing trees with the root at the top and the leafs at the bottom.
\end{remark}

Def.~\ref{def:ad:tree} only defines the abstract shape of trees.
But trees are only useful if we can store some data in each node.
For example, the following is a tree of integers:

\begin{center}
\begin{tikzpicture}
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {3};
\node[node] (01) at (0,-1) {6};
\node[node] (02) at (2,-1) {1};
\node[node] (000) at (-2,-2) {0};
\node[node] (010) at (-1,-2) {6};
\node[node] (011) at (1,-2) {5};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (0) -- (02);
\draw[arrow] (00) -- (000);
\draw[arrow] (01) -- (010);
\draw[arrow] (01) -- (011);
\end{tikzpicture}
\end{center}

Once we store data in a tree, we have to be a bit more careful: the order of children matters now.
For example, the above tree of integers is different from the tree of integers below even both are based on the same tree.

\begin{center}
\begin{tikzpicture}
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {3};
\node[node] (01) at (0,-1) {6};
\node[node] (02) at (2,-1) {1};
\node[node] (000) at (-2,-2) {0};
\node[node] (010) at (-1,-2) {5};
\node[node] (011) at (1,-2) {6};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (0) -- (02);
\draw[arrow] (00) -- (000);
\draw[arrow] (01) -- (010);
\draw[arrow] (01) -- (011);
\end{tikzpicture}
\end{center}

Keeping track of the order makes the definition more complicated.
The following definition is one out of several equivalent formal definitions:

\begin{definition}[Trees over a Set]\label{def:ad:labeledtree}
The set $Tree[A]$ contains the \textbf{trees over the set} $A$.
Such a tree over $A$ consists of
\begin{compactitem}
 \item a set $N$ (whose elements we call the \textbf{nodes}),
 \item a function $label:N\to A$ that maps nodes to elements of $A$ ($label(n)$ is called the \textbf{label} of $n$, it is the data stored in each node),
 \item a function $children:N\to N^*$ that maps every node to its list of children,
\end{compactitem}
such that $N$ and $children$ form a tree.
\end{definition}

\subsection{Binary Trees}

Binary trees are an important special case:

\begin{definition}[Binary Tree]\label{def:ad:bintree}
A \textbf{binary tree} is a tree in which all nodes have at most $2$ children.
If a node has $2$ children, the first and second child are called the \textbf{left} and \textbf{right} child, respectively.

Binary trees over a set are defined accordingly.

A binary tree is called \textbf{full} if all non-leaf nodes have exactly two children.
A full binary tree is called \textbf{perfect} all leafs have the same depth.
It is called \textbf{almost-perfect} if it is perfect except for missing some nodes at the deepest level as far to the right as possible.
\end{definition}

For example, the following are, from left to right, a non-full, a full but not perfect, and a perfect binary tree of integers:
\begin{center}
\begin{tikzpicture}[scale=.7]
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {3};
\node[node] (01) at (2,-1) {1};
\node[node] (010) at (1,-2) {6};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (01) -- (010);
\end{tikzpicture}
\tb\tb
\begin{tikzpicture}[scale=.7]
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {3};
\node[node] (01) at (2,-1) {1};
\node[node] (010) at (1,-2) {6};
\node[node] (011) at (3,-2) {5};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (01) -- (010);
\draw[arrow] (01) -- (011);
\end{tikzpicture}
\tb\tb
\begin{tikzpicture}[scale=.7]
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {3};
\node[node] (01) at (2,-1) {1};
\node[node] (000) at (-3,-2) {0};
\node[node] (001) at (-1,-2) {2};
\node[node] (010) at (1,-2) {6};
\node[node] (011) at (3,-2) {5};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (00) -- (000);
\draw[arrow] (00) -- (001);
\draw[arrow] (01) -- (010);
\draw[arrow] (01) -- (011);
\end{tikzpicture}
\end{center}
The middle tree would be almost-perfect if $6$ and $5$ were children of $3$ instead of $1$.

It is important to know the number of nodes in a binary tree:

\begin{theorem}\label{thm:ad:bintree}
A binary tree of height $h$ has at most $2^n$ nodes at depth $n$.
It has at most $2^{h+1}-1$ nodes in total.

If it is perfect, it has exactly $2^n$ nodes at depth $n$ and exactly $2^{h+1}-1$ nodes in total.
\end{theorem}
\begin{proof}
Exercise.
\end{proof}

In particular, the number of nodes grows exponentially with the depth.
Vice versa, we can organize $n$ nodes as a binary tree of height $\log_2 n$.
The latter property is often useful to obtain logarithmic implementations: if we organize $n$ elements in a (nearly) perfect binary tree, we can reach any element in $\log_2 n$ steps.

\subsection{Trees for Ordered Sets}

Special cases of trees are sometimes used to store sets of values for which we have a total order $O$.

\subsubsection{Heaps}\label{sec:ad:heaps}

Assume a fixed total order $O$ on $A$.

\begin{definition}[Heap]
$Heap[A,O]$ is the subset of $Tree[A]$ containing only trees in which all branches are sorted with respect to $O$.
\end{definition}

The elements of $Heap[\Z,\leq]$ are also called \textbf{min-heaps}.
The elements of $Heap[\Z,\geq]$ are also called \textbf{max-heaps}.

The left tree below is a (binary) min-heap, the right one is neither a min-heap nor a max-heap:

\begin{center}
\begin{tikzpicture}
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {12};
\node[node] (01) at (2,-1) {7};
\node[node] (010) at (1,-2) {12};
\node[node] (011) at (3,-2) {9};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (01) -- (010);
\draw[arrow] (01) -- (011);
\end{tikzpicture}
\tb\tb
\begin{tikzpicture}
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {12};
\node[node] (01) at (2,-1) {3};
\node[node] (010) at (1,-2) {12};
\node[node] (011) at (3,-2) {4};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (01) -- (010);
\draw[arrow] (01) -- (011);
\end{tikzpicture}
\end{center}

In a heap, every node is smaller (with respect to $O$) than all its descendants.
The root is always the smallest element in the heap.
That makes heaps practical for sorting.
Applications are presented in Sect.~\ref{sec:ad:heaplists}.

\subsubsection{Binary Search Trees}

Binary search trees are similar to heaps but the order property is different.
In a heap every node is smaller than both its children.
In a binary search tree, every node is greater than all its left descendants and smaller than all its right descendants.

They are discussed in Sect.~\ref{sec:ad:bst}.

\subsection{Variants}

Trees are simple enough to come up everywhere.
But they are difficult enough to defy standardization.
Contrary to, e.g., lists, the definition of tree can vary subtly across textbooks, programming language libraries, and computer scientists.

The following lists some details to watch out for when interacting with what someone else calls \emph{trees}.

\paragraph{Rooted Trees}
Some definitions speak of \emph{rooted trees}.
That is usually redundant because there are no trees without a root.

But some definitions (unlike ours) allow for trees where the root is undetermined and multiple nodes could be the root.
Then rooted trees are trees with a distinguished root node.

\paragraph{Trees vs. Labeled Trees}
We distinguish between trees, which just define the shape, and trees over a set, where the nodes are labeled with data.
Others may or may not make that distinction and may use the word \emph{tree} to refer to either concept.

\paragraph{Order of Children}
Some definition may make the nodes in a tree a \emph{set} of children instead of (as in our definition) a \emph{list}.

\paragraph{Leaf-Labeled Trees}
Our $Tree[A]$ data structure contains trees in which \emph{every} node stores data from $A$.
Occasionally, we are also interested in trees where only the \emph{leafs} are labeled.
And sometimes we need trees where inner nodes are labeled with elements of $A$ and leafs with elements of $B$.

\paragraph{Single Children in Binary Trees}
Some people will speak of binary trees if every node has $0$ or $2$ nodes (but not $1$).

When nodes with $1$ child are allowed (like in our definition), definitions may or may not distinguish whether that one child is the left or the right child.
Thus, they may consider the following trees to be the same (like in our definition) or different (which would make the definition of binary search tree in Sect.~\ref{sec:ad:bst} simpler):

\begin{center}
\begin{tikzpicture}[scale=.7]
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {3};
\node[node] (01) at (2,-1) {1};
\node[node] (010) at (1,-2) {6};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (01) -- (010);
\end{tikzpicture}
\tb
\begin{tikzpicture}[scale=.7]
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {3};
\node[node] (01) at (2,-1) {1};
\node[node] (010) at (3,-2) {6};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (01) -- (010);
\end{tikzpicture}
\end{center}

\paragraph{Properties of Binary Trees}
The properties \emph{complete}, \emph{full}, \emph{balanced}, and \emph{perfect} are all similar.
They all relate to the goal of arranging a fixed number of nodes into a tree of small height.

But their definitions vary slightly.

\paragraph{Heaps}
Some people say \emph{heap} to refer exclusively to heaps of integers.

Some people will assume that heaps are always binary trees.

%\paragraph{Binary Search Trees}
%The standard definition does not use lists.
%Instead, $BST[A,O]$ is usually just a subset of $Tree[A^?]$.
%In fact, most textbooks simply use $Tree[A]$ and label the leafs with $null$.
%
%Our definition is not standard but can be more intuitive and practical.

\section{Data Structures}

Trees can be mutable or immutable.
However, trees are mostly used to store data.
Many algorithms work with a single mutable tree and insert data into it or delete data from it over time.

We consider two different data structures and use the following as an example tree
\begin{center}
\begin{tikzpicture}[scale=.7]
\node[node] (0) at (0,0) {5};
\node[node] (00) at (-2,-1) {3};
\node[node] (01) at (2,-1) {1};
\node[node] (010) at (1,-2) {6};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (01) -- (010);
\end{tikzpicture}
\end{center}

\subsection{Using Lists}\label{sec:ad:listtree}

The simplest data structure for trees uses lists:

\begin{acode}
\aclassI{Tree[A]}{data: A,\; children: List[Tree[A]]}{}{}
\end{acode}

The example tree is represented as
\[\anew{Tree[\Z]}{5,\; \big[\anew{Tree[\Z]}{3,Nil},\;\anew{Tree[\Z]}{1, [\anew{Tree[\Z]}{6,Nil}]}\big]}\]

\subsection{Using Sibling Pointers}\label{sec:ad:pointertrees}

Some programmers or programming languages prefer a more awkward (but less memory-intensive) data structure that does not use lists.

Here every node has two pointers: one to its first child and one to its next sibling:
\begin{acode}
\aclassI{Node[A]}{data: A,\; firstChild: Node[A], nextSibling: Node[A]}{}{}
\end{acode}
For leafs, the field $firstChild$ is $null$; for the last child of a node, the field $nextSibling$ is $null$.
It would be better not to use $null$. But programmers who use this data structure usually do not mind.

The example tree is represented as
\[\anew{Node[\Z]}{5,\; \anew{Tree[\Z]}{3,null, \;\anew{Tree[A]}{1, \anew{Tree[\Z]}{6,null,null}}, null}, null}\]

\section{Applications}

The usefulness of lists and sets is self-evident because set- and list-structured data pervades all sciences.
The usefulness of trees is more questionable by comparison.
Therefore, we explain the most important applications.

\subsection{Tree-Structured Data}

Tree-structured data is not as pervasive as set- or list-structured data.
But if tree structures come up, they are often critical as a structure-giving concept.
That is because sets and lists provide only comparatively trivial structure.

\subsubsection{Hierarchic Grouping}

The most convincing application of trees comes up when recursively grouping data into subsets.
We give two examples.

\paragraph{Sectioning structure of documents}
Any document (book, paper, etc.) naturally has a sectioning structure given by its \textbf{sections}.
Here \emph{section} is the generic term; in practice, sections can be called, e.g., chapters, (sub)section, paragraphs, sentences, etc.

The sectioning structure forms a tree: the overall document is the root, and every subsection is a child of its containing section.
The root and each inner node are labeled with the their title.
They may also be labeled with other data, e.g., each element in a collection of articles can be labeled with its author(s).
The leafs (typically paragraphs or sentences) are labeled with strings that contain the actual text.
The table of contents shows the entire tree.

Almost every document description language provides an explicit means for sectioning.
For example, this includes the \texttt{hn} and \texttt{div} elements in HTML\footnote{\url{https://www.w3.org/TR/html5/}}, the \texttt{$\backslash$section} etc. commands in LaTeX, and the \texttt{text:h} and \texttt{text:p} elements in the OASIS OpenDocument standard\footnote{\url{http://docs.oasis-open.org/office/v1.2/os/OpenDocument-v1.2-os-part1.html}} for text documents.

In many documents, the sections are numbered, and the numbers provide unique identifiers for the children of a section.
Examples are software specifications and legal documents.
Often the path from the root to a node $n$ is used to identify the section at $n$. These paths are written as identifiers like $3.5.2$ for the second child of the fifth child of the third child of the root.

\paragraph{Structure of programs}
Programs can be seen as a special case of documents.
The structure into packages, classes, functions, and similar grouping declarations corresponds very closely to the sectioning structure of documents.

\paragraph{File Systems}
File systems usually employ a tree structure consisting of directories for inner nodes and files for leafs.
However, in a file system, the order of children usually does not matter. Instead, each node is labeled with a \emph{name} that all nodes with the same parent have different names.

The root of the file system tree is written \texttt{/} in Unix systems.
Paths like \texttt{/a/b} are used to describe the child named \texttt{b} of the child named \texttt{a} of the root.

\paragraph{Institutions}
Most institutions like bureaucracies or companies structure their members in a tree structure.
The root corresponds to the, e.g., president, and the path from the root to a leaf is the \emph{chain of command}.

\subsubsection{Decision Trees}

Many processes or developments can be described naturally as a sequence of decision steps.
At each step, an agent has to select one out of several options.
The agents include both software and hardware systems as well as humans.

Each node corresponds to a point in time.
The root corresponds to the start.
For every node, the children are the options among which to choose.
After making a choice, time progresses to the corresponding child, and a new decision must be made.

This yields a discrete model of time where the past is linear (the path from the root to the current node) and the future is branching (the subtree at the current node).

\subsection{Structure-Sharing for Lists}

A prefix of the list $[x_0,\ldots,x_i]$ is any list $[x_0,\ldots,x_i]$ for $i\leq n$.
A suffix is any list $[x_i,\ldots,x_n]$.
Very often we have to store a large set of lists that share common prefixes or suffixes.

If we represent all lists such that the suffixes are shared, using immutable lists automatically leads to a tree structure.
For example, consider the following lists:
\begin{acode}
a:=[]\\
\\
b:=cons(1,a)\\
c:=cons(2,a)\\
\\
d:=cons(3,b)\\
e:=cons(4,b)\\
f:=cons(5,b)\\
g:=cons(6,d)
\end{acode}
Because $cons$ for immutable lists does not trigger copying the argument list, the seven lists form a tree in memory.
Here the root is the empty list, and each list corresponds to the path from a node to the root.
Note that, contrary to the usual data structure for trees, in this tree each node points to its parent instead of the other way around.

Such lists with shared suffixes occur a lot in hierarchic data structures and recursive functions for them.

For example, when compiling or interpreting a program, we must maintain the function $Var$, which maps places $p$ in the program to the set of variables $Var(p)$ that are in scope at $p$.

Maintaining the lists $Var(p)$ for all $p$ is done most efficiently using a $T:Tree[List[V]]$ where $V$ is the type of variable declarations.
Every $p$ corresponds to a node of $T$ labeled with the variables declared locally in $p$, and the list $Var(p)$ is given by the concatenation of all lists in the path from $p$ to the root.

If, e.g., $p$ is a line inside a method $f$ inside a class $c$, then $Var(p)=Decl(f)+Decl(c)+global$ where $Decl(f)$ are the local variables declared in $f$ so far, $Decl(c)$ are the declarations of $c$, and $global$ is the list of global variables.




\subsection{Efficient Storage of Sets or Lists}

Sometimes the tree structures is an artifact of storing a set of objects efficiently.
Concretely, we store a set or a list $C$ of objects as a tree $T$.
The correspondence between $C$ and $T$ is such that every node or every leaf of $T$ is labeled with an element of $C$.

This can allow for more efficient access to an object of $C$.
For example, if $C$ has size $n$ and $T$ is a perfect binary tree, we access every object of $C$ using $O(\log n)$ steps.

Examples are heaps (see Sect.~\ref{sec:ad:heaplists}) and binary search trees (see \ref{sec:ad:bst}).

\subsection{Generic Data Description Languages}

Generic data description languages usually provide at two primitive constructors: lists and trees.
Intuitively, trees are used for hierarchically grouping and lists are used for aggregating objects.

Examples of such languages are XML and JSON.

This indicates that, in practice, all data structures can be encoded relatively conveniently by combining only lists and trees.

\section{Important Algorithms}

\subsection{Search}

Trees are often used to represent a problem.

\begin{example}
Consider a labyrinth in which some treasure is hidden.
We represent it as a tree.
The entrance is the root.
Every fork in the path is a node with multiple children---one child per direction we can go in.
Every dead end is a leaf.
One node in the tree is special because it has the treasure.

To find the treasure, we have to explore the labyrinth.
That means we have to visit every node of the tree until we find the treasure.
\end{example}

Many problems in real life can be seen as labyrinths in the sense that we have to make a series of decisions, each time choose between multiple options.

Therefore, many problems can naturally be represented as trees.
Moreover, if we do not have any special knowledge (e.g., a map leading to the treasure), the only thing we can do is systematically explore all nodes of the tree.

That is straightforward in principle, but we have to decide in which order we explore the nodes.
Two strategies are important:
\begin{itemize}
\item In Breadth-First Search (BFS), we explore nodes in increasing order of depth: first the root, then the children, then the grandchildren of the root, and so on.
We can visualize this as searching top-to-bottom (if the tree is drawn in the usual way with the root at the top).
Thus, we search the entire breadth before moving on to deeper nodes.
\item In Depth-First Search (DFS), we first explore all descendants of a node $n$ before moving on the siblings of $n$.
We can visualize this as searching left-to-right.
Thus, we search as deep as we can before moving on to the siblings.
\end{itemize}

Consider the tree below.
BFS yields abcdefg.
DFS yields abecfgd.
\begin{center}
\begin{tikzpicture}
\node[node] (0) at (0,0) {a};
\node[node] (00) at (-2,-1) {b};
\node[node] (01) at (0,-1) {c};
\node[node] (02) at (2,-1) {d};
\node[node] (000) at (-2,-2) {e};
\node[node] (010) at (-1,-2) {f};
\node[node] (011) at (1,-2) {g};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (0) -- (02);
\draw[arrow] (00) -- (000);
\draw[arrow] (01) -- (010);
\draw[arrow] (01) -- (011);
\end{tikzpicture}
\end{center}

BFS has the drawback of back-and-forth movement.
For the tree above, we have to go from a to b, back up to a and down to c, back up and down to d, then all the way back to b, so that we can go e, back up all the way to a, down to c again, and so on.
DFS is much simpler.

However, it is much more common to have a very high tree (i.e., long branches) than a very wide tree (i.e., lots of branches).
This is because we often have many decisions to make, but each decision only has a few options.
For example, many games consist of an unlimited number of turns where at each turn we have to choose from a limited number of moves.
In those situations, if DFS picks the wrong cild of the root early on, it may have to explore a huge subtree before coming back to pick the right child.

BFS is more balanced and predictable.
If we know that the probability of finding a solution becomes smaller at greater depths, BFS makes sure that we explore the most promising nodes first.

\subsubsection{Depth-First Search}\label{sec:ad:dfs}

DFS can be realized quite easily with a recursive function, especially if we use the data structure from Sect.~\ref{sec:ad:listtree}.
We use an arbitrary function $f$ as the payload, i.e., a function that is to be called at every node $n$.
For example, $f$ can check if $n$ is the needed solution or do some other work on $n$.

\begin{acode}
\afun{DFS[A]}{n: Tree[A], f:Tree[A]\to \Unit}{
  f(n)\\
  foreach(n.children, x \mapsto DFS[A](x,f))
}
\end{acode}

In this variant of DFS, $f$ acts on every node $n$ before it recurses into the children.
It is also possible to switch those two, i.e., first recurse into the children, then call $f(n)$.

\subsubsection{Breadth-First Search}\label{sec:ad:bfs}

BFS is a bit more complicated.
One way to do it is to use a queue that stores all nodes that we have already seen but not acted on yet.
That way we can avoid the back-and-forth movement.

\begin{acode}
\afun{BFS[A]}{n: Tree[A], f:Tree[A]\to \Unit}{
  needToVisit := \anew{Queue[Tree[A]]}{}\\
  enqueue(needToVisit, n)\\
  \awhile{!empty(needToVisit)}{
    n := dequeue(needToVisit)\\
    f(n) \\
    foreach(n.children, x \mapsto enqueue(needToVisit, x))
  }
}
\end{acode}

Here in every iteration of the loop, we process the next node $n$ (dequeue) and then put its children at the end of the queue.
That way all children of $n$ are guaranteed to be processed before any grandchildren of $n$.
\medskip

The above BFS-algorithm is interesting because we can easily turn it into a DFS-algorithm: all we have to do is use a stack instead of a queue.
That way all descendants of $n$ are processed before anything else.

\subsection{Min-Max Algorithm}\label{sec:ad:minmax}

Many games can be represented as trees.
Consider a $2$-player game in which the players alternate taking turns.
At every turn, a player has to choose among multple moves.
We assume there is no luck (e.g., no dice-rolling) and no hidden information (e.g., no bluffing).

We can represent all possible courses of the games in a single tree as follows:
\begin{compactitem}
 \item Every node represents a turn.
  \begin{compactitem}
    \item root: initial state
    \item nodes of even depth (including root): turn of player 1
    \item nodes of odd depth: turn of player 2
    \item leafs: terminal states (when the game is over)
  \end{compactitem}
 \item For every node $n$, the children of $n$ are the possible moves in that turn.
 \item Every branch represents a possible course of the game.
\end{compactitem}

For leafs $l$, let $score(l)\in\Z^\infty$ represent the outcome:
\begin{compactitem}
 \item $\infty$: player 1 wins
 \item positive values: player 1 is ahead
 \item $0$: draw
 \item negative values: player 2 is ahead
 \item $-\infty$: player 2 wins
\end{compactitem}
Thus, player 1 wants to maximize the result, player 2 wants to minimize it.

The min-max algorithm builds the entire tree by exploring all possible courses of the game.
Let $State$ be the type of game states.
We assume some basic functions $isTerminal:State\to\Bool$ and (for terminal states) $result:State\to\Z^\infty$ that represent the rules of the game.

Let us assume we have built the tree $game:Tree[State]$.
Then we can call the minmax algorithm with $minxmax(game,0)$ to aggregate the results of the terminal states:

\begin{acode}
\afun[\Z^\infty]{minmax}{current: Tree[State], depth: \N}{
  state := current.data \\
  \aifelse{isTerminal(state)}{
    result(state)
  }{
    childResults := map(current.children, n \mapsto minmax(n, depth+1))\\
    \aifelse{even(depth)}{
      \max(childResults)
    }{
      \min(childResults)
    }
  }
}
\end{acode}

If $minxmax(game,0)=\infty$, then player 1 has a perfect strategy to win every game.
Correspondingly for player 2.
If $minxmax(game,0)=0$, then both players have a perfect strategies to hold a draw.

In practice, the tree is usually far too big to build.
Therefore, instead of obtaining the result at terminal states, we must estimate the result at cut-off.
For example, at depth $6$, we estimate the current score using heuristic function $State\to\Z^\infty$.

This is a basic design used in artificially intelligent computer players for many games.
Many optimizations are needed to obtain strong players.

