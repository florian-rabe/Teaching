Like lists, sets are collections of objects.
Usually, these are polymorphic: $Set[A]$ contains all finite sets of elements of type $A$, just like $List[A]$ contains all lists of elements of type $A$.
The difference between sets and lists can be seen in the following diagram:

\begin{center}
\begin{tabular}{|l||l|l|}
\hline
& order matters & order does not matter \\
\hline\hline
repetition matters & lists & multisets/bags \\
repetition does not matter & unique list/sorted set & sets \\
\hline
\end{tabular}
\end{center}

Thus, sets arise from lists if we remove all repetitions and ignore the order.
For example, the sets $\{1,2,1,3\}$ and $\{3,2,1\}$ are equal, whereas the lists $[1,2,1,3]$ and $[3,2,1]$ are different.

\paragraph{Collection Data Structures}
In the following, we only consider sets.
But it is worth briefly mentioning the general case as well:
In general, we have $4$ very similar polymorphic data structures: $Set[A]$, $List[A]$, $Multiset[A]$, and $UniqueList[A]$.
These and some related ones are often called \emph{collection} data structures.

In \emph{multisets} (also called \emph{bags}) repetitions matter but not order.
An important application are databases and statistical data sets.
For example, the average of the multiset $Multiset(3,3,9)$ is $5$ whereas the average of the set $\{3,3,9\}=\{3,9\}$ is $6$.

In \emph{unique lists} (also called \emph{sorted sets} and some similar names) order matters but not repetitions.
These are not as commonly used as the other collection data structures --- often it is easier to use lists and simply remember that there are no repetitions than to implement unique lists separately.
Therefore, this data structure does not even have a standard name.

Sets can be seen as the special case of multisets where every elements occurs at most once.
Most operations on sets can be easily generalized to multisets.
For example, $Multiset(1,1,2)\cup Multiset(1,2)=Multiset(1,1,1,2,2)$.
Correspondingly, unique lists can be seen the special case of lists, where every element occurs at most once.
Most operations on lists can be easily specialized to unique lists.
For example, $UniqueList(1,3,5)+UniqueList(2,4)=UniqueList(1,3,5,2,4)$.

Several important operations are common to all collection data structures.
For example, $contains[A](x,a)$ for $a\in A$ works the same way no matter whether $x$ is a set, list, multiset, or unique list of elements of $A$.

Moreover, all collection data structures have in common that we can implement them mutably or immutably.
Thus, we actually have $2\times 2\times 2=8$ data structures organized by the three properties of repetition, order, and mutability.

\section{Specification}

The set $Set[A]$ contains the finite subsets of $A$.
It is countable if $A$ is countable.

Sets can be mutable or immutable.
The main operations for immutable sets are:

\begin{ctabular}{|l|l|l|}
\hline
function & returns & effect \\
\hline
$contains(x\in Set[A], a\in A)\in \Bool$ & $\true$ iff $a\in x$ & none \\
$insert(x\in Set[A], a\in A)\in Set[A]$ & $x\cup\{a\}$ & none \\
$delete(x\in Set[A], a\in A)\in Set[A]$ & $x\sm\{a\}$ & none \\
\hline
\end{ctabular}

% lattice operations

The main operations for mutable sets are:

\begin{ctabular}{|l|l|l|}
\hline
function & returns & effect \\
\hline
$contains(x\in Set[A], a\in A)\in \Bool$ & $\true$ iff $a\in x$ & none \\
$insert(x\in Set[A], a\in A)\in\Unit$ & nothing & $x:=x\cup\{a\}$ \\
$delete(x\in Set[A], a\in A)\in\Unit$ & nothing & $x:=x\sm\{a\}$ \\
\hline
\end{ctabular}

In both cases, we often need operations for combining and comparing sets.
We only consider the immutable case:
\begin{ctabular}{|l|l|l|}
\hline
function & returns & effect \\
\hline
$equal(x\in Set[A], y\in Set[A])\in \Bool$ & $\true$ iff $x=y$ & none \\
$union(x\in Set[A], y\in Set[A])\in Set[A]$ & $x\cup y$ & none \\
$inter(x\in Set[A], y\in Set[A])\in Set[A]$ & $x\cap y$ & none \\
$\mathit{diff}(x\in Set[A], y\in Set[A])\in Set[A]$ & $x\sm y$ & none \\
\hline
\end{ctabular}

Equality is listed explicitly here because it can be very complex.
For most data structures such as the ones for lists and trees, equality is straightforward.
This may or may not be the case for data structures for sets.

\section{Data Structures}

The complexity of data structures for sets is usually measured in terms of the size $n$ of the set.

\subsection{Bit Vectors}\label{sec:ad:vectorset}

\subsubsection{Design}
If $A$ is finite with $|A|=m$, an easy data structure for $Set[A]$ are bit vectors of length $m$ such as $Array[\Bool](m)$.
Given such a vector $a$, we put $a[i]=\true$ to represent that $i$ is in the set.

\subsubsection{Complexity}
We can implement $insert$ and $delete$ easily in $\Theta(1)$.
We can also implement $equal$, $union$, $inter$, and $\mathit{diff}$ easily in $\Theta(m)$.

A major drawback is the memory requirement: We need $\Theta(m)$ for each $x:Set[A]$, which is only feasible for small $m$.

\subsection{List Sets}\label{sec:ad:listset}

\subsubsection{Design}
If $A$ is much larger than the sets to be represented, a better data structure for $Set[A]$ is $ListSet[A]$.
It represents the set $\{a_1,\ldots,a_n\}$ as the list $[a_1,\ldots,a_n]$.
Thus, it represents sets as lists without repetition.

The operations on $ListSet[A]$ are defined in the same way as for $List[A]$ with one exception: the $insert(x,a)$ operation does nothing if $x$ already contains $a$.

\subsubsection{Complexity}
If $n$ is the size of the $ListSet$, the operations $contains$, $insert$, and $delete$ takes $\Theta(n)$.
However, higher-level operations like building a set with $n$ elements step-by-step by calling $insert$ $n$ times requires $n$ insertions and thus costs $\Theta(n^2)$.

Moreover, these operations require calls to the equality on $A$.
For example, to implement $contains(x,a)$, we have to compare $a$ to every element of $x$.
That may be easy, e.g., if $A=\Int$.
But it can be arbitrarily costly if $A$ is more complex data structure itself.

For equality, union, intersection, and difference of $x$ and $y$, we may have to compare every element of $x$ with every element of $y$.
So it may take $O(|x|\cdot|y|)$.

These operations quickly become too costly for large subsets of $A$.

\subsection{Hash Sets}\label{sec:ad:hashset}

\subsubsection{Design}
Hash sets try to combine the advantages of bit vector and list sets.
The key parameter is a function $hash:A\to \Z_m$.
This is called the hash function.

$hash$ has two purposes:
\begin{compactitem}
 \item The set $A$ is supported by a finite, managably small set $\Z_m$.
   That makes it feasible to use arrays of length $m$.
 \item The equality operation on $A$ is supported by the $O(1)$ equality on $\Z_m$.
   To check $a=a'$, we first check $hash(a)=hash(a')$.
   If false, we know $a\neq a'$; otherwise, we call the usual equality on $A$.
   That minimizes the number of times equality on $A$ is called.
\end{compactitem}

Of course, the function $hash$ will usually not be injective.
A \textbf{collision} is a pair $x,y\in A$ such that $hash(x)=hash(y)$.

A good hash function should be fast and rarely have collisions.
An (unrealistically) ideal hash function runs in $O(1)$ and the probability of $hash(x)=hash(y)$ is $1/m$.
Those two properties work against each other: For example, it is easy to be fast by always returning $0$, but that has maximally many collisions.
Vice versa, it is easy to minimize collisions by choosing $hash$ carefully, but then $hash$ may be very expensive to compute.
Thus, hash functions must make a trade-off.

For a fixed hash function $hash:A\to \Z_m$, the data structure $HashSet[A]$ is given by
\begin{acode}
\atypedef{HashSet[A]}{Array[ListSet[A]](m)}\\
\afunI{insert}{h: HashSet[A], a:A}{insert(h[hash(a)],a)}\\
\afunI{delete}{h: HashSet[A], a:A}{delete(h[hash(a)],a)}
\end{acode}
The elements of the array are called hash \textbf{buckets}.
Thus, the bucket for $i$ contains all elements of the set whose hash value is $i$.

\subsubsection{Complexity}
If $n$ is the size of the subset of $A$, the sets $h[0]$, \ldots, $h[m-1]$ have average size $n/m$.
Thus, $contains$, $insert$, and $delete$ take $n/m$ on average.
$equal$, $union$, $inter$, and $diff$ are similarly sped up.
\medskip

Asymptotically, hash sets do not beat list sets because they only spped up by a constant factor.
But that constant factor is a critical improvement.

The speed up is bigger if $m$ is bigger.
However, the memory requirement increases linearly with $m$: Even the empty subset requires $\Theta(m)$ space and $\Theta(m)$ time to initialize that space.

Optimized data structures for hash sets can dynamically choose $m$ in order to find a good trade-off.
Often users of the $HashSet$ data structure can choose the value of $m$.
That can help if they know in advance how big the subset is going to get and what kind of operations will be called.

\subsection{Binary Search Trees}\label{sec:ad:bst}

\subsubsection{Design}
If we have a total order $\leq$ on $A$, we can use binary trees to realize $Set[A]$.
The idea is that the nodes of the tree hold the elements of the set, and every node $n$ splits a range of values into two subranges: all left descendants of $n$ have smaller and all right descendants have greater values than $n$.

\begin{example}
We represent the set $\{20,29,30,33,34,37\}\sq \N$ as the following binary tree over $\N$:

\begin{center}
\begin{tikzpicture}[scale=.7]
\node[] (0) at (0,0) {$30$};
\node[] (00) at (-2,-1) {$20$};
\node[] (01) at (2,-1) {$35$};
\node[] (001) at (-1,-2) {$29$};
\node[] (010) at (1,-2) {$33$};
\node[] (011) at (3,-2) {$37$};
\node[] (0101) at (1.5,-3) {$34$};
\draw[arrow] (0) -- (00);
\draw[arrow] (0) -- (01);
\draw[arrow] (00) -- (001);
\draw[arrow] (01) -- (010);
\draw[arrow] (01) -- (011);
\draw[arrow] (010) -- (0101);
\end{tikzpicture}
\end{center}
Now we can locate an element efficiently by traversing just one branch of the tree.

Note how a binary search tree has to distinguish between the left and the right subtree even if there is only one subtree (e.g., for the nodes for $20$ and $33$).
In those cases, we have to use a dummy node as the omitted left child.
These are usually labeled with $null$ or $\bot$ and omitted when drawing the tree.
\end{example}

The formal definition is as follows:

\begin{definition}[Binary Search Trees]
$BST[A,O]$ is the subset of $Tree[A^?]$ containing only full binary trees satisfying the following properties:
\begin{compactitem}
 \item All leafs are labeled with the value $\bot$; all other nodes are labeled with elements of $A$.
 \item For every non-leaf node $n$:
  \begin{compactitem}
    \item All values in the left subtree of $n$ are strictly smaller than the one at $n$.
    \item All values in the right subtree of $n$ are strictly greater than the one at $n$.
  \end{compactitem}
\end{compactitem}
\end{definition}

The leafs hold dummy values as placeholders for elements that we may want to insert later.
When drawing binary search trees, we always omit them.
The reason for labeling all leafs with $\bot$ is a technicality: we already need dummy leafs for omitted left children; then it can be simpler to just define that all leafs are dummy nodes.

\subsubsection{Complexity}

Ideally, the binary tree is perfect or nearly perfect.
That happens when every node is labeled with the median of all the values among its descendants.

In that case, the height of the tree is logarithmic in the size $n$ of the set.
Then $contains$, $insert$, and $delete$ can be implemented in $O(\log n)$.
\medskip

But a series of random insertions and deletions may make the tree arbitrarily imperfect.
In the worst case, the tree degenerates to a list.
Therefore, a binary search tree must be rearranged from time to time.

This can be done from scratch in one big operation.
For example, we can do a depth-first traversal of the tree to obtain a sorted list of all elements.
Then we can create a perfect binary tree of height $\log n$ and put the elements into it.
Overall that takes $\Theta(n)$.

Alternatively, we can rearrange the tree incrementally.
Here we try to make minor changes to the tree after every insertion or deletion.
To keep the tree near perfect.
One way to do that is to use red-black trees (see Sect.~\ref{sec:ad:redblacktree}).

\subsection{Red-Black Trees}\label{sec:ad:redblacktree}

Red-black trees are a sophisticated variant of binary search trees.
They guarantee $\Theta(\log n)$ cost for insertion and deletion.

\subsection{Characteristic Functions}\label{sec:ad:charfun}

\footnote{This section was not part of the course.}

For every set $x:Set[A]$, we can define its characteristic function $\mathtt{Char}_x:A\to\Bool$ by
\[\mathtt{Char}_x(a) = contains(a,x)\]

In mathematics, the inverse is true as well: for every function $f:A\to \Bool$, we can define the set $\{x: A|f(x)\}$.
However, we can usually not implement that set in programming languages.
That is easy to see: bit vectors, list sets, hash sets, or binary search trees can only ever represent \emph{finite} sets.
But functions $f:A\to\Bool$ can easily represent \emph{infinite} sets as well.
For example, \[\alam[\String]{x}{\true}\tb:\tb\String\to \Bool\] is the characteristic function of the infinite set of all strings.

Therefore, characteristic functions are a representation of sets that is more expressive than all of the above.
If we want to represent infinite sets, characteristic functions are the only option.

All set operations on characteristic functions can be implemented in $O(1)$ with the following exceptions:
\begin{compactitem}
 \item $contains$ must call the characteristic function. There is no way to predict how long that will take.
 \item It is impossible to implement $equal$ correctly if $A$ is infinite.
 \item Operations that require finiteness such as computing the size of a set cannot be implemented.
\end{compactitem}
