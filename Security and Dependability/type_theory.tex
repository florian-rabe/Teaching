\section{Common Structure}\label{sec:sd:typetheory:common}
% polymorphism?

Formal systems such as type theories, logics, and programming languages share the same basic structure.

\subsection{Objects}

The objects of a formal system consist of \textbf{syntax} and \textbf{meta-syntax}\footnote{There is no standard word for this. So I made up \emph{meta-syntax}, which describes it best.}.

\subsubsection{Context-Free Syntax}

The syntax consists of
\begin{compactitem}
 \item Concepts. This is a finite set of primitive concepts that defines the different kinds of objects that exist in the formal system.\\
 Examples are \emph{type}, \emph{term}, \emph{formula}, \emph{program}, or \emph{proof}.
 \item Constructors. This is a finite set of primitive operations that build objects. Each object is an instance of one of the concepts.\\
 Examples are
   \begin{compactitem}
    \item $\Int$ builds the type of integers.
    \item $\times$ takes two types and returns their product type.
    \item $==$ takes two terms and returns a formula.
   \end{compactitem}
\end{compactitem}
The concepts and constructors together form a \textbf{context-free} grammar by using
\begin{compactitem}
 \item one non-terminal for every concept
 \item one production for every constructor
\end{compactitem}

\subsubsection{Context-Sensitive Meta-Syntax}

The meta-syntax consists of
\begin{compactitem}
 \item Judgments. This is a finite set of possible statements we can make about the syntax.\\
 Judgments are usually written using the $\der$ symbol.\\
 Examples are
  \begin{compactitem}
   \item ``term $t$ is well-formed and has type $A$'', which we usually write as $\oftype{}{}{t}{A}$.
   \item ``formula $F$ is well-formed'', which we usually write as $\oftype{}{}{F}{\form}$.
   \item ``formula $F$ is provable'', which we usually write as $\istrue{}{}{F}$.
  \end{compactitem}
 A judgment may hold/be true/be derivable or not.
 \item Rules. This is a finite set of possible ways to derive judgments. Only the judgments that can be derived by using the rules are true.\\
 Rules are usually written using a horizontal line between assumption/premise/hypothesis judgments and conclusion judgment.
  Examples are
  \begin{compactitem}
   \item the modus ponens rule ``if $\istrue{}{}{F\impl G}$ and $\istrue{}{}{F}$, then $\istrue{}{}{G}$'', which we usually write as
   \[\rul{\istrue{}{}{F\impl G} \tb \istrue{}{}{F}}{\istrue{}{}{G}}\]
   \item the pairing rule ``if $\oftype{}{}{s}{A}$ and $\oftype{}{}{t}{B}$, then $\oftype{}{}{(s,t)}{A\times B}$'', which we usually write as
   \[\rul{\oftype{}{}{s}{A} \tb \oftype{}{}{t}{B}}{\oftype{}{}{(s,t)}{A\times B}}\]
   \item the integer axiom ``all elements of the regular expression $[-](0|\ldots|9)^*$ are integers'', which we might write as
   \[\rul{x \in [-](0|\ldots|9)^*}{\oftype{}{}{x}{\Int}}\]
   (This rule uses an assumption that is not a formal judgment. That is allowed if the assumption can be checked in a well-defined way.)
 \end{compactitem}
\end{compactitem}
Judgments and rules together form an \textbf{inference system}.
 
The choice of concepts, constructors, judgments, and rules is not prescribed.
Every system is free to choose.
However, for every object $O$ there must be one distinguished judgment, that checks whether $O$ is a well-formed object.
Typically, there is a separate way to determine well-formedness depending on the concept of which $O$ is an instance.
Examples are:
\begin{compactitem}
 \item The judgment $\oftype{}{}{F}{\form}$ directly captures the well-formed formulas.
 \item For a term $t$ to be well-formed, we usually require that the judgment $\oftype{}{}{t}{A}$ holds for some type $A$, i.e., a term is well-formed if it is well-typed.
\end{compactitem}

The syntax and the meta-syntax together form a \textbf{context-sensitive} sublanguage of the context-free syntax.
This sublanguage consists of all objects of the context-free language for which the well-formedness judgment holds.

\subsection{Declarations}

\subsubsection{Introducing Declarations}

While objects are always anonymous, the declarations of a formal system introduce \textbf{names} for objects.
Like objects, the declarations are defined by a grammar and inference system.

Typically, there is a fixed non-terminal $Decl$ and a fixed judgment $\isdecl{}{}{D}$ for declarations $D$.
Each declaration-kind is defined by one production and one rule.

Examples are
\begin{compactitem}
 \item Term definitions are the declarations that introduce a name for a term.
 Those are the variable definitions known from most programming languages.
 They consist of the production
  \[Decl ::= \aval{x}{A}{t}\]
  for a new variable $x$ of type $A$ with initial value $t$
  and the rule
  \[\rul{\oftype{}{}{t}{A}}{\isdecl{}{}{\aval{x}{A}{t}}}\]
  that checks that the initial value has the expected type.
 \item Type definitions similarly introduce a name for a type.
 They consist of the production
  \[Decl ::= \atypedef{a}{A}\]
  for a new type variable $a$ with value $A$
  and the rule
  \[\rul{\oftype{}{}{A}{\type}}{\isdecl{}{}{\atypedef{a}{A}}}\]
  that checks that the initial value has the expected type.
\end{compactitem}
To make parsing easier, many languages (e.g., SML or Scala) require each declaration-kind to start with a special keyword (like $\akey{val}$ and $\akey{typedef}$ above).

We need to be able to use the names as objects later on.
Therefore, every concept comes with a built-in production for names.
For example, for terms we expect a production $term::=x$ where $x$ is the name of a variable.

Now \textbf{scope-checking} is the part of the inference system that ensures that a name $x$ may only be used if it has been previously declared.
That shows us that we cannot simply use the judgment $\oftype{}{}{x}{A}$---there would be no way to look up if $x$ has been declared.
Therefore, we have to collect all declarations that are in scope and carry them around.
That is the purpose of signatures and contexts.

\subsubsection{Assumptions}

Often we distinguish two kinds of declarations:
\begin{compactitem}
% \item \textbf{Global} declarations are the entire input (source file, program, etc.) that we are checking.
% These are fixed during checking.
% \item \textbf{Local} declarations are introduced temporarily while checking certain declarations.
% Examples are the variables declared inside a function.
 \item \textbf{Definitions} are declarations a definiens (i.e., an assignment to the new name). Most declarations in user input are of this form.
 \item \textbf{Assumptions} are declarations without a definiens.
 This should only be allowed in specific circumstances.\\
 Examples are the parameters of a function or a constructor.
\end{compactitem}

\subsubsection{Collecting Declarations}

A \textbf{context} $\Gamma$ is a list of declarations.

Usually all judgments for objects take a context as an additional argument.
It is usually placed to the left of the $\der$ symbol.
For example, we have $\oftype{}{\Gamma}{t}{A}$ or $\oftype{}{\Gamma}{F}{\form}$.
\medskip

Now the simplest form of scope-checking uses the rule
\[\rul{\aval{x}{A}{\_}\in \Gamma}{\oftype{}{\Gamma}{x}{A}}\]

And when checking input consisting of global declarations $D_1,\ldots,D_n$, we check for each $i=1,\ldots,n$ that
 \[\isdecl{}{D_1,\ldots,D_{i-1}}{D_i}\]
Thus, each declaration is checked in the context of the previous ones.
\medskip

This simple checking is not always sufficient.
There are many situations that require more complex treatment.
Examples are
\begin{compactitem}
 \item Each declaration should introduce a fresh name. Otherwise, the later declaration shadows the previous one.
 \item A list of mutually recursive functions cannot be checked in order because all functions must be in the context already when the first function is checked.
 \item Some declarations may contain local declarations inside them. For example, a function declaration may contain local variable declarations.
 \item Some declarations may only be allowed globally (e.g., class definitions in some programming languages) or only locally (e.g., undefined variables to represent the parameters of a function).
\end{compactitem}

\subsubsection{Polymorphic Declarations}

Very often it is convenient to introduce a new name that takes type parameters.
For example, the type $List[A]$ of lists over $A$ should take a parameter for the type $A$ of the elements.
Similarly, the function $revert[A]:List[A]\to List[A]$ takes a type parameter.

That can be accomplished by allowing for type assumptions in all declarations that we allow to be polymorphic.
Examples are:
\begin{compactitem}
 \item For parametric type definitions (also called \textbf{type operators}), we use
  \[Decl ::= \atypedef{a[(\atypedef{a}{})^*]}{A}\]
  \[a ::= a[A^*]\]
  where $a[A_1,\ldots,A_n]$ is the type that arises by applying $a$ to the parameters $A_1,\ldots,A_n$.
 \item For parametric constants (also called \textbf{polymorphic constants}), we use
  \[Decl ::= \aval{x[a^*]}{A}{t}\]
  \[t ::= t[A^*]\]
\end{compactitem}


\subsection{A Basic Formal System}

We now introduce a comprehensive example that we will build on later.
We first introduce an empty formal system, i.e., a type theory that has all the necessary structure but no non-trivial objects yet.
We can then extend it with specific types and terms in Sect.~\ref{sec:sd:typetheory:objects}

The concepts and constructors are given by the following context-free grammar:

\begin{commgrammar}
\gcomment{contexts}\\
\gprod{\Gamma}{Decl^*}{}\\
\gcomment{declarations}\\
\gprod{Decl}{\atypedef{a[a^*]}{A^?}}{type declaration (with optional definiens)}\\
\galtprod{\aval{x[a^*]}{A}{t^?}}{term declaration (with optional definiens)}\\
\gcomment{types}\\
\gprod{A}{a}{type names}\\
\gcomment{terms}\\
\gprod{t}{x}{term names}\\
\end{commgrammar}

The judgments are:
\begin{center}
	\begin{tabular}{|l|l|l|}
	\hline
	non-terminal & \multicolumn{2}{l|}{typing judgment} \\
	  \hline
		$\Gamma$ & $\;\;\;\iscont{}{\Gamma}$           & $\Gamma$ is well-formed\\
		$D$ & $\isdecl{}{\Gamma}{D}$        & declaration $D$ is well-formed in context $\Gamma$ \\\hline
		$A$ & $\oftype{}{\Gamma}{A}{\type}$ & $A$ is a well-formed type in context $\Gamma$ \\
		$t$ & $\oftype{}{\Gamma}{t}{A}$     & $t$ is a well-formed term of type $A$ type in context $\Gamma$ \\
		\hline
	\end{tabular}
\end{center}

The rules for this empty type type mostly take care of scope-checking:
\begin{compactitem}
\item Every declaration can use the previous ones (where $\epsilon$ is the empty context):
\[\rul{}{\iscont{}{\epsilon}}
\tb\tb
\rul{\iscont{}{\Gamma} \tb \isdecl{}{\Gamma}{D}}{\iscont{}{\Gamma,D}}
\]

\item Types and terms can be declared with type parameters and definiens:
\[
\rul{\oftype{}{\Gamma,\atypedef{a_1}{},\ldots,\atypedef{a_n}{}}{A}{\type}}{\isdecl{}{\Gamma}{\atypedef{a[a_1,\ldots,a_n]}{A}}}
\tb\tb
\rul{\oftype{}{\Gamma,\atypedef{a_1}{},\ldots,\atypedef{a_n}{}}{t}{A}}{\isdecl{}{\Gamma}{\aval{x[a_1,\ldots,a_n]}{A}{t}}}
\]
\item Declared names can be used later on with the right number of type parameters:
\[\rul{\begin{array}{c}
         \atypedef{a[a_1,\ldots,a_n]}{A}\;\in\; \Gamma \\
         \oftype{}{\Gamma}{A_1}{\type}\tb\ldots\tb \oftype{}{\Gamma}{A_n}{\type}
      \end{array}}
      {\oftype{}{\Gamma}{a[A_1,\ldots,A_n]}{\type}} \tb\tb
\rul{\begin{array}{c}
       \aval{x[a_1,\ldots,a_n]}{A}{\_}\;\in\; \Gamma \\
       \oftype{}{\Gamma}{A_1}{\type}\tb\ldots\tb \oftype{}{\Gamma}{A_n}{\type}
    \end{array}}
    {\oftype{}{\Gamma}{x[A_1,\ldots,A_n]}{A}}\]
\end{compactitem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concrete Objects}\label{sec:sd:typetheory:objects}

\subsection{Base Types}

\subsubsection{Types}

Base types $b$ are just names that are always in scope.
They are very easy to add using one production and one rule:

\[A \bbc b\]

\[\rul{}{\oftype{}{\Gamma}{b}{\type}}\]

Typical base types $b$ that we often add are
\begin{compactitem}
 \item $b=\Void$: the empty type
 \item $b=\Unit$: the unit type with one element
 \item $b=\Bool$: booleans
 \item $b=\Int$: integers
 \item $b=\String$: strings
\end{compactitem}

\subsubsection{Terms of Base Types: Base Terms}

Each base type $b$ comes with special base terms, often called literals that are defined by a regular expression $R_b$.

They are also easy to add using one production and one rule:

\[t \bbc R_b\]

\[\rul{l\in R_b}{\oftype{}{\Gamma}{l}{b}}\]

Typical literals are
\begin{compactitem}
 \item $R_\Void$= $\es$ (no literals for the empty type)
 \item $R_\Unit=()$ (the unique term of the unit type)
 \item $R_\Bool=\true|\false$
 \item $R_\Int=[-](0|\ldots|9)^*$
 \item $R_\String=(\backslash\backslash | \backslash" | "[^\backslash"])^*"$ (quoted string with $\backslash$ and " escaped)
\end{compactitem}

\subsubsection{Terms Formed From Base Terms}

Each base type $b$ comes with special operators that allow working with a term $t:b$.

We need no operations for $\Void$ and $\Unit$.

For booleans, we can use the if-then-else constructor:
\[t \bbc \aifelseI{t}{t}{t}\]
\[\rul{\oftype{}{\Gamma}{c}{\Bool}\tb \oftype{}{\Gamma}{s}{A}\tb \oftype{}{\Gamma}{t}{A}}{\oftype{}{\Gamma}{\aifelseI{c}{s}{t}}{A}}\]

For integers, we need the arithmetic operations such as
\[t \bbc t+t\]
\[\rul{\oftype{}{\Gamma}{s}{\Int}\tb \oftype{}{\Gamma}{t}{\Int}}{\oftype{}{\Gamma}{s+t}{\Int}}\]

We omit the other operations on integers and strings.

\subsection{Product Types}

Product types add the Cartesian product.

Like for base types, the definition consists of three parts.

\subsubsection{Product Type}

The first part introduces the new type:

\[A \bbc A\times A\]
\[\rul{\oftype{}{\Gamma}{A}{\type}\tb \oftype{}{\Gamma}{B}{\type}}{\oftype{}{\Gamma}{A\times B}{\type}}\]

\subsubsection{Terms of Product Type: Pairs}

The second part introduces terms of the new type:

\[t \bbc (t,t)\]
\[\rul{\oftype{}{\Gamma}{s}{A}\tb \oftype{}{\Gamma}{t}{B}}{\oftype{}{\Gamma}{(s,t)}{A\times B}}\]

\subsubsection{Terms Formed From Pairs: Projections}

The third part introduces terms formed from a term $t:A\times B$ of the new type:

\[t \bbc t.1 \bnfalt t.2\]

\[\rul{\oftype{}{\Gamma}{t}{A\times B}}{\oftype{}{\Gamma}{t.1}{A}}
\tb\tb
 \rul{\oftype{}{\Gamma}{t}{A\times B}}{\oftype{}{\Gamma}{t.2}{B}}\]

\subsection{Disjoint Union Types}

Disjoint union types add the union $A+B$ of two types in such a way that there is no overlap, i.e., $|A+B|=|A|+|B|$.
They are dual to the Cartesian product: all aspects mirror product types with the direction of functions reversed.
For example, we have two injections into $A+B$ instead of two projections out of $A\times B$.

\subsubsection{Union Type}

The first part introduces the new type:

\[A \bbc A+ A\]
\[\rul{\oftype{}{\Gamma}{A}{\type}\tb \oftype{}{\Gamma}{B}{\type}}{\oftype{}{\Gamma}{A+ B}{\type}}\]

\subsubsection{Terms of Union Type: Injections}

The second part introduces terms of the new type:

\[t \bbc \inj_1(t) \bnfalt \inj_2(t)\]
\[\rul{\oftype{}{\Gamma}{t}{A}}{\oftype{}{\Gamma}{\inj_1(t)}{A+B}}
\tb\tb
\rul{\oftype{}{\Gamma}{t}{B}}{\oftype{}{\Gamma}{\inj_2(t)}{A+B}}\]

\subsubsection{Terms Formed From Union Terms: Case Distinctions}

The third part introduces terms formed from a term $t:A+ B$ of the new type:

\[t \bbc cases(t,t,t)\]

\[\rul{\oftype{}{\Gamma}{t}{A + B} \tb \oftype{}{\Gamma,\aval{x}{A}{}}{c_1}{C}\tb \oftype{}{\Gamma,\aval{x}{B}{}}{c_2}{C}}{\oftype{}{\Gamma}{cases(t,c_1,c_2)}{C}}\]

A more intuitive notation for $cases(t,c_1(x),c_2(x))$ is $\amatchI{t}{\acase{\inj_1(x)}{c_1(x)},\acase{\inj_2(x)}{c_2(x)}}$.
Indeed, disjoint union types are rarely used as a primitive feature because they can be easily defined as an inductive type with constructors $\inj_1$ and $\inj_2$.

The formulation here is tweaked to be maximally modular, i.e., independent of other features.
Alternatively, $c_1$ and $c_2$ be taken to be functions $A\to C$ and $B\to C$.

\subsection{Function Types}

The most important type constructor in computer science is the one for function types---it is critical to implement computations.
It can be introduced systematically in the same way as products.

\subsubsection{Function Type}

The first part introduces the new type:

\[A \bbc A\to A\]
\[\rul{\oftype{}{\Gamma}{A}{\type}\tb \oftype{}{\Gamma}{B}{\type}}{\oftype{}{\Gamma}{A\to B}{\type}}\]

\subsubsection{Terms of Function Type: Anonymous Functions}

The second part introduces terms of the new type:

\[t \bbc \alam[A]{x}{t}\]
\[\rul{\oftype{}{\Gamma}{A}{\type}\tb\oftype{}{\Gamma,\aval{x}{A}{}}{t}{B}}{\oftype{}{\Gamma}{\alam[A]{x}{t}}{A\to B}}\]

Terms of function type are the $\lambda$-abstractions $\lambda x:A.t$.
In programming languages we usually write something like $\alam[A]{x}{t}$ instead.
The meaning is the same.

Our grammar and rule use the special case where all functions take exactly $1$ argument.
The general case can be defined accordingly (or can be obtained by using unit and product types).

$\lambda$-abstractions are more difficult than most other terms because they introduce a local variable.
This is no problem at all if we have understood type theory properly: we can simply add the variable to the context as a local assumption while checking the term $t$.
\medskip

However, many programming languages and programmers are confused or overwhelmed by this.
Therefore, they often do not use anonymous functions.

Instead, they introduce a special declaration for named functions:
\[Decl \bbc \afunI[B]{f}{x:A}{t}\]

Using anonymous functions, that is just a special case of a definition:
\[\afunI[B]{f}{x:A}{t} \tb\text{is the same as}\tb \aval{f}{A\to B}{\alam[A]{x}{t}}\]

However, named functions come up so often and are so convenient that most languages allow function declarations even if they are redundant.

\subsubsection{Terms Formed From Functions: Function Applications}

The third part introduces terms formed from a term $t:A\to B$ of the new type:

\[t \bbc t(t)\]

\[\rul{\oftype{}{\Gamma}{s}{A\to B}\tb \oftype{}{\Gamma}{t}{A}}{\oftype{}{\Gamma}{s(t)}{B}}\]

%records
  
\subsection{Objects with Local Definitions}

Local definitions introduce an abbreviation to be used in subsequent expressions.

Depending on the language, they are usually written as $\akey{let}\aval{x}{A}{s}\;\akey{in}t(x)$ or simply $\aval{x}{A}{s};t(x)$.
The latter tends to be nicer because it can be chained more easily.

Languages differ in what kind of definitions may be used locally in what kind of concepts.
For example, the above expressions use a local definitions of a \emph{term} inside a \emph{term}.

The following production and rule can be used to allow \emph{any} local definitions in a \emph{term}.
\[t \bbc Decl; t\]

\[\rul{\isdecl{}{\Gamma}{D}\tb\oftype{}{\Gamma,D}{t}{A}}{\oftype{}{\Gamma}{D; t}{A}}\]

Many programming language write these terms as $\{D;t\}$ with the understanding that chained local definitions can be grouped in a single pair of brackets as in $\{D_1;\ldots;D_n;t\}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Types}

Software is most dependable if programs represent the domain knowledge as closely as possible.
That requires being able to define new types capturing exactly the concepts of the domain.
The terms of these new types should capture exactly the needed domain data.

Data types are the most important way to do that.
Most typed programming languages allow defining data types.
But they may differ substantially in how they do it.

Data types are always named, i.e., they are introduced by declarations not by objects.
Like base, product, and function types, \emph{each} data type declaration introduces $3$ kinds of objects.

\subsection{Inductive Data Types}\label{sec:typetheory:idt}

An inductive data type represents a context-free grammar with a single non-terminal symbol.
It uses one term constructor for every production of the grammar such that its terms capture exactly the words of the grammar.
The operation on its terms is pattern-matching.

We usually want to work with arbitrary context-free grammars.
That is possible by using a set of mutually recursive inductive data types---one for each non-terminal.

\subsubsection{Type Declaration}

The declaration of an inductive data type looks as follows:

\[Decl \bbc \adata{a}{Con,\ldots,Con}\]
\[Con \bbc c(A,\ldots,A)\]
\[a,c \bbc \text{name} \]

Given $\adata{a}{Con_1,\ldots,Con_n}$, $a$ is the \textbf{name} of the data type and each $Con_i$ is a \textbf{constructor}.
For a constructor $c(A_1,\ldots,A_n)$, $c$ is the name of the constructor and the $A_i$ are its \textbf{argument types}.

The rule for checking a data type declaration looks as follows:
\[
\rul{\overbrace{\oftype{}{\Gamma,\atypedef{a}{}}{A_i}{\type}}^{\mfor i =1,\ldots,n}}
    {\isdecl{}{\Gamma}{\adata{a}{c_1(A_1),\ldots,c_n(A_n)}}}
\]
Here, for simplicity, the typing rule only covers the case where every constructor takes exactly $1$ argument.
The general case works accordingly (or can be obtained by using the unit type or a product type as the argument type).

Note that the data type $a$ is already assumed to exists when checking the types of the constructors.
That is critical to allow writing recursive data types such as:

\begin{example}\label{ex:typetheory:nat}
The standard example of an inductive data type are the natural numbers.
We obtain them as
\[\adata{nat}{zero(),succ(nat)}\]
\end{example}

%%% Positional arguments: also possible for IDT: For example, data nat(zero) = succ(nat), and eval(n:nat) = match n(0){succ(x) => x+1}
%%% better syntax for full symmetry: data nat(zero: nat){such: nat -> nat}
%%% constructor arguments of ADT should not be private - better allow every new instance to have arbitrary extra declarations; analogue for pattern matching are auxiliary definitions
%%% then main difference left: ADT's allow for infinite trees and possibly mutable fields - if mutable fields are allowed multiple instances must be distinguished!, otherwise, compiler is allowed to marge multiple instances with same arguments, i.e., only one start object needed for final coalgebra construction
%% new instance creation = fresh variable; interpret undefined val by creating new instance, allows symbolic interpretation

\subsubsection{The Type Provided by an Inductive Data Type}

Once declared, the name of the data type is a well-formed type:
\[\rul{\adata{a}{\ldots}\in\Gamma}{\oftype{}{\Gamma}{a}{\type}}\]

%In some programming language, each argument is of the form $s_i:A_i$ for a name $s_i$.
%The $s_i$ are called \textbf{selectors}.

The correspondence between grammars and inductive data types is as follows:
\begin{ctabular}{|p{.5\textwidth}|l|}
\hline
Grammar & Inductive Data Type \\
\hline
start symbol & not needed \\
non-terminal $N$ & inductive data type declaration with name $N$\\
production $N::= T_0 N_0 T_1 \ldots T_{n-1} N_{n-1} T_n$ & constructor for $N$ with argument types $N_i$ \\
name for production (must be invented) & name of constructor \\
name for occurrence $N_i$ of non-terminal symbol in production (must be invented) & selector \\ 
terminal symbols $T_i$ in production & ignored \\
\hline
\end{ctabular}

\subsubsection{Terms of an Inductive Data Type}

The constructors form elements of the type $a$:

\[t \bbc c(t,\ldots,t)\]
\[\rul{\adata{a}{c_1(A_1),\ldots,c_n(A_n)}\;\;\in\;\;\Gamma \tb \oftype{}{\Gamma}{t}{A_i}}{\oftype{}{\Gamma}{c_i(t)}{a}}\]

Again we only use constructors with exactly $1$ argument.

\subsubsection{Terms Formed From Terms of Inductive Data Type}

To operate on a term of an inductive type, we use \textbf{pattern-matching}.

\[t \bbc \amatchI{t}{Cas,\ldots,Cas}\]
\[Cas \bbc \acase{c(x_1,\ldots,x_n)}{t}\]

In $\amatchI{t}{Cas_1,\ldots,Cas_n}$, $t$ is the matched term and the $Cas_i$ are the \textbf{cases}.
In a case $\acase{c(x_1,\ldots,x_n)}{t}$, we call $c(x_1,\ldots,x_n)$ the \textbf{pattern} where $c$ is a constructor name and the $x_i$ are variables.
$t$ is called the \textbf{body} of the case.

Finally, the typing rule is (again assuming all constructors take exactly $1$ argument): 

\[\rul{\adata{a}{c_1(A_1),\ldots,c_n(A_n)}\;\;\in\;\;\Gamma \tb \oftype{}{\Gamma}{t}{a}\tb
       \overbrace{\oftype{}{\Gamma,\aval{x}{A_i}{}}{t_i}{B}}^{\mfor i=1,\ldots,n}}
      {\oftype{}{\Gamma}{\amatchI{t}{\acase{c_1(x)}{t_1},\ldots,\acase{c_n(x)}{t_n}}}{B}}\]


\subsection{Abstract Data Types}\label{sec:typetheory:adt}

Inductive and abstract data types are dual to each other.

An inductive data type is defined bottom-up: the terms are exactly the \emph{syntax trees} of the words over some context-free grammar.
That makes it easy to build terms---we just apply one of the constructor.
But it makes it difficult to operate on a term---we have to pattern-match for all possible cases.

An abstract data type is defined top-down by its \emph{behavior}: any object that exhibits the required behavior is a term of the type.
That makes it easy to operate on a term---we just apply one of the required behaviors.
But it makes it difficult to build terms---we have to implement all required behaviors.

\subsubsection{Type Declaration}

The declaration of an abstract data type looks as follows:

\[Decl \bbc \aclassAI{a}{x:A,\ldots,x:A}{}{Field,\ldots,Field}\]
\[Field \bbc f:A\]
\[a,f,x \bbc \text{name} \]

Given $\aclassAI{a}{x_1:A_1,\ldots,x_n:A_m}{}{Field_1,\ldots,Field_n}$, $a$ is the \textbf{name} of the data type and each $Field_i$ is a \textbf{field}.
The $x_i:A_i$ are called the \textbf{constructor arguments}.
For a field $f:A$, $f$ is the name of the field and $A$ its type.

The rule for checking the declarations is as follows:

\[\rul{\overbrace{\oftype{}{\Gamma,\atypedef{a}{}}{A_i}{\type}}^{\mfor i=1,\ldots,m} \tb 
       \overbrace{\oftype{}{\Gamma,\atypedef{a}{}}{B_i}{\type}}^{\mfor i=1,\ldots,n}}
      {\isdecl{}{\Gamma}{\aclassAI{a}{x_1:A_1,\ldots,x_m:A_m}{}{f_1:B_1,\ldots,f_n:B_n}}}
\]

Like for inductive data types, the new type $a$ is already assumed when checking the types of the constructor arguemtns and the fields.
That is critical to describe many practical examples:

\begin{example}\label{ex:ad:datatype:automata}
Standard examples of abstract data types tend to be polymorphic.

One of the standard examples of abstract data types is an automaton.
In this example, we use numbered states and strings as the input and output of a transition:

\[\aclassAI{automaton}{state:\Int}{}{inFinalState: \Bool,\; transition: \String\to \String}\]

The constructor argument $state$ provides the initial state as well as the private variable that tracks the current state.
In concrete implementations (see also Rem.~\ref{rem:ad:datatype:dual}), the values of $inFinalState$ and $transition$ may depend on and change the value of $state$.

Such an automaton may or may not be finite---that depends on which states are reachable from the initial state by applying transitions.
\end{example}

\begin{remark}[Duality]\label{rem:ad:datatype:dual}
Intuitively, inductive data types start with nothing and build all objects inductively by applying constructors.
(We do not even need a base case---instead, we use constructors that take $0$ arguments.)
Therefore, there is wide agreement on their intuition, syntax, and semantics (because it is easy to agree on what \emph{nothing} means).
In other words, by default we assume objects are not members of an inductive data type; only those whose membership can be proved by applying constructors to build them are allowed.

Abstract data types are dual in the sense that they start with \emph{everything} and discard all objects that show the wrong behavior.
In other words, by default we assume all objects are members of an inductive data type; but we only disallow those membership can be disproved by applying fields to obtain an illegal behavior.

But it is much harder to agree on what \emph{everything} is.
Therefore, intuition, syntax, and semantics of abstract data types can vary widely across languages.
Particular incarnations include coalgebras, coinductive data types, and object-orientation-like classes.
\medskip

This has an important consequence on the relation between specification and implementation.
If a specification includes an inductive data type, every implementation must include the same data type with the same meaning.
(If the specification and the implementation are written in the same language, e.g., by using SML signatures and SML structures, respectively, this can even get awkward because the exact declaration must be repeated twice.)

But if a specification includes an abstract data type, the choice of implementation language can substantially affect what objects can be built.
We often write the specification in a logic and the implementation in a programming language.
Then it is possible that the abstract data type is very weak in the logic: the logic might be able to define no or only trivial behavior.
For example, the type theory of this section cannot define many objects of the abstract data type of automate from Ex.~\ref{ex:ad:datatype:automata}.
The same abstract data type in the programming language may be much larger because the programming can express so much more behavior.

That is not a defect.
It shows the abstraction at work: objects of an abstract data type are black boxes that we can never inspect, and how the behavior is defined is completely open and irrelevant.
No language ever knows what kind of objects an abstract data type has.
So even if a logic cannot define any object of an abstract data type, neither can it prove that the type is empty.
\end{remark}

\subsubsection{The Type Provided by an Abstract Data Type}

Like for inductive data types, once declared, the name is a new type:
\[\rul{\aclassAI{a}{\ldots}{}{\ldots}\;\;\in\;\;\Gamma}{\oftype{}{\Gamma}{a}{\type}}\]

\begin{remark}[Relation to Object-Orientation]
Object-oriented programming languages provide a very rich formalism for defining abstract data types.

Our variant here is a special case that arises if we make the following restrictions about classes:
\begin{compactitem}
 \item There is no inheritance.
 \item There is exactly one constructor.
 \item The constructor arguments are exactly the private variables of the class.
 \item All methods are public and abstract.
 \item When creating a new instance, all methods must be implemented.
\end{compactitem}
This is obviously too restrictive for programming.
But it is enough to explain the theory systematically.
It is straightforward to relax these restrictions when defining practical languages.
\end{remark}

There is a correspondence between certain kinds of machines and abstract data types.
We do not spell out the formal details of the machines here and only sketch the correspondence informally:
\begin{ctabular}{|p{.5\textwidth}|l|}
\hline
Machine & Abstract Data Type \\
\hline
set of possible states & product $A_1\times \ldots \times A_n$ of constructor argument types\\
initial state & constructor arguments $(s_1,\ldots,s_n)$ \\
current state & current tuple of private variables $(x_1,\ldots,x_n)$ \\
query about current state (e.g., being final) & apply field of non--function type \\
$f$-transition (push button labeled $f$) & apply field $f$ of function type\\
possible inputs for transition $f$ & argument types of $f$\\
possible outputs of transition $f$ & return type of $f$\\
\hline
\end{ctabular}

\subsubsection{Terms of an Abstract Data Type: New Instance Creation}

Terms of an abstract data type $a$ are created using the \textbf{new} operator.
They are also called \textbf{instances} of $a$.

\[t \bbc \anewA{a}{t,\ldots,t}{Def,\ldots,Def}\]
\[Def \bbc f=t\]

In $\anewA{a}{t_1,\ldots,t_n}{Def_1,\ldots,Def_n}$, the $t_i$ are the \textbf{constructor arguments}, and the $Def_i$ are the \textbf{field definitions}.

Again, we state the typing rule only for a special case: we assume that the constructor takes exactly $1$ argument.
The general case works accordingly.

\[\rul{\aclassAI{a}{x:A}{}{f_1:A_1,\ldots,f_n:A_n}\;\;\in\;\;\Gamma\tb
       \oftype{}{\Gamma}{t}{A} \tb
       \overbrace{\oftype{}{\Gamma}{t_i}{A_i}}^{\mfor i=1,\ldots,n}}
      {\oftype{}{\Gamma}{\anewA{a}{s}{f_1=t_1,\ldots,f_n=t_n}}{a}}\]


\subsubsection{Terms Formed From a Term of an Abstract Data Type: Field Access}

To operate on a term of an abstract data type, we access its fields:

\[t \bbc t.f\]

\[\rul{\aclassAI{a}{\ldots}{}{f_1:A_1,\ldots,f_n:A_n}\;\;\in\;\;\Gamma\tb
       \oftype{}{\Gamma}{t}{a}}
      {\oftype{}{\Gamma}{t.f_i}{A_i}}\]

\subsection{Polymorphic Data Types}

Data types are usually allowed to be polymorphic.

Therefore, the grammar must actually allow all declarations and all references to constructors and fields to take type parameters.

We omit the rules and only list the resulting productions:

\begin{commgrammar}
\gcomment{data type declarations}\\
\gprod{Decl}{\adata{a[a^*]}{Con^*}}{inductive}\\
\galtprod{\aclassAI{a[a^*]}{(x:A)^*}{}{Field^*}}{abstract}\\
\gcomment{types (same productions as before)}\\
\gprod{A}{a[A^*]}{data type applied to type parameters}\\
\gcomment{building and using terms}\\
\gprod{t}{c[A^*](t^*)}{constructor application}\\
\galtprod{\amatchI{t}{Cas^*}}{pattern-match}\\
\galtprod{\anewA{a}{t^*}{Def^*}}{new instance}\\
\galtprod{t.f[A^*]}{field access}\\
\gcomment{auxiliary non-terminals}\\
\gprod{Con}{c(A^*)}{constructor declaration}\\
\gprod{Cas}{\acase{c(x^*)}{t}}{case in pattern-match}\\
\gprod{Field}{f:A}{field declaration}\\
\gprod{Def}{f=t}{field definition in new instance}\\
\end{commgrammar}

\begin{example}[Lists and Trees]
The standard example of a polymorphic inductive data type are lists over a type $a$:
\[\adata{List[a]}{nil,{cons(a,List[a])}}\]

Trees are only slightly more complex, e.g., for complete binary trees whose leafs are labeled with elements of $a$:
\[\adata{Tree[a]}{leaf(a),{node(Tree[a],Tree[a])}}\]
\end{example}

\begin{example}[Iterators and Streams]
The standard example of a polymorphic abstract data type are iterators (also called streams) over a type $a$:
\[\aclassAI{Iterator[a]}{}{}{hasNext:\Bool,\;next:a}\]
Technically, the return type of $a$ should $a^?$ (i.e., options of $a$), but it is commonly agreed that the behavior of $next$ is unspecified if no next element is left.

If next always returns another element, we obtain streams, i.e., infinite sequences:
\[\aclassAI{Stream[a]}{}{}{next:a}\]

Like for automata, we have to generalize the language to allow for defining more interesting objects.
In particular, new instances should be allowed to have additional private variables, e.g., to track the current position in the iterator/stream.
\end{example}

\subsection{Inheritance}

Inheritance generalizes abstract data types in two ways:
\begin{compactitem}
 \item Fields may already have a definition. Such fields are not implemented anymore when creating a new instance.
 \item We can form a new data type by extending an existing one, which has the effect of copying over all its fields.
\end{compactitem}

Both generalizations can be seen as a matter of convenience.
However, by fixing certain definitions in an class, we gain enormous power to fine-tune the desirable behavior of the instances.

\begin{remark}[Inheritance for Inductive Data Types]
Inheritance works exactly the same way for inductive data types: fields with definition are skipped when pattern-matching, and new types can reuse the set of constructors of an existing type.

However, this is not used in practice.\footnote{Possibly because no one has understood it clearly so far.}
\end{remark}

\subsubsection{Inductive Data Types Via Classes}

Another advantage of inheritance is that we can mimic inductive data types.

The correspondence is as follows:
\begin{ctabular}{|l|l|}
\hline
Inductive Data Type & Classes \\
\hline
type declaration & abstract class without constructor arguments or fields\\
constructor & concrete extension \\
constructor arguments & constructor arguments of concrete extension \\
constructor application & new instance \\
pattern-matching & combination of if-then-else and is-instance-of\\
\hline
\end{ctabular}

Thus, we can mimic the inductive type
\begin{acode}
\adata{a}{\ldots, {c_i(A_1,\ldots,A_n)},\ldots}
\end{acode}
as
\begin{acode}
\aclassA{a}{}{}{}\\
\vdots \\
\aclass{c_i}{x_1:A_1,\ldots,x_n:A_n}{a}{}\\
\vdots
\end{acode}

Constructing a term $c_i(t_1,\ldots,t_n)$ corresponds to $\anew{c_i}{t_1,\ldots,t_n}$.
Equality at $a$ must be implemented such that $x==y$ iff $x$ and $y$ are instances of the same class $c_i$ and created with equal constructor arguments.

Pattern-matching must be implemented manually.

\section{Implementation}

Formal systems can be implemented very systematically in any programming language that supports inductive data types.

The design is as follows:
\begin{compactenum}
 \item Define inductive types $DT$ that represents the context-free grammar.
 \item Define parsing and printing functions that translate between $\String$ and $DT$.
 \item Define checking functions that check whether $DT$-objects are well-formed.
 \item Define processing functions that take $DT$-objects. In particular,
  \begin{compactitem}
   \item interpreters or evaluators turn objects into their values,
   \item static analysis tool perform additional checks or statistics,
   \item compilers transform objects into equivalent objects in a different formal system.
  \end{compactitem}
\end{compactenum}

The following table gives an overview:

\begin{ctabular}{|l|l|}
\hline
Component \ldots & contains for each concept/non-terminal $N$ one \ldots\\
\hline
data type & inductive data type $N$ \\
printer  & function $printN(o: N): \String$ \\
parser   & function $parseN(input: \String): N$ \\
checker  & function $checkN(o: N):\Bool$\\
\hline
%interpreter & function $interpret(t: term): \Unit$ \\
%compiler    & function $compile(t: term):T$ where $T$ is target type \\
%\hline
\end{ctabular}
In each component, the respective functions are usually mutually recursive.

The processing pipeline is always
\begin{center}
\begin{tikzpicture}
\node[ellipse,draw] (S) at (0,0) {source};
\node[ellipse,draw] (D) at (0,2) {DT};
\node[ellipse,draw] (C) at (0,4) {DT, checked};
\node[ellipse,draw] (T) at (6,4) {target type};
\node[ellipse,draw] (O) at (6,0) {output};
\draw[arrow] (S) --node[left] {parser} (D);
\draw[arrow] (D) --node[left] {checker} (C);
\draw[arrow] (C) --node[above] {processor} (T);
\draw[arrow] (T) --node[right] {printer} (O);
\end{tikzpicture}
\end{center}

\section{Evaluation}

\subsection{Overview}

The framework from Sect.~\ref{sec:sd:typetheory:common} still has to be extended by one aspect: we need an additional judgment \[\evobj{}{\Gamma}{t}{v}\] that describes how a term $t$ is evaluated to a value $v$.
This judgment is called \textbf{evaluation}.
For example, we expect a rule like $\evobj{}{\Gamma}{1+1}{2}$.
\medskip

From the perspective of programming languages, we can think of terms as programs and of evaluation as running the program.
From the perspective of logic, we can think of the evaluation rules as a set of axioms that describe the equality relation between objects.
\medskip

Type theories can very quite drastically in how evaluation works.
Everything can vary including the abstract shape of the judgment, its notation, and its rules.
Often differences between formal systems manifest themselves in the details of this judgment.

In particular, the treatment of function application has received a lot of attention.

\subsection{Typing vs. Evaluation}

Primarily we evaluate \emph{terms} to \emph{values}.
Values are a subset of the terms that can no longer be simplified.
For example, all the literals of the base types are values.

Because terms occur inside declarations and contexts, we also have to evaluate those, usually by recursing into the terms.
Moreover, depending on the specific type theory used, we may also have to evaluate types.
For example, if we allow type definitions, we have to expand those evaluation.
However, the evaluation of types is very simple.
If it becomes too difficult (e.g., non-terminating, undecidable, or with side-effects), the formal system quickly becomes unmanageable.

Both typing and evaluation typically consist of one judgment per non-terminal and one rule per production.
Thus, both describe a family of mutually recursive functions on the grammar.
This yields the following table:

\begin{center}
	\begin{tabular}{|l|l|l|l|}
	\hline
	non-terminal & typing judgment & \multicolumn{2}{l|}{evaluation judgment} \\
	  \hline\hline
		$\Gamma$ & $\iscont{}{\Gamma}$ & $\;\;\;\evcont{}{\Gamma}{\Gamma'}$           & $\Gamma$ evaluates to $\Gamma'$\\
		$D$ & $\isdecl{}{\Gamma}{D}$ & $\evdec{}{\Gamma}{D}{D'}$        & declaration $D$ evaluates to $D'$ \\
		$A$ & $\oftype{}{\Gamma}{A}{\type}$ & $\evobj{}{\Gamma}{A}{A'}$        & type $A$ evaluates to $A'$ \\
		$t$ & $\oftype{}{\Gamma}{t}{A}$ & $\evobj{}{\Gamma}{t}{t'}$        & term $t$ evaluates to $t'$ \\
		\hline
	\end{tabular}
\end{center}


Because we think of evaluation as running a program, evaluation is the primary \emph{dynamic} operation.
Typing is the primary \emph{static} operation.
Like all dynamic operations, evaluation always happens \emph{after} type-checking and is only applied to well-formed objects.

Correspondingly, typing errors are called \textbf{static} or \textbf{compile-time} errors, and evaluation errors are called \textbf{dynamic} or \textbf{run-time} errors.
\medskip

As a general rule, the evaluation of well-formed terms should not cause errors, i.e., we want to statically verify that there will be no run-time errors.

In practice, however, some run-time errors must be accepted, e.g., errors that are
\begin{compactitem}
 \item intentionally generated by the programmer (i.e., correct program behavior that happens to take the form of an abort)
 \item caused by interaction with the environment such as hardware or network failures or missing access rights to files
 \item caused by missing dependencies (e.g., type-checking against a library that at run-time is missing or present in a different version)
\end{compactitem}
Moreover, as we will see in Sect.~\ref{sec:formsys:decsoucomp}, sufficiently complete formal systems such as programming languages may make the absence of run-time errors undecidable.

\subsection{Basic Rules}

The rules for the empty formal system are
\begin{itemize}
\item Contexts are evaluated in order by evaluating every declaration::
\[\rul{}{\evcont{}{\epsilon}{\epsilon}}
\tb\tb
\rul{\evcont{}{\Gamma}{\Gamma'} \tb \evdec{}{\Gamma'}{D}{D'}}{\evcont{}{\Gamma,D}{\Gamma',D'}}
\]

Note how every declaration $D$ is evaluated in the context, in which all previous declarations have been evaluated already.

\item Declarations are evaluated by evaluating every object in them:
\[\rul{\evobj{}{\Gamma}{A}{A'}}{\evdec{}{\Gamma}{\atypedef{a}{A}}{\atypedef{a}{A'}}}
\tb\tb
\rul{\evobj{}{\Gamma}{A}{A'}\tb\evobj{}{\Gamma}{t}{t'}}
    {\evdec{}{\Gamma}{\aval{a}{A}{t}}{\aval{a}{A'}{t'}}}\]

Polymorphic declarations or declarations in which optional parts are absent are evaluated accordingly.

\item Declared names evaluate to their definition if they have one and to themselves otherwise:
\[\rul{\atypedef{a}{A}\;\in\; \Gamma \tb \evobj{}{\Gamma}{A}{A'}}
      {\evobj{}{\Gamma}{a}{A'}}
\tb\tb
\rul{\atypedef{a}{}\;\in\; \Gamma}
      {\evobj{}{\Gamma}{a}{a}}
\]

\[\rul{\aval{x}{A}{t}\;\in\; \Gamma \tb \evobj{}{\Gamma}{t}{t'}}
      {\evobj{}{\Gamma}{x}{t'}}
\tb\tb
\rul{\aval{x}{A}{}\;\in\; \Gamma}
      {\evobj{}{\Gamma}{x}{x}}
\]

Note how the definiens is evaluated recursively here.
This is usually redundant because all previous declarations are already evaluated, i.e., the definition will not change anymore.
But occasionally, we use formal systems where this step is necessary.
\end{itemize}

\subsection{Rules for Individual Language Features}

For most language features---such as product and function types---evaluation simply recurses through the expressions.
However, in a few key places non-trivial steps happen.
Those few places are the reason why evaluation ``makes something happen''.
Most importantly, this includes function application.

In the following, we give evaluation rules for important features.
We omit data types---they are similar but more complex.

\subsubsection{Types}

Evaluation for types just straightforwardly recurses through the type.
The expansion of definitions from above is the only non-trivial step:

\[\rul{\text{for base types } b}
      {\evobj{}{\Gamma}{b}{b}}
\]

\[
\rul{\evobj{}{\Gamma}{A}{A'} \tb \evobj{}{\Gamma}{B}{B'}}
    {\evobj{}{\Gamma}{A \times B}{A'\times B'}}
\tb\tb
\rul{\evobj{}{\Gamma}{A}{A'} \tb \evobj{}{\Gamma}{B}{B'}}
    {\evobj{}{\Gamma}{A \to B}{A'\to B'}}
\tb\tb
\rul{\evobj{}{\Gamma}{A}{A'} \tb \evobj{}{\Gamma}{B}{B'}}
    {\evobj{}{\Gamma}{A + B}{A'+ B'}}
\]

\subsubsection{Terms Regarding Base Types}

The literals trivially evaluate to themselves:

\[\rul{l\in R_b}
      {\evobj{}{\Gamma}{l}{l}}
\]

Operator applications evaluate to the result of calling the corresponding function in the underlying system.
The underlying system can be another programming language but is ultimately the hardware on which evaluation is executed.

For example, 
\[\rul{\evobj{}{\Gamma}{s}{i} \tb \evobj{}{\Gamma}{t}{j} \tb i,j\in \Z}
      {\evobj{}{\Gamma}{s+t}{i\oplus j}}
\]
where $s$ and $t$ evaluate to the integers $i$ and $j$, and $i\oplus j $ is actual integer addition performed by the underlying system.
Note that here $s+t$ is a term, i.e., $+$ is just a symbol, where $i\oplus t$ is the result of an operation.
\medskip

Operator application is straightforward.
However, to allow for generalizations to programming languages (where evaluation may run forever, fail, or have side-effects), we have to be careful about the order in which the arguments are evaluated.

The typical convention is to evaluate the argument left-to-right.
For example, $s$ is evaluated before $t$.
\medskip

Of particular importance are the rules for the if-operator:
\[\rul{\evobj{}{\Gamma}{c}{\true}\tb \evobj{}{\Gamma}{t}{t'}}
      {\evobj{}{\Gamma}{\aifelseI{c}{t}{e}}{t'}}
\tb\tb
\rul{\evobj{}{\Gamma}{c}{\false}\tb \evobj{}{\Gamma}{e}{e'}}
      {\evobj{}{\Gamma}{\aifelseI{c}{t}{e}}{e'}}
\]
These rules only evaluate the branch that is needed---either $t$ or $e$.
The other branch is discarded.

For example, if we define the boolean operators as in $s\wedge t:=\aifelseI{s}{t}{\false}$, this yields short-circuit evaluation:
\[\rul{\evobj{}{\Gamma}{s}{\false}}
      {\evobj{}{\Gamma}{s\wedge t}{\false}}
\tb\tb
\rul{\evobj{}{\Gamma}{s}{\true} \tb\evobj{}{\Gamma}{t}{t'}}
      {\evobj{}{\Gamma}{s\wedge t}{t'}}
\]
\medskip

The equality operator presents a problem though when it is combined with functions: equality of functions is undecidable.
There are two approaches to work around this problem:
\begin{compactitem}
 \item Functional languages tend to use typing rules that restrict equality to those types for which equality is decidable.
 \item Imperative languages tend to allow equality for all types but then implement it using reference equality.
  In that case, equality of functions will be true less often than it should be.
\end{compactitem}



\subsubsection{Terms Regarding Product Types}

Pairs are evaluated by a simple recursion:
\[\rul{\evobj{}{\Gamma}{s}{s'} \tb \evobj{}{\Gamma}{t}{t'}}
      {\evobj{}{\Gamma}{(s,t)}{(s',t')}}
\]

Projections are more complicated because something actually happens: we retrieve a component from a pair using the rules
\[\rul{\evobj{}{\Gamma}{u}{(s,t)}}
      {\evobj{}{\Gamma}{u.1}{s}}
\tb\tb
\rul{\evobj{}{\Gamma}{u}{(s,t)}}
      {\evobj{}{\Gamma}{u.2}{t}}
\]

If $u$ does not evaluate to a pair, we simply recurse.

\subsubsection{Terms Regarding Disjoint Union Types}

Like pairs, the terms of the new type---the injections---are evaluated by a simple recursion:
\[\rul{\evobj{}{\Gamma}{t}{t'}}
      {\evobj{}{\Gamma}{\inj_1(t)}{\inj_1(t')}}
\tb\tb
\rul{\evobj{}{\Gamma}{t}{t'}}
      {\evobj{}{\Gamma}{\inj_2(t)}{\inj_2(t')}}\]

Like for projections, something happens when evaluating the terms that use a term of the new type: a case distinction is contracted if we know which case applies.
\[\rul{\evobj{}{\Gamma}{u}{\inj_1(t)}\tb\evobj{}{\Gamma}{c_1(t)}{t'}}
      {\evobj{}{\Gamma}{cases(u,c_1,c_2)}{t'}}
\tb\tb
\rul{\evobj{}{\Gamma}{u}{\inj_2(t)}\tb\evobj{}{\Gamma}{c_2(t)}{t'}}
    {\evobj{}{\Gamma}{cases(u,c_1,c_2)}{t'}}
\]
Note that only the case that applies is evaluated---that is important in case there are side effects.

If $u$ does not evaluate to an injection, we simply recurse.

\subsubsection{Terms Regarding Function Types}

Intuitively, the key evaluation rule relevant to product types is $\beta$-reduction: substitute the variable $x$ in $\alam[A]{x}{t}$ with $s$ in order to evaluate $(\alam[A]{x}{t})\,s$.
However, the details  present considerable difficulties.

Historically very important was the choice between call-by-name and call-by-value evaluation.
\textbf{Call-by-value} evaluates the argument of the function first, then substitutes it for $x$:
\[\rul{\evobj{}{\Gamma}{f}{\alam[A]{x}{t}} \tb
       \evobj{}{\Gamma}{s}{s'} \tb\evobj{}{\Gamma}{t[x/s']}{u}}
      {\evobj{}{\Gamma}{f(s)}{u}}
\]
where $t[x/s]$ is the result of substituting $x$ with $s$ in $t$.
\textbf{Call-by-name} first substitutes, then evaluates the result:
\[\rul{\evobj{}{\Gamma}{f}{\alam[A]{x}{t}}\tb \evobj{}{\Gamma}{t[x/s]}{u}}
      {\evobj{}{\Gamma}{f(s)}{u}}
\]
Without side-effects, both yield the same result.
Call-by-name is usually less efficient because $s$ has to be evaluated multiple times if $x$ occurs multiple times in $t$.
But if side-effects are possible, the order matters; moreover, evaluating $s$ multiple times may be exactly what the programmer wants.
\medskip

Another problem is the evaluation of anonymous functions.
The intuitive rule
\[\rul{\evobj{}{\Gamma}{A}{A'} \tb \evobj{}{\Gamma,x:A'}{t}{t'}}
      {\evobj{}{\Gamma}{\alam[A]{x}{t}}{\alam[A']{x}{t'}}}
\]
does not work in the presence of side-effects: The side-effects of $t$ should not be triggered in this rule, when the function is \emph{constructed}.
Instead, they should occur be triggered later when the function is \emph{called}.

On the other hand, we cannot delay the entire evaluation of an anonymous function until call time.
Consider the following program:
\[\aval{v}{\Int}{1} \tb;\tb \aval{f}{\Int\to\Int}{\alam[\Int]{x}{x+v}} \tb ; \tb \aval{v}{\Int}{2} \tb ;\tb \aprint{f(1)}\]
Here the reference to $v$ must be evaluated before $f(1)$ is applied: otherwise, $f$ would be moved into a different environment in which $v$ refers to a different declaration.
Therefore, programming languages have to perform \textbf{closures} in which an anonymous function is made independent of its environment.

The following is a simple definition of closure:

\begin{definition}[Closure]
Assume that $\Gamma$ are fully evaluated.
In particular, all declarations in $\Gamma$ have a definition that does not refer to any previously declared names.

Then given an object $X$ in context $\Gamma$, we define $Closure_\Gamma(X)$ as the object that arises from $X$ by replacing every name with its definition.
\end{definition}

Then we can use the following rule:
\[\rul{A'=Closure_\Gamma(A) \tb t'=Closure_\Gamma(t)}
      {\evobj{}{\Gamma}{\alam[A]{x}{t}}{\alam[A']{x}{t'}}}
\]
This rule resolves all references but does not apply any evaluation rules to $t$.
The resulting function does not refer to any names anymore and can therefore be safely transported into another context.

Many formal systems are simple enough so that the closure of a type is the type itself, i.e., only the terms have to be closed.
