\section{Formulas}\label{sec:sd:formulas}

Like programming languages, logics add new concepts to type theory: formulas and proofs.
Only proofs are fundamentally new and correspond very closely to programs.

\subsection{Formulas as Terms}

Formulas are almost already covered by type theory and part of all programming languages.
Indeed, type theory already has the type $\Bool$ with operators for equality and propositional connectives.
However, logic goes beyond that by introducing the quantifiers $\forall$ and $\exists$.

Using the type $\Bool$ for formulas is a simple solution in situations where we anyway use type theories and programming languages that have it already.
The combination of function types and formulas as terms leads to what is called higher-order logic (HOL).
This was the logic originally introduced by Church when developing the $\lambda$-calculus \cite{churchtypes}.

\subsubsection{Grammar and Typing}

In principle, it is not difficult to add them to type theories as well, and many type theories do that to some extent.
This requires just two productions
\[t \bbc \afa[A]{x}{t} \bnfalt \exists x:A.t\]
with corresponding typing rules
\[\rul{\oftype{}{\Gamma,\aval{x}{A}{}}{t}{\Bool}}{\oftype{}{\Gamma}{\afa[A]{x}{t}}{\Bool}}
\tb\tb
\rul{\oftype{}{\Gamma,\aval{x}{A}{}}{t}{\Bool}}{\oftype{}{\Gamma}{\aex[A]{x}{t}}{\Bool}}
\]

\subsubsection{Evaluation}

We cannot extend the typing-evaluation pair of algorithms known from type theory and programming languages to logic: the evaluation of quantified formulas is undecidable.

More precisely, it is undecidable whenever the domain of quantification---the type $A$ above---is infinite.
If $A$ is finite, we can (usually inefficiently) evaluate quantified formulas by testing the instances for every possible $x:A$.
If $A$ is infinite, testing can only evaluate universally quantified formulas to $\false$ (by finding some instance that is $\false$) or existentially quantified ones to $\true$ (by finding some instance for which it is true).
However, even if $A$ is finite, it is usually too big to make evaluation by testing useful.

Actually, undecidable evaluation in logic is not all that new---we also found it in programming languages.
Firstly, they allow for non-termination, which already makes evaluation undecidable.
Secondly, programming languages and even many type theories routinely use the equality operator even though its evaluation is also undecidable in certain situations, e.g., for function types.
However, while such undecidable behavior is accidental in programming and can be worked around, quantified formulas with undecidable evaluation are essential in logic.

\subsubsection{Proving}

To handle these formulas, we have to replace evaluation with an entirely new concept: proving.

Proving can be seen as a static variant of evaluation: we try to evaluate terms in the presence of \emph{arbitrary unknown values}.
Concretely, we assume that $\Gamma$ contains undefined names as in $\aval{x}{A}{}$.
In logic, these names are usually called \textbf{free variables}.
For example, to prove $\afa[A]{x}{F}$, we try to evaluate $F$ in a context that declares $\aval{x}{A}{}$.

Note that in executable programs, free variables are not allowed: every variable must have a concrete value.

If free variables are allowed, a formula $F$ does not necessarily evaluate to $\true$ or $\false$ anymore.
Instead, there are three options:
\begin{compactitem}
 \item Free variables can be eliminated, and $F$ evaluates to $\true$.
 \item Free variables can be eliminated, and $F$ evaluates to $\false$.
 \item $F$ evaluates to some term that still contains free variables.
\end{compactitem}

The distinction between the options is undecidable.
Proving tries to establish that one of the former holds.

Proving may use evaluation rules but usually a new set of rules and possibly auxiliary non-terminals and judgments have to be invented.
Together, these are called the \textbf{proof system} or \textbf{calculus}.

Implementations of such proof systems are called \textbf{theorem provers}.
The practical ones are usually extremely sophisticated, often comprising $>10^5$ lines of code.
Still, the problem of finding a proof of a given formula is so complex that \emph{fully automated} provers perform very poorly in practice.

Note that the problem of \emph{finding} a proof has to be distinguished from the problem of \emph{checking} a given proof.
The latter is easily decidable as we will see in Sect.~\ref{sec:sd:axthm}.

\subsection{Formulas not as Terms}

The treatment of formulas has received a great deal of attention, and multiple different approaches have been developed.
We will not pursue these in the sequel but list them here for completeness.

\subsubsection{Formulas as a Separate Concept}

The most obvious alternative is to use a separate concept, i.e., a new non-terminal symbol.
This is standard practice in first-order logic (FOL), where terms and formulas are strictly separated.

This is particularly reasonable for untyped FOL---the standard variant of FOL.
Here there are no types, i.e., no non-terminal $A$.
Alternatively, we can say that there is exactly one base type, and all terms have the same type.

In typed FOL, we have terms, types, and formulas.
Here we usually have a base type $\Bool$.
Thus, equality and propositional connectives must be duplicated as operators on terms and as operators on formulas.
An advantage of this design is that the quantifiers can be restricted to the formula-level so that the evaluation terms stays decidable.

\subsubsection{Formulas as Types}

A surprising but formally appealing variant is to make all formulas special cases of \emph{types}.
This is common in constructive type theories like Coq or Agda.

This has the advantage that proofs can be elegantly introduced as terms whose type is a formula.
A proof $P$ of $F$ would be represented as a term $\oftype{}{}{P}{F}$.

A drawback of this design is that all boolean operators are again duplicated.

An advantage is a striking elegance between type operators and connectives.
For example, if formulas are types, product types yield conjunction, and function types yield implication.
All logical operators except negation have meaningful analogues as operators on types.

This has made it possible to present theorems as programs.
For example, a theorem like $\afa[A]{x}{\afa[B]{y}{\aex[C]{z}{\true}}}$ can be represented as a function $\afunI[C]{f}{x:A,y:B}{P}$.
Giving the body $P$ of this function becomes equivalent to finding a proof of the theorem.

\section{Proofs as Terms}\label{sec:sd:proofterms}

As for programs, we have to decide whether proofs are a new non-terminal symbol or a special case of terms.
Both work well.
But for the same reason as for programs, it makes the language easier to make them terms: it eliminates the need for duplicating productions.

The details of what proof constructors to add and what typing rules to give them goes beyond the scope of this treatment.
We only give the necessary features for an empty logic and some examples.

\subsection{Empty Formal System}

We introduce a new type constructor that lifts boolean terms to types:

\[A \bbc \aproof{t}\]

\[\rul{\oftype{}{\Gamma}{t}{\Bool}}{\oftype{}{\Gamma}{\aproof{t}}{\type}}\]

The basic intuition is that the typing judgment becomes a proving judgment: we say that $P$ is a proof of $F$ using assumptions $\Gamma$ if
\[\oftype{}{\Gamma}{P}{\aproof{F}}\]

\subsection{Common Logical Features}

Logical features are very similar to type theoretical features.
In both cases, we usually add three productions and typing rules:

\begin{ctabular}{|l|l|}
\hline
Type theory & Logic \\
\hline
type constructor & formula constructor \\
\multicolumn{2}{|c|}{term constructor for building \ldots} \\
\ldots terms of that type & \ldots proofs of that formula \\
\ldots new terms from terms of that type & \ldots new proofs using proofs of that formula \\
\hline
\end{ctabular}

Thus, we need three productions and three typing rules each for conjunction, disjunction, negation, implication, universal quantification, and existential quantification.

\subsubsection{Implication}

We add implication using three productions
\[t \bbc t\impl t \bnfalt implIntro(x:t,t) \bnfalt modusPonens(t,t)\]
and three typing rules
\[\rul{\oftype{}{\Gamma}{F}{\Bool}\tb\oftype{}{\Gamma}{G}{\Bool}}{\oftype{}{\Gamma}{F\impl G}{\Bool}}\]

\[\rul{\oftype{}{\Gamma,\aval{x}{\aproof{F}}{}}{P}{\aproof{G}}}{\oftype{}{\Gamma}{implIntro(x:\aproof{F},P)}{\aproof{(F\impl G)}}}
\tb\tb
\rul{\oftype{}{\Gamma}{P}{\aproof{(F\impl G)}}\tb \oftype{}{\Gamma}{Q}{\aproof{F}}}{\oftype{}{\Gamma}{modusPonens(P,Q)}{\aproof{G}}}\]

In logic textbooks, the typing rules for the proofs are usually written by omitting the proof terms themselves.
Then we obtain the more familiar-looking
\[\rul{\iscons{}{\Gamma,\aproof{F}}{}{\aproof{G}}}{\iscons{}{\Gamma}{}{\aproof{(F\impl G)}}}
\tb\tb
\rul{\iscons{}{\Gamma}{}{\aproof{(F\impl G)}}\tb \iscons{}{\Gamma}{}{\aproof{F}}}{\iscons{}{\Gamma}{}{\aproof{G}}}\]
If we additionally write simply $F$ instead of $\aproof{F}$, we obtain the usual notation.

\subsection{Logics for Reasoning about Systems}

Logics like FOL and HOL are sufficient for reasoning about mathematical concepts.
(The difficulty here is usually to enrich the type theory in order to allow for more natural representations of mathematical objects.)

But for reasoning about dynamic systems like physical systems and machines, we need more.
Specifically, we must be able to represent the \emph{change} of the system over time.

For software systems, we can use \textbf{discrete} time, i.e., a representation of change as a sequence of states.
This corresponds to representing points in time as natural numbers.
For example, to verify a piece of code $C$, we have to talk about the values of all variables in two different states: \emph{before} and \emph{after} execution of $C$.

For physical systems, especially those interacting via sensor data, we may have to use \textbf{continuous} time: a representation of points in time as real numbers and of all values as functions over the real numbers.

Both present substantial challenges, and a variety of different logics has been developed.

\section{Axioms and Theorems as Declarations}\label{sec:sd:axthm}

A major advantage of representing formulas and proofs as terms is that we do not need to introduce new declarations for axioms and theorems.
They are normal value declarations whose type happens to be $\aproof{F}$ for some $F$.

If there is no definition, this yields an axiom declaration.
The declaration $\aval{a}{\aproof{F}}{}$ introduces an axiom named $a$ that asserts $F$.
This corresponds to assuming that we have a proof of $F$ without further specifying where it comes from.

If there is a definition, this yields a theorem declaration.
The declaration $\aval{a}{\aproof{F}}{P}$ introduces a theorem named $a$ that asserts $F$ and whose proof is $P$.
This corresponds to introducing $a$ as an abbreviation for the proof $P$.

Type-checking of these declarations proceeds in the same way as before.
In particular, to check $\isdecl{}{}{\aval{a}{\aproof{F}}{P}}$, we have to check $\oftype{}{}{P}{\aproof{F}}$, i.e., we have to check that $P$ is indeed a proof of $F$.
Thus, proof-checking becomes a special case of type-checking.

\section{Discrete-Time Logic}\label{sec:sd:discretetime}

\subsection{Grammar and Intuitions}

The basic idea of discrete time is to use a set of states and transitions between them.
All the intuitions and concepts from finite automata (which are sometimes alternatively called \emph{finite state machines}) can be generalized to arbitrary state machines.
The set of states is usually infinite or at least very large.

The central is the following:
\begin{definition}[States]\label{def:sd:state}
A \textbf{state} is a possible value for the set of cells in the environment.

A \textbf{transition} is a triple $(s,f,s')$, where $s$ is a state, $f$ is any function that can be applied to the environment, and $s'=f(s)$.

Here \emph{any function} means any combination of the operations introduced in Def.~\ref{def:sd:environment} and~\ref{def:sd:environmentio}.
\end{definition}

Now the evaluation of every term (including the truth of formulas) depends on the state.
Thus, the meaning of every term is a function from states to values.
In particular, the meaning of a formula is no longer a boolean but a function from states to booleans.

When designing logics, we try to avoid talking about states explicitly---that would just amount to reformulating verification problems in a different formalism.
Instead, we try to talk about states indirectly or to introduce names for only a few certain states.

\subsubsection{Modal Logic}

Historically, the central ideas go back to modal logic, which introduced the following operators in the early $20$th century:
\begin{ctabular}{|l|l|l|}
\hline
Formula & Holds in state $s$ if & also called\\
\hline
$\square F$ & $F$ holds in all possible successor states of $s$ & necessarily $F$\\
$\diamond F$ & $F$ holds in some possible successor state of $s$ & maybe $F$\\
\hline
\end{ctabular}

Here a possible successor state of $s$ is any state $s'$ such that there is a transition $(s,f,s')$.

Modal logics can be classified further according to the properties of the successor relation.
The following are the most important ones:
\begin{compactitem}
 \item Reflexivity: the successors of $s$ include $s$ itself.
 \item Transitivity: the successors of $s$ include all successors of successors of $s$.
\end{compactitem}
For reasoning about programs, it is most practical to assume both reflexivity and transitivity.

The successor relation of Def.~\ref{def:sd:state} is indeed reflexive and transitive:
\begin{compactitem}
 \item Reflexivity follows by using the identity function $id$ on the environment, which yields the do-nothing transition $(s,id,s)$.
 \item Transitivity follows by using the composition of functions: given $(s,f,s')$ and $(s',f',s'')$, we obtain the first-$f$-then-$f'$ transition $(s,f'\circ f,s'')$.
\end{compactitem}


\subsubsection{Dynamic Logic}

Dynamic logic expands on modal logic by restricting what transitions should be taken into consideration:

\[t \bbc \abox{t}{t} \bnfalt \adia{t}{t}\]

\begin{ctabular}{|l|l|}
\hline
Formula & Holds in state $s$ if  \\
\hline
$\abox{P}{F}$ & $F$ holds in all successor state of $s$ reachable by evaluating $P$ \\
$\adia{P}{F}$ & $F$ holds in some successor state of $s$ reachable by evaluating $P$ \\
\hline
\end{ctabular}

If $P$ is a deterministic program, then for every initial state $s$ there is at most $1$ successor state reachable by executing $P$: if $P$ terminates, there is exactly $1$ state; otherwise, there is no state.

In that case, the operators become very easy:
\begin{ctabular}{|l|l|}
\hline
Formula & Holds in state $s$ if  \\
\hline
$\abox{P}{F}$ & if $P$ terminates from $s$, $F$ holds afterwards \\
$\adia{P}{F}$ & $P$ terminates from $s$ and $F$ holds afterwards \\
$\adia{P}{\true}$ & $P$ terminates from $s$ \\
\hline
\end{ctabular}

\subsection{Typing}

The typing rules for the modal operators are very simple:

\[\rul{\oftype{}{\Gamma}{F}{\Bool}}
      {\oftype{}{\Gamma}{\square F}{\Bool}}
\tb\tb
\rul{\oftype{}{\Gamma}{F}{\Bool}}
    {\oftype{}{\Gamma}{\diamond F}{\Bool}}\]

For the operators of dynamic logic, there are different choices for the typing rules.
The following is a simple example:

\[\rul{\oftype{}{\Gamma}{P}{A}\tb \oftype{}{\Gamma}{F}{\Bool}}
      {\oftype{}{\Gamma}{\abox{P}{F}}{\Bool}}
\tb\tb
\rul{\oftype{}{\Gamma}{P}{A}\tb \oftype{}{\Gamma}{F}{\Bool}}
    {\oftype{}{\Gamma}{\adia{P}{F}}{\Bool}}\]

\subsection{Proving}

\subsubsection{Modal Logic}

Reflexivity and transitivity can be easily captured by the following axioms:

\[\rul{}{\iscons{}{\Gamma}{}{\square F\impl F}}\tb\tb\rul{}{\iscons{}{\Gamma}{}{F\impl \diamond F}}\]

\[\rul{}{\iscons{}{\Gamma}{}{\square F\impl \square\square F}}\tb\tb\rul{}{\iscons{}{\Gamma}{}{\diamond\diamond F\impl \diamond F}}\]

However, it is subtly difficult to give the basic rules for $\square$ and $\diamond$.

\subsubsection{Dynamic Logic}

The rules for the dynamic operators cannot be given generically.
Instead, we usually need to give a separate proof rule for every production of the program $P$.
We expand on that in Ch.~\ref{sec:sd:deductive}.